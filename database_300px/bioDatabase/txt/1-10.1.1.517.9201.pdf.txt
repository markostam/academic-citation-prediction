Optimal Stopping with Forced Exits
Hui Wang∗
Division of Applied Mathematics
Brown University
Providence, R.I. 02912
huiwang@cfm.brown.edu
Abstract
We consider a continuous time optimal stopping problem with mul-
tiple entries and forced exits. The value for such an optimization prob-
lem with a general payoﬀ function is solved in closed form under the
assumption that the state process is a geometric Brownian motion and
the forced exits come in according to a Poisson process. The eﬀect due
to the forced exits is analyzed. It is shown that the presence of the
forced exits is a true risk (meaning that it will reduce the value and
enlarge the continuation region) if and only if the entry cost is large
enough compared to the running cost.
1 Introduction
Classical stochastic optimal stopping problems usually assume that the de-
cision maker has the right to select an entry time and/or an exit time ac-
cording to the information history, in order to maximize or minimize a given
functional. Such a formulation has been extensively studied and successfully
applied to many disciplines including economics investment theory, Þnancial
option pricing, and so forth [1, 2, 3, 6, 9, 12, 13, 14]. The purpose of this
paper is to study an alternative setup with multiple entries and forced exits.
Also of interest is the eﬀect on the optimal behavior due to the presence of
these forced exits.
Consider the following standard formulation of an optimal stopping prob-
lem. Let c and k be two non-negative constants, X = (Xt) a non-negative
∗Research supported in part by the National Science Foundation under Grant NSF-
DMS-0103669.
stochastic process, and h : R+ → R+ a measurable function. The objective
is to Þnd an optimal stopping time (i.e., entry time) so as to maximize the
total discounted payoﬀ
E
∙Z ∞
τ
e−rt (h(Xt)− c) dt− e−rτk
¸
(1.1)
over all stopping times τ taking values in [0,+∞]. This optimization cri-
terion is motivated by the classical economic investment valuation problem
as follows. Suppose an investor must decide a (random) time τ to invest
in an economic project. The cost for initiating the project is k, while the
operation of the project entails a running cost with constant rate c. The
project will yield a payoﬀ with a rate in the form of h(X) where X is some
exogenous price process. For more details, please see [6], where a special
case is studied with h(x) = x and X a geometric Brownian motion.
In this paper we consider the following optimal stopping problem. Let
{Tj} be a given sequence of increasing, positive random variables, which
stand for the (potential) times of forced exits. The objective is to judiciously
choose a sequence of stopping times {τi} so as to maximize
E
∞X
i=1
∙Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¸
with the constraint τn < σn ≤ τn+1, where σn .= inf {Tj : τn < Tj}, for
every n. In other words, τn indicates the n-th entry time, while σn, the
(potential) exit time right after τn, is the n-th forced exit time.
Formulations of this type do not seem to have attracted much attention
in the probability literature. The present paper is concerned with the math-
ematical analysis of such models, and how the presence of these forced exits
aﬀects the optimal behavior as compared with that of the standard model
with payoﬀ (1.1). This model may also yield some interesting applications.
For example, it may be used as an extension to the previously mentioned
classical investment model, in order to incorporate the so-called liquidation
risk [4]: Suppose the investor borrows money from a bank in order to Þnance
the project. In return, the bank can be given the right to seize the project
asset and put them to alternative uses (i.e., liquidation), at times it sees Þt.
The reason for such liquidation could be that the bank needs money for a
new business or for consumption. Once liquidation happens, the investor
will not be able to acquire the output from the project. However, he can
restart the whole investment process again by Þnding another Þnancier. In
this case, {Tj} will be the sequence of potential liquidation times.
2
Remark 1.1 In our formulation, the exit times {σn} are completely de-
termined by our choice of {τn} and the pre-given sequence {Tj}. The case
where the exit times {σn} are also chosen by the decision maker has been
considered by many authors; see [5, 6, 7] and references therein. The latter
will yield a larger value function than that from the standard model (1.1).
This, however, is not always the case for our formulation; see Section 6 for
more details.
Remark 1.2 Of course, a fully ßedged investment model with liquidation
risk should be much more complicated. For example, we will assume {Tj}
is independent of the price process X, and the entry cost k is Þxed for each
round of investment. More realistically, one should allow some correlations
between {Tj} and X, and allow k to change for each investment cycle. But
by making these simpliÞcations, the model is more analytically tractable,
and will still provide some insights on the eﬀect of liquidation risk.
The paper is organized as follows. Section 2 gives the mathematical for-
mulation of the optimization problem. In Section 3, we provide a brief review
of an importance class of ODEs that will be very useful to our analysis. The
variational inequality associated with the value function is presented in Sec-
tion 4, which is then explicitly solved. A veriÞcation argument is given in
Section 5. The eﬀect of forced exits is analyzed in Section 6. It is discovered
that the forced exit is a true risk (meaning that it will reduce the value
function and enlarge the continuation region) if and only if the entry k is
large compared to the running cost c. To ease exposition, some of the more
technical proofs are deferred to the Appendix.
2 Model formulation
Let (Ω,F , P ) be a complete probability space with Þltration F = (Ft) satis-
fying the usual conditions: right-continuity and completion by P -negligible
sets. It is assumed to be rich enough to carry a standard Brownian motion
W = (Wt,Ft) and a Poisson process N = (Nt,Ft) with rate λ.
Let X be the geometric Brownian motion deÞned by
dXt = bXt dt+ σXt dWt, X0 = x, (2.2)
where b,σ are constants with σ > 0, and x ∈ R+ is the initial condition. The
inÞnitisimal generator of X is denoted by L; i.e., for any twice continuously
diﬀerentiable function f : R→ R,
(Lf)(x) .=
1
2
σ2x2f 00(x) + bxf 0(x). (2.3)
3
Let {Tj} denote the arrival times of the Poisson process N with con-
vention T0 = 0. Also let h : R+ → R+ be a measurable function, c, k two
non-negative constants, and r > 0 the discount factor. The objective is to
maximize the expected total discounted payoﬀ
V (x)
.
= sup
τi
Ex
∞X
i=1
∙Z σi
τi
e−rt (h(Xt)− c) dt− e−rτik
¸
(2.4)
over all sequence of stopping times {τi} taking values in [0,∞] with the
constraint τn < σn ≤ τn+1 for every n, where σn .= inf {Tj : τn < Tj}.
Condition 2.1 We will make the following assumptions in the paper.
1. The Brownian motion W and the Poisson process N are independent.
2. The function h is continuous, non-decreasing, non-constant, and sat-
isÞes h(0) = 0. Furthermore, for every initial condition x,
Ex
Z ∞
0
e−rth(Xt) dt <∞.
3. At least one of the constants c and k are strictly positive.
The inpendence of W and N is imposed for the convenience of analysis.
Using the methodology of this paper, one may still be able to solve the more
general case where the arrival rate of N is a function of W , but the analysis
will become more cumbersome. Condition 2.1.2 is not diﬃcult to verify,
and it guarantees that the optimization problem (2.4) is well deÞned; see
Remarks 2.1 and 3.2. Condition 2.1.3 is imposed to avoid the triviality.
Remark 2.1 For every stopping time sequence {τi}, we have
∞X
i=1
¯¯¯¯Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¯¯¯¯
≤
Z ∞
0
e−rt(h(Xt) + c) dt+
∞X
i=1
e−rτik.
However, it is not diﬃcult to see that τi ≥ Ti−1 for all i = 1, 2, . . .. Therefore,
Ex
∞X
i=1
e−rτik ≤ k Ex
∞X
i=1
e−rTi−1 = k Ex ·
∞X
i=1
(λ/(λ+ r))i−1 = k(λ+ r)/r,
which in turn implies that the optimization problem (2.4) is well deÞned.
4
3 Review of an important ODE
In this section, we discuss a class of ordinary diﬀerential equations that will
play an important role in the analysis.
Recall the deÞnition (2.3) and consider the second order ODE of form
−(r + λ)f + Lf + h = 0. (3.5)
A pair of fundamental solutions associated with this equation is (xβ+(λ), xβ−(λ)),
where
β±(λ)
.
=
µ
1
2
− b
σ2
¶
±
sµ
1
2
− b
σ2
¶2
+
2(λ+ r)
σ2
. (3.6)
It is not diﬃcult to see that, for every non-negative λ,
β−(λ) < 0 < β+(λ). (3.7)
Remark 3.1 To ease exposition, we will use β± instead of β±(λ) when no
confusion is incurred. We will also write α±
.
= β±(0).
Proposition 3.1 Under Condition 2.1.2, the ODE (3.5) admits a twice
continuously diﬀerentiable particular solution on R+
Hλ(x)
.
=
2
σ2(β+ − β−)
∙
xβ−
Z x
0
s−β−−1h(s) ds+ xβ+
Z ∞
x
s−β+−1h(s) ds
¸
=
2
σ2(β+ − β−)
∙Z 1
0
s−β−−1h(xs) ds+
Z ∞
1
s−β+−1h(xs) ds
¸
.
Furthermore, the function Hλ admits the stochastic representation
Hλ(x) = Ex
Z ∞
0
e−(r+λ)th(Xt) dt.
Proof. The proof can be found in [10, 11].
Remark 3.2 The paper [10] also proved the following result, which is very
useful for the veriÞcation of Condition 2.1.2. Given λ ≥ 0 and any non-
negative function h, if the function Hλ(x) as deÞned in Proposition 3.1 is
Þnite for every x, then
Ex
Z ∞
0
e−(r+λ)th(Xt) dt <∞.
The next result is also useful to the analysis. Its proof is deferred to
Appendix A.
5
Lemma 3.2 Assume that Condition 2.1.2 holds. Then for any λ ≥ 0
lim inf
x→∞ x
−β+Hλ(x) = 0.
Given any initial condition X0 ≡ x > 0, we have, with probability one,
lim
t→∞ e
−(r+λ)tHλ(Xt) = 0.
Moreover, for any stopping time sequence {τn} such that τn → ∞ almost
surely, we have
lim
n→∞E
x
h
e−(r+λ)τnHλ(Xτn)
i
= 0.
4 An auxiliary optimization problem and the vari-
ational inequality
To facilitate the analysis, we will introduce an auxiliary optimization prob-
lem as follows. Recall that {Tj} is the arrival times of the Poisson process
N . DeÞne the value function
V¯ (x)
.
= sup
τi
Ex
(Z T1
0
e−rt(h(Xt)− c) dt (4.8)
+
∞X
i=1
∙Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¸)
,
where the supremum is over all stopping time sequences {τi} such that
T1 ≤ τ1, and τn < σn ≤ τn+1 for every n where σn .= inf {Tj : τn < Tj}.
If we compare this optimization problem with the deÞnition (2.4) of V ,
the only diﬀerent piece is the extra Þrst integral in (4.8). One can interpret
V and V¯ as follows. At any time, the decision maker is either waiting to
make an entry (say, idle), or has already made one but not been forced out
yet (say, active). The value function V is the total expected discounted
payoﬀ the decision maker can achieve if he is idle, while V¯ if the decision
maker is active.
4.1 Heuristics for the variational inequality
We will proceed heuristically in order to derive the variational inequality
that the pair (V, V¯ ) should satisfy. It is not unnatural to guess that the
optimal policy for an idle decision maker should take the following form:
make an entry whenever the process X exceeds some threshold x∗, and wait
6
otherwise. As for an active decision maker, there is no choice until the
Þrst forced exit time T1, after which the decision maker becomes idle and
it should follow the same strategy described above to behave optimally.
If it is optimal for an idle decision maker to wait when the process X
is below x∗, then one would expect
−rV + LV = 0, for every x ∈ (0, x∗).
However, when the process X exceeds x∗, the idle decision maker will pay
entry cost k and become active. Therefore, one expects that
V = V¯ − k, for every x ≥ x∗.
Now let us see what equation that V¯ satisÞes. Consider an active
decision maker with the price state X0 = x. In a small time interval of
length 4t, the Poisson process N has probability λ4t to make a jump; i.e.,
with probability λ4t the decision maker will be forced to exit and become
idle and with probability 1− λ4t, the decision maker will stay active.
If the decision maker behave optimally afterwards, he will achieve the value
V (X4t). This suggests, at least formally, that
V¯ (x) = (h(x)− c)4t+ λ4t · V (X4t) + (1− λ4t)e−r4t Ex V¯ (X4t)
≈ (h(x)− c)4t+ λ4t · V (x) + (1− λ4t)[V¯ (x) + (−rV¯ + LV¯ )4t]
= V¯ (x) + [−rV¯ + LV¯ + λ(V − V¯ ) + h(x)− c]4t,
which yields
−rV¯ + LV¯ + λ(V − V¯ ) + h(x)− c = 0, for every x > 0.
We obtain the following variational inequality from the preceding heuris-
tic argument.
Variational Inequality: Find a pair of functions v ∈ C1(R+), v1 ∈ C2(R+),
and a constant x∗ > 0, such that
−rv + Lv = 0; if 0 < x < x∗, (4.9)
v − (v¯ − k) = 0; if x ≥ x∗, (4.10)
−rv¯ + Lv¯ + λ(v − v¯) + h(x)− c = 0; if x > 0 (4.11)
hold, and the growth condition
|v(x)| ≤ ε1H0(x) + ε2, |v¯(x)| ≤ ε1H0(x) + ε2 (4.12)
is satisÞed for some positive constants ε1, ε2 and every x > 0.
7
Remark 4.1 The value functions V and V¯ easily satisfy the growth condi-
tion (4.12). For example, it is clear from the deÞnition (2.4) and Proposition
3.1 that
0 ≤ V (x) ≤ Ex
Z ∞
0
e−rth(Xt) dt = H0(x),
The growth condition for V¯ is similarly obtained.
Remark 4.2 Thanks to (4.10) and (4.11), v satisÞes
−rv + Lv − (r + λ)k + h(x)− c = 0, ∀ x > x∗. (4.13)
However, equations (4.9), (4.13), the smooth-Þt-principle, and the growth
condition alone are not enough to yield a unique solution for v. On the
other hand, the addition of function v1 does ensure a unique solution to the
variational inequality; see Proposition 4.1. This says that the introduction
of the auxiliary problem (4.8) is somewhat necessary.
4.2 The solution to the variational inequality
The task of this section is to explicitly compute the solution to the varia-
tional inequality.
Recall α± as deÞned in Remark 3.1. It follows from equation (4.9) that,
for x < x∗,
v(x) = A+x
α+ +A−xα− ,
for some constants A±. However, the growth condition (4.12) ensures that
v is bounded around the neighborhood of 0, which in turns implies that
A− = 0 since α− < 0. Therefore,
v(x) = A+x
α+ , if x ∈ (0, x∗). (4.14)
On the other hand, equations (4.9) and (4.11) yield
−(r + λ)(v − v¯) + L(v − v¯) + h(x)− c = 0, for x ∈ (0, x∗).
Thanks to the results in Section 3, we have
v − v¯ = B+xβ+ +B−xβ− −Hλ(x) + c/(r + λ), if x ∈ (0, x∗),
for some constants B±. Similarly, the growth condition (4.12) implies B− =
0. Thus we have
v¯(x) = A+x
α+ −B+xβ+ +Hλ(x)− c/(r + λ), if x ∈ (0, x∗). (4.15)
8
On the interval (x∗,∞), equations (4.10) and (4.11) imply that
−rv¯ + Lv¯ + h(x)− c− λk = 0.
Again, thanks to the results in Section 3, we have (abusing the notation)
v¯(x) = Cxα+ +A−xα− +H0(x)− (c+ λk)/r, if x ∈ (x∗,∞),
for some constants C and A−. However, it is clear that C = 0 because of the
growth condition (4.12) and that lim infx→∞ x−α+H0(x) = 0 (see Lemma
3.2). Hence
v¯(x) = A−xα− +H0(x)− (c+ λk)/r, if x ∈ (x∗,∞), (4.16)
and
v(x) = A−xα− +H0(x)− [c+ (r + λ)k]/r, if x ∈ (x∗,∞). (4.17)
It remains to determine the four unknowns (x∗;A+, A−, B+). However,
the continuity of v, v0, v¯, v¯0 across the optimal investment boundary x∗ yields
four equations as follows.
A−(x∗)α− +H0(x∗)− [c+ (r + λ)k]/r = A+(x∗)α+,
α−A−(x∗)α− + x∗H 00(x
∗) = α+A+(x∗)α+ , (4.18)
−B+(x∗)β+ +Hλ(x∗)− c/(r + λ) = k,
−β+B+(x∗)β+ + x∗H 0λ(x∗) = 0.
Some algebra yields that x∗ is the solution to the equationZ 1
0
s−β−−1h(xs) ds =
σ2β+
2
µ
c
r + λ
+ k
¶
(4.19)
and
A± =
(x∗)−α±
α+ − α− ·
∙
x∗H 00(x
∗)− α∓H0(x∗) + α∓ c+ (r + λ)k
r
¸
, (4.20)
B+ = (x
∗)−β+ · x∗H 0λ(x∗)/β+. (4.21)
We have the following proposition, whose proof is technical and is de-
ferred to Appendix.
9
Proposition 4.1 Assume that Condition 2.1.2 and Condition 2.1.3 hold.
Let h(∞) .= limx→∞ h(x), and assume h(∞) > c + (r + λ)k. Then equa-
tion (4.19) admits a unique, strictly positive solution x∗. The constants
(A+, A−, B+) deÞned by (4.20) and (4.21) are all strictly positive. Further-
more, the triple (v, v¯; x∗) determined by equations (4.14)(4.17) is the unique
solution to the variation inequality (4.9)(4.12). In particular, the function
v is non-negative, non-decreasing, and satisfy
v(x) ≥ v¯(x)− k,
for all x > 0, where the equality holds if and only if x ≥ x∗.
5 The solution to the optimization problems
It is expected that the solution v to the variational inequality is the value
function of the optimization problem (2.4), and x∗ is the optimal threshold.
In this section we give a rigorous proof of this result.
Theorem 5.1 Assume Condition 2.1, and let h(∞) .= limx→∞ h(x).
1. If h(∞) ≤ c+(r+λ)k, then V (x) ≡ 0, and it is optimal never to make
an entry, or τ∗n ≡ ∞ for every n.
2. Assume h(∞) > c+ (r + λ)k. Let (v, v¯;x∗) be the unique solution the
variational inequality (4.9)-(4.12). Then V (x) = v(x) for every x > 0.
Furthermore, it is optimal to make an entry whenever the state process
X exceeds the threshold x∗. In other words, the sequence of optimal
entry times is recursively deÞned by
τ∗n
.
= inf
©
t ≥ σ∗n−1 : Xt ≥ x∗
ª
for every n ≥ 1, where σ∗0 .= 0 and σ∗n .= inf {Tj : τ∗n < Tj} for n ≥ 1.
Proof. In this proof, for any stopping time τ , the values of e−rτv(Xτ ) and
e−rτ v¯(Xτ ) on the set {τ =∞} are naturally set as 0, thanks to the growth
condition (4.12) and Lemma 3.2.
We will Þrst give the proof for the second case. Assume that h(∞) >
c + (r + λ)k. We will start by showing that, for any stopping time τ and
σ
.
= inf{Tj : τ < Tj} the Þrst Poisson arrival time after τ , we have
e−rτ v¯(Xτ ) = Ex
∙Z σ
τ
e−rt(h(Xt)− c) dt+ e−rσv(Xσ)
¯¯¯¯
Fτ
¸
. (5.22)
10
Thanks to strong Markov property of process X and the memoryless prop-
erty of exponential random variables, it is suﬃcient to show
v¯(x) = Ex
"Z U
0
e−rt(h(Xt)− c) dt+ e−rUv(XU )
#
, (5.23)
where U is an independent exponential random variable with rate λ. Con-
sider the process M = (Mt,Ft) where
Mt
.
= e−(r+λ)tv¯(Xt) +
Z t
0
e−(r+λ)s [h(Xs)− c+ λv(Xs)] ds.
Since v¯ is twice continuously diﬀerentiable, we can apply Ito formula and
equation (4.11) to obtain
Mt = v¯(x) +
Z t
0
e−(r+λ)sv¯0(Xs)σXs dWs.
However, it is not diﬃcult to see |xv¯0(x)| ≤ c1xα+ + c2 for some constants
c1, c2, thanks to equations (4.15), (4.16), and (A.3). Hence the above sto-
chastic integral, or the process M , deÞnes a true martingale. Therefore, for
every t ≥ 0, we have
v¯(x) = Exe−(r+λ)tv¯(Xt) + Ex
Z t
0
e−(r+λ)s[h(Xs)− c+ λv(Xs)] ds. (5.24)
But the growth condition (4.12) and Proposition 3.1 imply that
lim
t→∞E
xe−rt|v¯(Xt)| = 0.
Furthermore, the Monotone Convergence Theorem yields
lim
t→∞E
x
Z t
0
e−(r+λ)s[h(Xs)− c+ λv(Xs)] ds
= Ex
Z ∞
0
e−(r+λ)s[h(Xs)− c+ λv(Xs)] ds
However, note that
Ex
Z ∞
0
e−(r+λ)sλv(Xs) ds = Ex
Z ∞
0
e−rsv(Xs) · λe−λs ds = Exe−rUv(XU )
11
and by Fubinis Theorem
Ex
Z ∞
0
e−(r+λ)s(h(Xs)− c) ds (5.25)
= Ex
Z ∞
0
e−rs(h(Xs)− c)
Z ∞
s
λe−λt dt ds
= Ex
Z ∞
0
∙Z t
0
e−rs(h(Xs)− c) ds
¸
· λe−λt dt
= Ex
Z U
0
e−rs(h(Xs)− c) ds.
Now letting t → ∞ in equation (5.24), we arrive at (5.23), which yields
(5.22) readily.
We now show that the process
©
e−rtv(Xt)
ª
is a non-negative super-
martingale. The non-negativity is trivial from Proposition 4.1. Even though
v is only continuously diﬀerentiable, one can still apply Ito formula [8, Ex-
ercise 6.24], and it follows that
Yt
.
= e−rtv(Xt)−
Z t
0
[−rv(Xs) + Lv(Xs)] ds (5.26)
is a local martingale. However, by (4.9), (4.13), (A.6), we have −rv+Lv ≤ 0.
Therefore, Y is non-negative, whence a supermartingale. It is now easy to
see that
©
e−rtv(Xt)
ª
is also a supermartingale.
Fix an arbitrary stopping times τ . Let σ is the Þrst passage time of level
x∗ after τ ; i.e.,
σ
.
= inf {t ≥ τ : Xt ≥ x∗} .
We claim that
e−rτv(Xτ ) = Ex
£
e−rσv(Xσ)
¯¯Fτ ¤ , (5.27)
Thanks to strong Markov property, we can assume τ = 0 without loss of
generality. It follows from (5.26) and (4.9) that
Yt∧σ = e−r(t∧σ)v(Xt∧σ)−
Z t∧σ
0
(−rv + Lv)(Xu) du = e−r(t∧σ)v(Xt∧σ),
which is a local martingale. Clearly,
n
e−r(t∧σ)v(Xt∧σ)
o
is bounded, hence
it is a true martingale. Therefore,
v(x) = Ex
h
e−r(t∧σ)v(Xt∧σ)
i
,
for every t. Letting t→∞, (5.27) (with τ = 0) follows readily from Bounded
Convergence Theorem.
12
We are now in a position to prove that V (x) = v(x) and that {τ∗n} deÞne
an optimal sequence of entry times. Fix an arbitrary sequence of stopping
times {τn} such that τn < σn ≤ τn+1, where σn .= inf{Tj : τn < Tj}. We
have
v(x) ≥ Ex £e−rτ1v(Xτ1)¤
≥ Ex £e−rτ1 v¯(Xτ1)− e−rτ1k¤
= Ex
∙Z σ1
τ1
e−rt(h(Xt)− c) dt+ e−rσ1v(Xσ1)− e−rτ1k
¸
≥ Ex
∙Z σ1
τ1
e−rt(h(Xt)− c) dt− e−rτ1k
¸
+ Exe−rτ2v(Xτ2).
Here the Þrst and the last inequalities hold since {e−rtv(Xt)} is a non-
negative supermartingale; the second inequality follows from Proposition
4.1; the equality is due to (5.22). Repeating the above operation, it follows
that
v(x) ≥ Ex
nX
i=1
∙Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¸
+ Exe−rτn+1v(Xτn+1)
≥ Ex
nX
i=1
∙Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¸
for all n ≥ 1. Letting n → ∞, applying the Dominated Convergence The-
orem (thanks to Remark 2.1), and then taking supremum over all possible
sequence {τn}, we have v(x) ≥ V (x).
However, it is easy to see, when τi = τ
∗
i as deÞned in Theorem 5.1, all the
inequalities above become equalities, thanks to Proposition 4.1 and (5.27).
We have
v(x) = Ex
nX
i=1
"Z σ∗i
τ∗i
e−rt(h(Xt)− c) dt− e−rτ∗i k
#
+ Exe−rτ
∗
n+1v(Xτ∗n+1)
for all n ≥ 1. It is not diﬃcult to see that τ∗n ≥ Tn−1, which implies that
τ∗n → ∞ almost surely. Thanks to Lemma 3.2 and the growth condition
(4.12) we have
lim
n→∞E
xe−rτ
∗
n+1v(Xτ∗n+1) = 0.
Letting n→∞, we arrive at
v(x) = Ex
∞X
i=1
"Z σ∗i
τ∗i
e−rt(h(Xt)− c) dt− e−rτ∗i k
#
,
13
by the Dominated Convergence Theorem. It follows that v(x) ≤ V (x),
which in turns implies that v(x) = V (x) and {τ∗n} is the optimal strategy.
We complete the proof for the second case.
Now assume that h(∞) ≤ c+(r+ λ)k. It suﬃces to show that for every
stopping time sequence {τn}, we have
Ex
∙Z σi
τi
e−rt(h(Xt)− c) dt− e−rτik
¸
≤ 0,
for every i. Again thanks to the strong Markov property of X and the
memoryless property of exponential distributions, we only need to show
Ex
"Z U
0
e−rt(h(Xt)− c) dt
#
≤ k,
where U is an independent exponential random variable with rate λ. But
equation (5.25) and the monotonicity of h imply that
Ex
"Z U
0
e−rt(h(Xt)− c) dt
#
= Ex
Z ∞
0
e−(r+λ)s(h(Xs)− c) ds
≤ (h(∞)− c)/(r + λ)
≤ k.
This completes the proof.
Remark 5.1 Under the conditions of Theorem 5.1, one can similarly prove
that, for the auxiliary optimization problem (4.8), the value function V¯ = v¯
and the optimal entry threshold is x∗.
Corollary 5.2 Assume that Condition 2.1 holds, and let h(∞) .= limx→∞ h(x).
Consider the classical optimal stopping problem
V0(x)
.
= sup
τ
Ex
∙Z ∞
τ
e−rt(h(Xt)− c) dt− e−rτk
¸
.
1. If h(∞) ≤ c+ rk, then V0(x) ≡ 0, and τ∗ ≡ ∞ is optimal.
2. Assume h(∞) > c+rk. The optimal exercise boundary x∗ is the unique
solution to the equationZ 1
0
s−α−−1h(xs) ds =
σ2α+
2
µ
c
r
+ k
¶
14
and the value function is
V0(x) =
(
A+x
α+ ; if x ≤ x∗
H0(x)− [c+ rk]/r ; if x > x∗ .
Here
A+
.
=
(x∗)−α+
α+ − α− ·
∙
x∗H 00(x
∗)− α−H0(x∗) + α− c+ rk
r
¸
is a strictly positive constant.
Proof. The proof is similar and omitted.
Remark 5.2 It follows from Theorem 5.1 and Corollary 5.2 that the opti-
mal exercise threshold for the optimization problem (2.4) with forced exits is
the same as that of the classical optimization problem with a discount factor
r + λ instead of r. Note that the claim is not true for the value function.
6 Eﬀects of forced exits
In this section, we study the eﬀect on the optimal behavior due to the
presence of the forced exits. We will slightly alter the notation. Instead of
simply using x∗ and V respectively for the optimal exercise boundary and
the value function, we will write x∗(λ) and V (x;λ), in order to distinguish
among diﬀerent Poisson arrival rates λ. The case λ = 0 corresponds to the
classical model without forced exits. We will adopt the following assumption:
Condition 6.1 h(∞) =∞.
Even though the results in this section hold under weaker conditions, Con-
dition 6.1 greatly eases the exposition.
The next lemma states that as λ → 0, the optimal exercise boundary
and the value function converge to those of the classical model, respectively.
Lemma 6.1 Assume that Conditions 2.1 and 6.1 hold. We have
lim
λ→0
x∗(λ) = x∗(0), lim
λ→0
V (x;λ) = V (x; 0).
This convergence result is not surprising at all. The proof is trivial from
Theorem 5.1 and Corollary 5.2, and thus omitted.
A question that arises quite naturally is as follows. Will the presence
of the forced exits raises the optimal exercise boundary and reduces the
15
value function, compared to the classical model? If we consider our model
as an economics investment model with liquidation risk, then this question
is asking if the presence of liquidation risk will make the investment policy
more conservative and reduce the value of the investment. For this reason,
we will call the presence of forced exits a true risk if it raises the optimal
exercise boundary and reduce the value function.
It is not diﬃcult to guess that the answer depends very much on the
relative size of the entry cost k to the running cost c. Consider the extreme
case when k = 0. The presence of forced exits will only enlarge the value.
Indeed, when a forced exit arrives, the decision maker can either immediately
make an entry without any cost as if the forced exit had never occurred, or
just choose to wait for better times to enter. Hence the forced exits only
serve as a free opportunity to the decision maker. In contrast, if c = 0, then
it is clear that the presence of forced exits always reduces the value.
It will be our main task in this section to give the necessary and suﬃcient
condition for the presence of forced exits to be a true risk. More interestingly,
we also give the necessary and suﬃcient condition so that the presence of
forced exits is a true risk no matter what the value of the arrival rate λ is.
Indeed, this is the case if and only if the entry cost k is large compared to
the running cost c.
Theorem 6.2 Assume that Conditions 2.1 and 6.1 hold. Suppose k > 0.
Then the following conclusions hold.
1. There exists a cirtical threshold λ¯ ≥ 0, such that x∗(λ) ≥ x∗(0) and
V (x;λ) ≤ V (x; 0) for all x ∈ R+ if and only if λ ≥ λ¯.
2. Furthermore, the threshold λ¯ = 0 if and only if k ≥ g(c) for some
continuous function g : R+ → R+ with g(0) = 0 and g(∞) =∞. The
function g is sublinear in the sense that g(c) ≤ 2c/(σα−)2.
3. The function g is non-decreasing, if we further assume that h¯(x)
.
=
h(ex) is convex; e.g., h(x) = xp with p > 0.
Remark 6.1 Theorem 6.2 asserts that the presence of forced exits is a
true risk regardless of the arrival rate λ if and only if k ≥ g(c). It is also
interesting to note that g is not always non-decreasing, as Example 2 shows.
The proof of Theorem 6.2 relies on the following lemma, which is inter-
esting by itself. It asserts that the value is reduced whenever the optimal
exercise boundary is raised. The converse, however, is not true, as shown
by Example 1 below.
16
Lemma 6.3 Assume that Conditions 2.1 and 6.1 hold. For any λ, if x∗(λ) ≥
x∗(0), then V (x;λ) ≤ V (x; 0) for every x ∈ R+.
The proof of this lemma and Theorem 6.2 is lengthy and technical, and
will be deferred to Appendix B.
Remark 6.2 For the case k = 0, it is straightforward to show that x∗(λ) ≤
x∗(0) and V (x;λ) ≥ V (x; 0) for every x.
7 Examples
Example 1 We will consider the example with h(x) = x. It is not diﬃcult
to check that now Condition 2.1.2 is equivalent to b < r.
One can explicitly solve equation (4.19) to determine the optimal exercise
boundary for every λ:
x∗(λ) = ch1(λ) + kh2(λ),
where the functions h1, h2 are deÞned as
h1(λ)
.
=
β+
β+ − 1
r − b+ λ
r + λ
, h2(λ)
.
=
β+
β+ − 1(r − b+ λ).
With some algebra, one can verify that h01(0) < 0 and h02(0) > 0. Let
θ
.
= −h01(0)/h02(0).
We claim that g(c) is a linear function. More precisely, we have g(c) = θc.
Indeed, observe that
dx∗
dλ
¯¯¯¯
λ=0
= ch01(0) + kh
0
2(0) = h
0
2(0)(k − θc).
If k > θc, then x∗(λ) > x∗(0) for λ small enough. Thanks to Theorem 6.2
and Lemma 6.3, we have λ¯ = 0, which in turn implies that g(c) ≤ θc. On
the other hand, if k < θc, then x∗(λ) < x∗(0) for λ small enough. Thus
λ¯ > 0, and g(c) ≥ θc. To conclude, we must have g(c) = θc.
Numerical Illustration: Set r = 0.2, b = 0.15 and σ = 1. It follows that
θ ≈ 5.9. Let c = 1. Then λ¯ = 0 if and only if λ ≥ g(c) = θc ≈ 5.9.
Figure 1 shows the diﬀerence of optimal exercise boundary x∗(λ)−x∗(0)
as a function of λ, for k = 1, 3, 7. Clearly, for each k, the critical threshold
λ¯ is the intersection of the corresponding curve with the horizontal dotted
17
line. For instance, λ¯ ≈ 1.5 if k = 1, and λ¯ ≈ 0.25 if k = 3. When k = 7, as
expected, we have λ¯ = 0.
Figure 2 displays the diﬀerence of value functions V (x;λ)− V (x; 0) as a
function of x with the Poisson arrival rate λ set as 0.1. Note that the case
k = 3 shows that the converse of Lemma 6.3 cannot be true in general.
Example 2 This example is constructed in order to show that the function
g need not be non-decreasing in general.
Let the parameters be set as b = 0, r = 1,σ = 1, and the payoﬀ function
h(x)
.
=

x ; if 0 ≤ x ≤ 1
1 ; if 1 < x ≤M
x− (M − 1) ; if x > M
.
HereM is large positive constant whose speciÞc value is of little importance.
Clearly, the function h¯(x)
.
= h(ex) is not convex.
It follows from the proof of Theorem 6.2 that λ¯ = 0 if and only if
φ(c, k)
.
= − dβ−
dλ
¯¯¯¯
λ=0
Z 1
0
s−α−(− log s)dh (x∗(0)s)− k ≤ 0.
But in this case, we have α− = −1, dβ−/dλ|λ=0 = −2/3. Thus
φ(c, k) =
2
3
Z 1
0
s(− log s)dh (x∗(0)s)− k.
Thanks to Corollary 5.2, the optimal exercise boundary x∗(0) for the clas-
sical optimization problem (λ = 0) is uniquely determined by equationZ 1
0
h(xs) ds = c+ k.
From these consideration, it is not diﬃcult to compute the function g,
at least numerically. The result with M taken as 20, which is reported in
Figure 3, shows that the function g is not non-decreasing.
Summary. This paper considers a class of optimal stopping problems with
multiple entries and forced exits. Explicit solutions are obtained for general
payoﬀ functions under mild assumptions. It is discovered that the presence
of these forced exits will always raise the optimal exercise boundary and
reduce the value function if and only if the entry cost k and the running cost
c satisfy k ≥ g(c) for some function g. It is interesting that the function
g is not non-decreasing in general. Suﬃcient conditions for g to be non-
decreasing is also given.
18
References
[1] G. Barone-Adesi and R.E. Whaley (1987). Eﬃcient analytic approxi-
mations of American option values. Journal of Finance 42, 301-320.
[2] M.J. Brennan and E.S. Schwartz (1985). Evaluating natural resource
investments. Journal of Business 58, 135-157.
[3] J.M. Conrad (1997). Global warming: when to bite the bullet. Land
Economics 73, 164-173, 1997.
[4] D. Diamond and R. Rajan (2001). Liquidity risk, liquidity creation, and
Þnancial fragility: A theory of banking. Journal of Political Economy
109, 287-327.
[5] A.K. Dixit (1989). Entry and exit decisions under uncertainty. Journal
of Political Economy 97, 620-638.
[6] A.K. Dixit and R.S. Pindyck (1994). Investment under uncertainty.
Princeton University Press.
[7] J.K. Duckworth and M. Zervos (2000). An investment model with entry
and exit decisions. J. Appl. Prob. 37, 547-559.
[8] I. Karatzas and S. Shreve (1991). Brownian Motion and Stochastic
Calculus. Springer-Verlag, New York.
[9] I. Karatzas and S. Shreve (1998). Methods of Mathematical Finance.
Springer-Verlag, New York.
[10] T.S. Knudsen, B. Meister, and M. Zervos (1998). Valuation of invest-
ments in real assets with implications for the stock prices. SIAM. J.
Control Optim. 36, 2082-2102.
[11] T. Kobila (1993). A class of solvable stochastic investment problems
involving singular controls. Stochastics Stochastics Rep. 43, 29-63.
[12] R. McDonald and D. Siegel (1986). The value of waiting to invest.
Quarterly J. Econ. 101, 707-728.
[13] R.C. Merton (1990). Continuous-Time Finance. Cambridge-Oxford:
Blackwell.
[14] B. Oksendal and K. Reikvam (1998). Viscosity solutions of optimal
stopping problems. Stochastics Stochastics Rep. 62, 285-301.
19
Appendix A. Collection of Proofs
Proof of Lemma 3.2. Suppose that lim infx→∞ x−β+Hλ(x) > 0. Then it
is not diﬃcult to see that there exists a constant ε > 0 such that
ε ≤ xβ−−β+
Z x
0
s−β−−1h(s) ds,
for x large enough. Since h is non-decreasing, we have
ε ≤ xβ−−β+
Z x
0
s−β−−1h(x) ds = h(x)x−β+/(−β−).
But this implies thatZ ∞
x
s−β+−1h(s) ds ≥ −β−ε
Z ∞
x
s−β+−1sβ+ ds =∞,
or Hλ(x) =∞, for x large enough. This is a contradiction.
We now show that there exists positive constants c1, c2 such that
Hλ(x) ≤ c1xβ+ + c2 (A.1)
for all x. Indeed, simple computation yields
xH 0λ(x) =
2
σ2(β+ − β−)
∙
β−
Z 1
0
s−β−−1h(xs)ds
+ β+
Z ∞
1
s−β+−1h(xs)ds
¸
. (A.2)
In particular, we have xH 0λ(x) ≤ β+Hλ(x). This easily implies thath
x−β+Hλ(x)
i0 ≤ 0.
Equation (A.1) follows readily. Note that we also have (abusing the notation
a bit)
xH 0λ(x) ≤ c1xβ+ + c2. (A.3)
It is not diﬃcult to check
e−(r+λ)tXβ+t = x
β+ exp{σβ+Wt − σ2β2+t/2}.
This and (A.1) clearly imply
lim
t→∞ e
−(r+λ)tHλ(Xt) = 0. (A.4)
20
As for the second part of the lemma, we observe that Proposition 3.1, the
strong Markov property of X and equation (A.4) yield
Ex
h
e−(r+λ)τnHλ(Xτn)
i
= Ex
∙Z ∞
τn
e−(r+λ)th(Xt) dt
¸
.
Applying the Dominated Convergence Theorem, we complete the proof.
Proof of Proposition 4.1. We Þrst show that equation (4.19) admits a
unique strictly positive solution. Let
f(x)
.
=
Z 1
0
s−β−−1h(xs) ds.
Clearly f is non-decreasing. It is not diﬃcult to see that f is also continuous
with limx→0 f(x) = 0 from Dominated Convergence Theorem. Furthermore,
it follows from Monotone Convergence Theorem that
f(∞) .= lim
x→∞ f(x) = h(∞)
Z 1
0
s−β−−1 ds = −h(∞)/β−.
However, observing that
β+β− = −2(r + λ)/σ2, (A.5)
we have
f(∞) = σ
2β+h(∞)
2(r + λ)
>
σ2β+
2
µ
c
r + λ
+ k
¶
,
which is exactly the right-hand-side of (4.19). The existence of a strictly
positive solution to (4.19) is now clear. As for the uniqueness, suppose
x1 < x2 are both solutions to equation (4.19). Since h(x1s) ≤ h(x2s) for
every s, we must have h(x1s) = h(x2s) for almost every s ∈ [0, 1]. But the
continuity of h further implies that h(x1s) = h(x2s) for every s ∈ [0, 1]. In
particular, we have
h(x2) = h
µ
x2 · x1
x2
¶
= · · · = h
µ
x2 ·
µ
x1
x2
¶n¶
,
for every n. The continuity of h then implies h(x2) = h(0) = 0, and whence
h(x2s) = 0 for all s ∈ [0, 1]. This is a contradiction since the right-hand-side
of (4.19) is strictly positive.
21
We now show that the three constants (A+, A−, B+) are all strictly pos-
itive. Since h is non-decreasing and non-constant, we have
β−
Z 1
0
s−β−−1h(xs) ds+ β+
Z ∞
1
s−β+−1h(xs) ds
> h(x)
∙
β−
Z 1
0
s−β−−1 ds+ β+
Z ∞
1
s−β+−1ds
¸
= 0.
This and equation (A.2) give H 0λ(x) > 0 for every x > 0, which implies that
B+ is strictly positive. In order to show A+ is strictly positive, it suﬃces to
show that
x∗H 00(x
∗)− α−H0(x∗) + α−[c+ (r + λ)k]/r > 0.
It follows from (A.5) and (A.2) (with λ = 0) and some computation that
the above inequality is equivalent to
α+
Z ∞
1
s−α+−1h(x∗s) ds− [c+ (r + λ)k] > 0.
However, we have
α+
Z ∞
1
s−α+−1h(x∗s) ds ≥ α+h(x∗)
Z ∞
1
s−α+−1 ds = h(x∗).
It follows trivially from the monotonicity of h, (4.19) and (A.5) that
h(x∗) > c+ (r + λ)k. (A.6)
Therefore, A+ is strictly positive. Similarly, in order to show A− > 0, we
only need to show
α−
Z 1
0
s−α−−1h(x∗s) ds+ [c+ (r + λ)k] > 0.
But integration by parts yields
α−
Z 1
0
s−α−−1h(x∗s) ds = −h(x∗) +
Z 1
0
s−α−dh(x∗s)
> −h(x∗) +
Z 1
0
s−β−dh(x∗s)
= β−
Z 1
0
s−β−−1h(x∗s) ds.
22
The desired inequality now follows from (4.19) and (A.5).
It is clear that (v, v¯, x∗) is the unique solution to the variational inequal-
ity, and v(x)− v¯(x) = −k for all x ≥ x∗. We will show by contradiction that
v(x) − v¯(x) > −k for all x ∈ (0, x∗). Suppose v(x) − v¯(x) ≤ −k for some
x ∈ (0, x∗). Since limx→0[v(x) − v¯(x)] = c/(r + λ) > −k, one can always
Þnd a x¯ ∈ (0, x∗) such that
v(x¯)− v¯(x¯) ≤ −k, v0(x¯)− v¯0(x¯) = 0.
But this amounts to
−B+x¯β+ +Hλ(x¯)− c/(r + λ) ≥ k
−β+B+x¯β+ + x¯H 0λ(x¯) = 0.
Multiplying the Þrst inequality by β+ and then subtracting the second equal-
ity, we arrive at
β+Hλ(x¯)− x¯H 0λ(x¯) ≥ β+[c/(r + λ) + k].
However,
β+Hλ(x¯)− x¯H 0λ(x¯) =
2
σ2
Z 1
0
s−β−−1h(x¯s) ds.
Now by the deÞnition (4.19) of x∗ and that h is non-decreasing, we have
x¯ ≥ x∗. This is a contradiction.
It remains to show that v is strictly increasing (whence strictly positive).
This is trivial on interval (0, x∗). As for x ≥ x∗, observe that (4.13) and
simple computation yields
xv0(x)− α−v(x) = xH 00(x)− α−H0(x) + α−[c+ (r + λ)k]/r
=
2
σ2
Z ∞
1
s−α+−1h(xs)ds+ α−[c+ (r + λ)k]/r,
which is clearly non-decreasing. Again, we will show by contradiction that
v is non-decreasing on interval (x∗,∞). Suppose this is not the case. We
can always Þnd x1 > x2 > x
∗ such that v(x1) < v(x2). Since v0(x∗) > 0, the
maximum of v over interval [x∗, x1] must be obtained at some interior point
y2 ∈ (x∗, x1). We have y2 < x1, v(x1) < v(y2), and v0(y2) = 0. This implies
the existence of y1 ∈ (y2,∞) such that v(y1) < v(y2) and v0(y1) ≤ 0. We
have
y1v
0(y1)− α−v(y1) ≤ −α−v(y1) < −α−v(y2) = y2v0(y2)− α−y2.
23
This contradicts the fact that xv0(x)−α−v(x) is a non-decreasing function.
We complete the proof.
Proof of Lemma 6.3. In the proof, we will denote by A±(λ) the coeﬃcients
A± obtained in equation (4.20), in order to distinguish among diﬀerent
Poisson arrival rates λ. Naturally, we just set A+(0) as the coeﬃcient A+
in Corollary 5.2. We will consider the following three cases separately.
Case 1: x ≤ x∗(0). In this case, V (x;λ) = A+(λ)xα+ and V (x; 0) =
A+(0)x
α+ , thus we only need to show A+(λ) ≤ A+(0). However, thanks
to equation (4.18) and that A−(λ) ≥ 0 (Proposition 4.1), we have
A+(λ) ≤ [x∗(λ)]−α++1H 00(x∗(λ)).
Therefore, it suﬃces to show that the function
f(x)
.
= x−α++1H 00(x)
is a non-increasing function, or f 0(x) ≤ 0. However, direct computation
yields that
f 0(x) =
2
σ2
∙
−α−xα−−α+−1
Z x
0
s−α−−1h(s)ds− x−α+−1h(x)
¸
,
which implies, thanks to the monotonicity of h, that
f 0(x) ≤ 2
σ2
∙
−α−xα−−α+−1h(x)
Z x
0
s−α−−1ds− x−α+−1h(x)
¸
= 0.
Case 2: x ≥ x∗(λ). In this case,
V (x;λ)− V (x; 0) = A−(λ)x−α− − λk/r.
Since α− < 0 and A−(λ) > 0, it suﬃces to show that
A−(λ)[x∗(λ)]−α− ≤ λk/r,
or equivalently,
x∗(λ)H 00(x
∗(λ))− α+H0(x∗(λ)) + α+[c+ (r + λ)k]/r ≤ (α+ − α−)λk/r.
Thanks to equation (A.2), all we need to show is that
− 2
σ2
Z 1
0
s−α−−1h(x∗(λ)s) ds+ α+[c+ rk]/r ≤ −α−λk/r. (A.7)
24
However, since h is non-decreasing, we have
LHS ≤ − 2
σ2
Z 1
0
s−α−−1h(x∗(0)s) ds+ α+[c+ rk]/r = 0,
where the equality is from Corollary 5.2. Inequality (A.7) is now immediate.
Case 3: x∗(0) < x < x∗(λ). Without loss of generality, assume x∗(0) <
x∗(λ). We will show by contradiction. Suppose V (x;λ) > V (x; 0) for some
x∗(0) < x < x∗(λ). Thanks to the results from Case 1 and Case 2, there
exists an x¯ ∈ (x∗(0), x∗(λ)) such that
V (x¯;λ)− V (x¯; 0) > 0
and
V 0(x¯;λ)− V 0(x¯; 0) = 0;
here V 0 is the derivative with respect to x. Since for x∗(0) < x < x∗(λ),
V (x;λ) = A+(λ)x
α+ and V (x; 0) = H0(x)− [c+ rk]/r, we have
A+(λ)x¯
α+ > H0(x¯)− [c+ rk]/r
and
α+A+(λ)x
α+ = x¯H 00(x¯).
It follows that
α+H0(x¯)− α+[c+ rk]/r < x¯H 00(x¯),
or
α+H0(x¯)− x¯H 00(x¯) =
2
σ2
Z 1
0
s−α−−1h(x¯s) ds < α+[c+ rk]/r.
Thanks to Corollary 5.2 and that h is non-decreasing, we have x¯ < x∗(0),
which is a contradiction.
Proof of Theorem 6.2. Thanks to (4.19) and (A.5), x∗(λ) is the unique
solution to the equation
−β−
Z 1
0
s−β−−1h(xs) ds = c+ (r + λ)k.
Clearly, x∗(λ) ≥ x∗(0) if and only if
−β−
Z 1
0
s−β−−1h (x∗(0)s) ds− λk ≤ c+ rk.
25
Regarding the left-hand-side of the above inequality as a function of λ and
denote it by f(λ), then x∗(λ) ≥ x∗(0) if and only if
f(λ) ≤ c+ rk.
We claim that f is a concave function. Indeed, integrating by parts, we
arrive at
f(λ) = h (x∗(0))−
Z 1
0
s−β−dh (x∗(0)s)− λk.
However, it is not diﬃcult to check that for any s ∈ [0, 1], s−β− is a convex
function with respect to λ. Therefore, f(λ) is a concave function. Since
f(0) = c+ rk by Corollary 5.2 and clearly limλ→∞ f(λ) = −∞, there exists
a λ¯ ≥ 0 such that f(λ) ≤ c + rk if and only if λ ≥ λ¯. Or equivalently,
x∗(λ) ≥ x∗(0) if and only if λ ≥ λ¯. Thanks to Lemma 6.3, we have proved
the Þrst part of the theorem.
The preceding proof also implies that λ¯ = 0 if and only if f 0(0+) ≤ 0.
However, it follows readily from Dominated Convergence Theorem and the
concavity of f that
f 0(0) = − dβ−
dλ
¯¯¯¯
λ=0
Z 1
0
s−α−(− log s)dh (x∗(0)s)− k. (A.8)
Regard f 0(0) as a function of c and k, say ϕ(c, k). It is rather straightforward
to see that ϕ is a continuous function on region {(c, k) : c ≥ 0, k ≥ 0}, and
ϕ(c, 0) ≥ 0 with equality if and only if c = 0. We need the following result.
Lemma A.1 The function ϕ(c, k) is strictly decreasing with respect to k.
It is non-decreasing with respect to c if we further assume h¯(x)
.
= h(ex) is
convex. Moreover, ϕ(c, k) ≤ 0 if k ≥ 2c/(σα−)2.
We will assume that Lemma A.1 holds for now, and leave its proof to the
end. It follows easily that there exists a function g : R+ → R+ such that
ϕ(c, k) ≤ 0 if and only if k ≥ g(c) with g(0) = 0. Clearly g(c) ≤ 2c/(σα−)2.
We now prove that g is continuous. Let c ∈ R+ and a sequence of {cn} such
that cn → c. It suﬃces to show that
lim sup
n
g(cn) ≤ g(c) ≤ lim inf
n
g(cn)
Let K be an arbitrary non-negative number such that K < lim supn g(cn).
Then there exist a subsequence, still indexed by n, such that K < g(cn),
26
which implies that ϕ(cn,K) > 0, since ϕ is strictly decreasing in k (Lemma
A.1). Therefore, by the continuity of ϕ, we have
ϕ(c,K) = lim
n
ϕ(cn,K) ≥ 0.
It follows that K ≤ g(c). Since K is arbitrary, we arrive at lim supn g(cn) ≤
g(c). The proof of the inequality lim inf g(cn) ≥ g(c) is very similar and thus
omitted.
If we further assume that h¯(x) = h(ex) is convex, then Lemma A.1
asserts that ϕ is non-decreasing with respect to c. It follows readily that g
is non-decreasing.
Proof of Lemma A.1. Equation (A.8) and integration by parts yield
ϕ(c, k) = − dβ−
dλ
¯¯¯¯
λ=0
Z 1
0
s−α−−1[1− α− log s]h(x∗(0)s) ds− k.
Let
ε
.
= − dβ−
dλ
¯¯¯¯
λ=0
=
1
σ2
"µ
1
2
− b
σ2
¶2
+
2r
σ2
#−1/2
,
which is strictly positive.
Thanks to Corollary 5.2 and equation (A.5), we have
ϕ(c, k) = ε
∙
c+ rk
−α− − α−
Z 1
0
s−α−−1 log s · h(x∗(0)s) ds
¸
− k.
Some algebra yields that
ϕ(c, k) = ε
"
c
−α− +
σ2α−k
2
− α−
Z 1
0
s−α−−1 log s · h(x∗(0)s) ds
#
.
Since α− < 0, and x∗(0) increases as k increases, the function ϕ is clearly
strictly decreasing with respect to k. Moreover, it is obvious that
ϕ(c, k) ≤ ε
"
c
−α− +
σ2α−k
2
#
.
Thus, φ(c, k) ≤ 0 whenever k ≥ 2c/(σα−)2.
Finally, assume that h¯(x)
.
= h(ex) is convex. Then h¯ is diﬀerentiable
almost everywhere. Let D+h¯ denote the right-derivative of h¯. Then D+h¯
27
is a right continuous, non-decreasing function. Furthermore, by a change of
variable in equation (A.8), we have
φ(c, k) = ε
Z 0
−∞
e−α−t(−t)dh
³
x∗(0)et
´
− k
= ε
Z 0
−∞
e−α−t(−t)dh¯ (t+ log x∗(0))− k
= ε
Z 0
−∞
e−α−t(−t)D+h¯ (t+ log x∗(0)) dt− k
But as c increases, so does x∗(0). Since D+h¯ in non-decreasing for every t,
we complete the proof.
28
Forced exits arrival rate
dif
fer
en
ce
 be
tw
ee
n o
pti
ma
l e
xe
rci
se
 bo
un
da
rie
s
0.0 0.5 1.0 1.5 2.0 2.5 3.0
0
2
4
6
8
10
k=1
k=3
k=7
Figure 1: Eﬀect of forced exits on optimal exercise boundaries
State x
dif
fer
en
ce
 be
tw
ee
n v
alu
e f
un
cti
on
s
0 10 20 30 40 50
-
3
-
2
-
1
0
1
k=1
k=3
k=7
Figure 2: Eﬀect of forced exits on value functions
29
Running cost c
En
try
 co
st 
k
0.0 0.5 1.0 1.5 2.0
0.0
0.5
1.
0
1.
5
2.
0
Figure 3: The function g is not always non-decreasing
30
