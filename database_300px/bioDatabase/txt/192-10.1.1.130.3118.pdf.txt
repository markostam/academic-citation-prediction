Journal of Research of the National Bureau of Standards Vol. 49, No. 1, July 1952 Research Paper 2341
Solution of Systems of Linear Equations by
Minimized Iterations1
Cornelius Lanczos
A simple algorithm is described which is well adapted to the effective solution of large systems
of linear algebraic equations by a succession of well-convergent approximations.
1. Introduction
In an earlier publication [14]2 a method was
described which generated the eigenvalues and eigen-
vectors of a matrix by a successive algorithm based
on minimizations by least squares.3 The advantage
of this method consists in the fact that the successive
iterations are constantly employed with maximum
efficiency which guarantees fastest convergence for
a given number of iterations. Moreover, with the
proper care the accumulation of rounding errors
can be avoided. The resulting high precision is of
great advantage if the separation of closely bunched
eigenvalues and eigenvectors is demanded [16].
It was pointed out in [14, p. 256] that the inversion
of a matrix, and thus the solution of simultaneous
systems of linear equations, is contained in the
general procedure as a special case. However, in
view of the great importance associated with the
solution of large systems of linear equations, this
problem deserved more than passing attention.
It is the purpose of the present discussion to adopt
the general principles of the previous investigation
to the specific demands that arise if we are not inter-
ested in the complete analysis of a matrix but only
in the more special problem of obtaining the solution
of a given set of linear equations
Ay=b0 (1)
with a given matrix A and a given right side bQ.
This is actually equivalent to the evaluation of one
eigenvector only, of a symmetric, positive definite
matrix. It is clear that this will require considerably
less detailed analysis than the problem of construct-
ing the entire set of eigenvectors and eigenvalues
associated with an arbitrary matrix.
2. The Double Set of Vectors Associated
With the Method of Minimized Iterations
The previous investigation [14] started out with
an algorithm (see p. 261) which generated a double
set of polynomials, later on denoted by Pi(x) and
qt(x) (see p. 274). Then a second algorithm was
1 The preparation of this paper was sponsored (in part) by the Office of Naval
Research.2
 Figures in brackets indicate the literature references at the end of this paper.
3 The present paper is a natural sequel to the previous publication and depends
on the previous findings. The reader's familiarity with the earlier development
is assumed throughout this paper; the symbolism of the present paper is in har
mony with that used before, in particular the notation pq, if applied to vectors,
shall mean the scalar product of these two vectors.
207064—52 3
introduced, called "minimized iterations", which
avoided the numerical difficulties of the first algor-
ithm (see p. 287) and had, in addition, theoretically
valuable properties for the solution of differential
and integral equations (p. 272).
In this second algorithm, however, only one-halj
of the previous polynomials were represented, cor-
responding to the Pi(x) polynomials whose coeffi-
cients appeared in the full columns of the original
algorithm [14, (60)]. The polynomials g^x), asso-
ciated with the half columns of [14, (60)] did not
come into evidence in the later procedure.
The vectors bi} generated by minimized itera-
tions, correspond to the polynomials Pi(x) in the
sense
bk=pk(A)b0. (2)
We should expect that the vectors generated by
gk(A)b0 might also have some significance. We will
see that this is actually the case. It is of consid-
erable advantage to translate the entire scheme
[14, (60)] into the language of minimized iterations,
without omitting the half columns. We thus get
a double set of vectors, instead of the single set
considered before.
The additional work thus involved is not super-
fluous because the second set of polynomials can be
put to good use. Moreover, the two sets of poly-
nomials belong logically together and complement
each other in a natural fashion. From the practical
standpoint of adapting the resultant algorithm to
the demands of large scale electronic computers, we
gain in the simplicity of coding. The recurrence
relations which exist between the polynomials
Pt(x)> Q.t(x) a r e simpler in structure than the recur-
rence relation obtained by eliminating the second
set of polynomials.
We want to simplify and systematize our notations.
The vector obtained by letting the polynomial
pk(A) operate on the original vector b0, shall be
called pk:
pk=pk(A)b0. (3)
We thus distinguish between pk as a vector and
pk(A) as a polynomial operator. Hence the notation
pk will take the place of the previous bk. Cor-
respondingly we denote the associated second set
of vectors by gk:
(4)
33
Both of these vector sets have invariant signifi-
cance. The vectors pk(A)b0 can be characterized
as the solution of the following minimum problem.
Form the polynomial
pk=[Ak-(a0+a1A+ . . .
p* = [A**-(ao+alA* + . .
(5)
determining the coefficients at by the condition that
the square of the length of pk, that is, the invariant
pkpt shall become a minimum.
The vectors qk(A)b0 can be characterized as the
solution of the following minimum problem. Form
the polynomial
qk=[l-(d1A+a2A2+ . . . + akAk)]b0
. . . + akA**)]b0*
(6)
determining the coefficients at by_ the condition
that the square of the length of ~qk, that is, the
invariant ~qkq*, shall become a minimum.
In the case (5) the highest coefficient of the
polynomial is normalized to 1, and in the case
(6) the lowest coefficient is unity.4
After the minimization we shall normalize, for the
sake of convenience, the largest coefficient of qk
once more to 1; hence we define
(7)
While the vectors pk and p* form a biorthogonal
set of vectors [14, p. 266], this cannot be said of the
vectors qk. However, the vectors qk are of particular
importance for the solution of sets of linear equations.
If we form the ratio
2 / * - ' = gk(A)-qk(0) (8)
we have obtained a solution of the equation
Ay*_i — bo= — qk. (9)
Hence we see that if the vectors qk are at our disposal,
we can at every step of our algorithm obtain an
optimum solution of smallest residual. Indeed, the
vector qk was defined by the condition that it shall
have the smallest length among all the linear com-
binations which can be formed with the help of the
successive iterates
bm=Abm-1=AmbQ
up to the order k.
The alternate solution
pk(A)-Vk(0)
-Pk(0)A
(10)
(11)
4
 The definition of the vectors pk and p% reveals the following remarkable
property of this vector set. Let 60 remain unchanged but the matrix A be
changed to A—\I, where X is arbitrary. The vectors pk, p% remain invariant
with respect to this transformation. The same cannot be said of the vectors
gives a larger residual for the same k, except if we
proceed to the very end of the process, k=n, in
which case the residual vanishes for both yk and yk
and both coincide with the exact solution y:
yn-i=yn-i=y. (12)
3. The Complete Algorithm for Minimized
Iterations
We will now proceed to the exposition of the
completed algorithm which does not omit one-half
of the basic algorithm [14, (60)] but translates the
entire algorithm into the frame of reference of min-
imized iterations.
The algorithm [14, (60)], generated a double set of
polynomials, mutually interlocked by the following
recurrence relations:
(13)
Elimination of the qk{x) leads to the three-term
recurrence relation for the pk(x) alone:
= PkVk (x).+xqk (x)
O +pk+1(x).
with5
Pk+i 0*0 = (x—ak)pk ^t (x) (14)
(15)
On the other hand, elimination of the pt(x) leads to
the three-term recurrence relation of the qt(x) alone:
with
(16)
(17)
Replacing x by A, A* and letting these polynomials
operate on b0, &*> we obtain the following relations
between the vectors pk and qk:
(18)
refers to the multiplication
(19)
The notation "prime'
by the matrix A:
« The negative signs in (14) and (16) are chosen because for symmetric and
positive definite matrices an important prediction can be made concerning the
signs of the fundamental scalars. The original algorithm which introduces the
hi and fcj coefficients reveals [14, p. 262] that both of these coefficients arise from
a minimization process and both of them have the significance of the square of a
length. In the case of symmetric (or Hermitian) and positive definite matrices
the metric is real and the square of a length necessarily positive. Hence the
hi and hi' are all positive, the pi, at all negative. This makes the a, and 0< (and
likewise the a,-, 01) always positive for such matrices.
34
Now the biorthogonality of the vectors pt gives
if we multiply the upper left equation (18) by pi
ptqi Pti?
ph=
~vt¥r~wt'
(20)
Moreover, the same equation shows the orthogon-
ality of gl to all pt, except m=k and k+1. In
particular
(21)
Now we prime the second equation and multiply on
both sides by pt. This gives:
(22)
We introduce the scalars hi and h{ by putting
(23)
and obtain:
K
hk
hi-4 (24)
K
This completely translates the "progressive algo-
rithm77 into tlie language of minimized iterations.
The ht numbers are identical with the ht of the
scheme [14], (60) (p. 263), corresponding to the full
columns 0, 1, 2, . . . , while the h[ give the A-numbers
of the half columns 0.5, 1.5, 2.5, 6
A remarkable relation between the pt and the de-
terminant of the matrix A can be obtained if in the
first equation of (13) we substitute x=0:
Hence
and
Pk(O) = PoPi • • • Pk-i
Pn(O)=PoPl - • • Pn-1-
(25)
(26)
(27)
Since pn(A) 60—0 yields the characteristic equation
of the matrix A, (—l)npn(0) must be the determinant
» associated with the matrix A. The determinant of
A is thus obtained as the product of all the pz-,
multiplied by (— l)n:
\ = (-l)npoPl Pn-l- (28)
In the following sketch of the general work scheme
we will restrict ourselves to the particularly impor-
tant case of symmetric matrices. This suffices for
6
 The same algorithm shows another remarkable property of the qi vectors.
These vectors do not form an orthogonal set because the polynomials Qi(A) have
the property to give orthogonality only if they operate on -JA 60 rather than 60 it-
in addition the second and third rows form likewise a mutually orthogonal (biorthogo-
nal) set.
the purpose of solving linear equations that can al-
ways be symmetrized, by transforming the originally
given set
, . (29)
(30)
(31)
into
where
Ay=b0,
A=G*6
bo=G*g. (32)
The matrix A is now symmetric and positive definite.
In this case the general scheme is reduced to one
half of its original size, since
A=A* (33)
We need not distinguish between pt and pf, qt, and
qf, since our reference system is orthogonal and the
adjoint vector coincides with the vector itself.
The actual construction of the symmetrized matrix
A is a very "expensive" operation, since it is equiva-
lent to n matrix multiplications of the type Ab.
Actually, we never need the matrix A itself but only
A operating on a certain vector b. By the associa-
tive law (G*6)b=G*(6b). Hence the operation Ab
is equivalent to the performing of the two successive
matrix multiplications ba) = Gb and b{2) = G*ba).
This requires 2n2 multiplications, compared with
%n2(n+l) multiplications required for constructing
G*G.
Every cycle in the following iteration scheme
consists of the construction of three] vectors, viz.,
pi} qt, q-. The third is merely the matrix A applied to qt.
Hence the problem is reduced to the construction of
the vectors pt and qt. In the following symbolic
work scheme (34) the sequence of operations is indi-
cated by going from row to row, and in each row
from the left to the right:
Po=bo
q'0=AqQ
ho=p\
K
"hi
(34)
This scheme is characterized by great uniformity and
is well suited to coding for large scale machines.
The generation of each new pair of piy qt vectors
occurs constantly by the same scheme and involves
35
for both vectors uniformly the immediately preceding
vector and the penultimate vector (we skip the vector
between). For example, px is obtained as a combi-
nation of #q and 60 (we skip q0), whereas % is obtained
as a combination of p\ and q0 (we skip (&). The
immediate predecessor is merely added, whereas the
earlier predecessor is always multiplied by the nega-
tive ratio of the last two /i-numbers (h'o and h0 in the
case of p1} hi and h'o in the case of qi).
It may help the coder to have a geometric picture
of the scheme as a whole—such as the scheme that
might profitably be used by a desk computer. In
such an arrangement the pt and at factors should be
placed in front of the respective rows that they mul-
tiply. Hence we keep a column free in front of the
vector scheme and write down p0, immediately in
front of p0; a0 in front of q0, and so on. Moreover,
it is of advantage to carry an extra column at the
end of the vector scheme which makes the vectors
n+1 -dimensional instead of ri-dimensional. This
extra column does not participate in the formation of
the hi and At'? but otherwise we operate with it
exactly as with the other columns. The element
that completes q- is always put equal to zero. The
first two vectors p0 and q0 are completed by 1.
This "surplus" column provides two important
scalars, namely, pk(0) and g*(0). The last row gives
pny which is the null vector. The "surplus" element
Pn(0) associated with pn terminates the algorithm,
and gives the determinant of A, multiplied by
The following numerical example is intentionally
simple, since the aim is to display the operations
rather than the numerical details. For the same
reason the fractions encountered are not changed
into decimals but left in fractional form.
2
1
0
0
- 1
2
- 1
0
0
_ • ^
2
- 1
(T
0
- 1
2 .
y
1
1
1
0
Po'. — 1
<r0: - i
-
Pi' —f-o:
tn: - *
p2: —V°-
<r2: - A
. 5
Po :
tfo :
qi :
Pi :
0i :
tf:
P2 :
92 :
*
:
P s :
9a :
9 a :
p 4 :
1
1
1
i
- i
i
-i
0
0
f
*
0
1
1
0
- I
3
- 2
- 1
0
— 1
— T
- \
- A
0
l
l
l
i
- *
1
0
l
- A
- A
0
0
0
- 1
- 1
- 1
- f
1
1
1
+
A
A
0
Po(O) =
go(O) =
0
Pi(O) =
ffi(O) =
0
P2(O) =
32(0) =
0
Ps(O) =
gs(O) =
0
P4(0) =
1
1
- 1
3
2
- 2
— • ¥ •
5
» . :
*.':
hl:
W:
h :
K :
3
2
1
*
2
A
vi: - i
V2- i
The scheme comes automatically to a halt whenever
the first pi vanishes in all its components. If the
vector 60 has no "blind spots" in the direction of any
of the principal axes, then the scheme will continue
until k=n, and the first pt that vanishes will be pn.
This is pi in our example, since n=k. The element
in the bracketed column associated with p± is 5.
Hence the determinant of the given system is estab-
lished as 5.
Numerical checks. The algorithm provides the
following powerful checks for the numerical calcula-
tions : ^
(a) The dot-pr%duct of any two different ^-vectors
is zero.
(b) The dot-product of any ^-vector with any q'-
vector except its own pair, is zero.
(c) Within each cycle the scalar hf can be obtained
in two different ways: h/=Piq/ = qiq/.
If we are interested in finding the characteristic
equation of the matrix, we proceed in identical fash-
36
ion with the only difference that we put in the brack-
eted column opposite to g/ not zero but the algebraic
quantity X times the element immediately above it.
In our example, if we write the successive vertical
elements of each cycle horizontally, the bracketed
column becomes:
Cycle 0: 1, 1, X
l : -*+A, - t + X , -fX + X2
2: T.—
3: _
2-4X + X2, 2X-4X2+X3
4: 5-20X + 21X2-8X3+X4.
The last polynomial is the characteristic polynomial
whose roots give the eigenvalues X* of the matrix.
The significance of the last column t\i will be ex-
plained in the next chapter.
4. Solution of the Linear System by the
g-Expansion
So far we have constructed the two vector sets pt
and (/*, which characterize the method of minimized
iterations. Our aim is, however, to obtain the solu-
tion y of the given linear set. For this purpose we
assume that the vector y is expanded into the qr
vectors:
n=l
i=0
We now form the equation
Ay=bo=po
(35)
(36)
for the right side of (35). Making use of the first
equation of the fundamental recurrence relation (18),
we obtain the following recurrence set for the coeffi-
cients rjt of the expansion (35):
(37)
Hence
starting with
In solved form
1
PoPi • •
_ Vi
Pz + l
1
Po
• Pi J
1
0<+i(O)
(38)
(39)
(40)
The vector equation (35), if translated into matrix
language, has the following significance. Write the
7) i as a column vector and multiply this column with
the successive columns of the matrix Q, formed
out of the middle vectors qt of the iteration scheme(34). For this reason the numerical scheme (34)
is augmented by a last column, composed of the
successive -qu and written down in the corresponding
rows of the vectors &. We find in our numerical
scheme the element
1
—
Po
in the row q0, the element
in the row qi} and so on. p
the successive columns of the qt yields
f h l i
} Multi lication of this
column by      i l
the successive components o t e solution y:
9 13 12 6
Substitution into the original equation shows that
this is indeed the correct solution.
If we do not carry the bracketed surplus column
of our scheme, then it is convenient to generate
the rji in succession on the basis of the recursion
(38), writing each rjt in line with the vector gf. If
the bracketed column is at our disposal, then we
merely take the negative reciprocal of the first
bracketed element in each cycle and transfer it to
the qt immediately preceding it. For example the
first element of cycle 1 in the bracketed column
2 3is — —; the negative reciprocal is —, which is trans-it 2i
ferred to the middle line of the previous cycle. Then
7 5
— is transferred as — •=• to the middle line of the pre-
o 7
vious cycle, and so on, until all the first elements
of the bracketed column are exhausted, the last
rii=r]n_-i being the reciprocal of the determinant
\A\. The sign of the r)t always alternates between
+ and —.
The objection may be raised that the vectors
Pi and qt have no invariant significance in relation
to the matrix A. They depend on b0 and thus,
while we did get the solution or the given linear
set, yet the matrix inversion gives much more because
it is immediately applicable to any given right side
b0.
Now the remarkable fact holds that actually our
piy qt, although generated with the help of some
specific bo, nevertheless, include the solution of a
linear set with any given right side c. The right
side of the equations (37) is t, 0, 0, . . . only
because the vector &0> analyzed in the reference
system of the pt, has these components. Since,
however, the Vi form an orthogonal set of vectors,
37
we can immediately analyze any given c in this
frame of reference. The components of c in this
system become
h0 hn-1
generally
hi
(41)
and these are the quantities that in the general case
appear on the right side of (37):
PoVo= — Mo
— Vo=— Mi
Pi+lVi + l—Vi= Mz + 1-
(42)
This set is again readily solvable by recursions.
Then after obtaining the vector rj, we obtain y once
more by (39).
Example. In our numerical example let us replace
the right side by
c = 0 , 0, 0, 1.
The dot-products of this c with the vectors ph
divided by ht become:
n 3 3 10 l
and the step by step solution of (42) gives:
Multiplying again by the g-vectors we obtain
1 2 3 4
y
which is the correct solution.
Ay=c (43)
with any given right side c is obtainable if. we first
construct the pt, q{ vectors with the help of some
definite b0, which can be arbitrary except for the fact
that it shall have no blind spots in the direction of
any of the principal axes. If &0 is deficient in the
direction of m axes of A, then the iteration scheme
will come to an end after n—m iterations. This
will necessarily happen if the matrix A has multiple
eigenvalues, no matter how b0 was chosen. Let a
certain X* have the multiplicity /x. Then there is a
^-dimensional subspace in which the direction of the
principal axes is undetermined. Let us project b0
into this subspace. We get a definite vector which
may be chosen as one of the principal axes. Then
&o is still deficient in the other JJL—1 possible orthogo-
nal axes.
From this viewpoint the premature termination
of our scheme can always be conceived as a conse-
quence of the deficiency of b0, no matter whether that
deficiency originates in the accidental degeneracy of
&o, or in the degeneracy of the matrix A. When-
ever this situation is encountered, we do not obtain
a full solution of the equation (43). Yet we have
obtained a preliminary ya) which solves the equation
at least in all the nondeficient directions. If we then
form Aya)— c=ca), this c(1) will contain only
dimensions which before did not come into evi-
dence. We can now repeat the scheme (34)
once more, using c(1) as the b0 of the new scheme; we
obtain a new set of pt, & vectors which can be added
to the previous set. Assuming that c(1) does not
bring in newer deficiencies relative to the previously
omitted subspace, we will now have a complete set of
Pt, q_i vectors which include the entire space. If some
dimensions are still omitted, the procedure can be
continued, until all ^-dimensions of the vector space
are exhausted.
The outstanding feature of the recurrence relations
(37) and (42) is the fact that they are two-term re-
lations. This has the following remarkable conse-
quence. We have pointed out before that we can
consider the successive stages of our iteration process
as a succession of approximations. At every step of
the process we can form the ratios (11) or (8) and
thus obtain approximations yk and yk which come
nearer and nearer to the true solution as the residual
diminishes. Now the set (42) shows that this suc-
cessive approximation process does not need constant
readjustments as we go from k to k + 1. The pre-
vious approximation remains unchanged, we merely
add one more vector, mamely, rjk+iqk+i.
The expansion (35) into the g-vectors thus imi-
tates the behavior of an orthogonal expansion whose
coefficients remain unchanged as we gradually intro-
duce more and more vectors of the function space
until finally all dimensions are exhausted. This
shows the superiority of the (/-vectors for expansion
purposes. If the vectors pt are used, the relations
involve three-term recurrences and we cannot solve
the set by one single recursion, but need the proper
linear combination of two recursions; this involves
constant modification of the approximation pre-
viously obtained.
If we pursue our procedure as a sequence of suc-
cessive approximations which may be terminated at
any point where the residual has dropped down below
a preassigned limit, it will be important to obtain
not only the subsequent corrections, but also the
remaining residual. This residual is directly avail-
able. The remaining residual, that is, right side
minus left side of the linear system after substituting
the kth. approximation yk, is simply given by the
quantity
(44)
38
For example, if in our numerical scheme we stop
with rj2, we obtain the approximation
The residual associated with this approximation is
thus
~h IT T — it
which can be verified by substitution.
By merely watching the last two columns of our
scheme we can constantly keep track of the successive
whittling down of the residual. The length of the
remaining residual is obtained by multiplying the
last rji by the square root of the next following ht (we
skip h/). For example in our numerical problem the
lengths of the successive residuals become:
\rt\: V3 = 1.7321,"
The simple expression of the residual (44) is of
great advantage if we decide to use our process in
"blocks" rather than as a continuous succession of
operatiorfs. The accumulation of rounding errors
tends to destroy the orthogonality of the pt more and
more. If we do not want to take recourse to the
lengthy process of constant reorthogonalization, we
can break our operations in blocks as soon as we
notice that the rounding errors have done too much
damage to the orthogonality. In that case we
evaluate the remaining residual and start the process
independently over again. The accumulation of
rounding errors is thus avoided, at the price of
retarded convergence.
Now the expression (44) shows that very little
adjustment is needed in order to change from the
continuous technique to the block technique.
The residual of the last block serves as the initial
vector of the new block. Now the residual of a block
of k+1 cycles (the cycles being numbered as 0, 1, 2,
. . ., k) is —VkPk+i' In the continuous flow of
operations the next cycle would have started with
pk+1. The changing over to independent blocks
merely requires that we should multiply this vector
by the negative value of the preceding rjkJ but this is
equivalent to the division by pk+i(Q) which can be
found in the surplus column of the same pk+i row.
Hence the change to the block technique merely
requires that we should continue in the regular
fashion up to the row
which terminates that block. The next block starts
with
Vk+i -,
P*+i(0)' '
and we repeat under it once more
These are the p0, q0 of the new block, and now we
continue with the scheme in the regular fashion,
until the next block is exhausted, and so on.
The solution itself is obtained exactly as before, by
transferring the — l/pk+1(0) to the row of the qk and
then adding up the contributions of all the qk.
We see that the block technique does not require
essentially more work than the continuous technique,
except that the total number of cycles needed for a
certain accuracy is increased, compared with the con-
tinuous technique constantly corrected by reorthog-
onalization.
If the right side &0 is changed to some other given
vector c, then special precaution is necessary due to
the fact that we do not possess now a universal
•orthogonal reference system which includes the entire
space but each block provides its own partial refer-
ence system. We determine for the first block the
Hi according to (41) and then obtain the rji by the
recursions (42). But coming to the second block we
have to replace c by the new vector c(2)=c—^^tPi
and repeat the process of obtaining the fit ajid the
rji for the new block with this new vector. Then we
reduce similarly c(2) to c(3) for the next block and so on.
The duality of the vectors pi} qt is mirrored by the
duality of the two kinds of approximate solutions yk
and yk, defined by (11) and (8). The recurrence
relations (13) permit us to establish recurrence rela-
tions between these two sets of solutions. We per-
form the operations (11) and (8) in (13), replacing x
by A, and let these polynomials operate on b0. This
gives: ^
(45)
qk+i(O)yk=qk(O)akyk-.1—pk+1(O)yk.
We can simplify these relations by introducing the
proportional vectors
PoPi • • • Pk
since from (26),
Hence we obtain
(Tk
PoPi • • • Pk+1
- PoPi • • . Pk+l
(46)
(47)
(48)
(49)
The recurrences (48) and (49) start with
Po
0*0
(50)
39
The recursion (48) expresses our previous solution
(35), (37) in slightly different form. However, an
additional approximation is now provided by the
scheme (49) which generates the vk by a process anal-
ogous to that in (48). The vectors vk are of value if
we want a_solution of smallest residual, since this
solution is yk and not yk. After obtaining vk by the
scheme (49), we can also obtain yk by multiplying
by the constant oo0"i . . . <rk/qk+i(0).
The residual associated with yk is given by
(51)
and this is the absolutely smallest residual obtainable
by k iterations. In the previous numerical example
the length of the residual associated with yx is 1.9365,
which is larger than the original length 1.7321 of the
vector b0. The length of rx associated with the solu-
tion yi, on the other hand, is
[gil
= 1.2910,
which is smaller than the original length.
The result is different, however, if we investigate
the error of the solution, that is, \y—yk\, rather than
the magnitude of the residual, which is \A(y—yk)\.
The solution yk has the property _to minimize(y—yk)A(y—yk) while the solution yk minimizes[A(y—yk)]2. The first quantity is less biased com-
pared with the direct drror square \y—yk\2 than the
second. Hence yk yields a smaller error in the solu-
tion, although a larger error in the residual than yk.
To illustrate; in the numerical example the length of
the vectorj/—yx is 1.884, while the length of the
vector y—yi is 3.0768. For this reason the vector
yk will usually be of smaller significance than the
vector yk.
5. The Preliminary Purification of the
Vector b0
In principle we have obtained a method for the so-
lution of sets of linear equations which is simple and
logical in structure. Yet from the numerical stand-
point we must not overlook the danger of the possible
accumulation of rounding errors. The theoretically
demanded orthogonality of the vector set pt can be
quickly lost if we do not watch out for rounding
errors. Now we can effectively counteract the dam-
aging influence of rounding errors by constantly
orthogonalizing every new pt to all the previously
obtained pt. We do that by a correction scheme
described in the earlier paper [14, p. 271, (60)].
This constant orthogonalization, however, is a
lengthy process which basically destroys the sim-
plicity of the generation of every new pt and qt by
using only two of the earlier vectors. In order to make
the corrections, all the previous pt have to be con-
stantly employed.
This consideration indicates that it will be advis-
able not to overstress our algorithm to too great a
length. If by some means fast convergence can be
enforced, the scheme might terminate in much fewer
than n steps. Even if theoretically speaking the last
vector vanishes exactly only after n iterations, it is
quite possible that it may drop practically below neg-
ligible bounds after a relatively few iterations.
We can predict in advance, under what conditions
we may expect fast convergence. If we want the
scheme to terminate after less than n steps, it is neces-
sary and sufficient that the vector b0 shall be deficient
in the direction of certain axes. The more "blind
spots" the vector b0 has in the direction of various
principal axes, the quicker will the scheme terminate.
In the practical sense it will not be necessary that
b0 shall be exactly deficient in certain axes. It will
suffice if the components of b0 in the direction of cer-
tain principal axes are small. Strong convergence in
this sense means that we shall reduce the components
of b0 in as many axes as possible.
That such a "purification" of b0 of many of its com-
ponents is actually possible, is shown by the Sylves-
ter-Cayley procedure by which the largest eigenvalue
and associated eigenvector of a matrix may be ob-
tained [8, p. 134]. In principle any linear set of
equations is solvable by the Sylvester-Cayley pro-
cedure. Indeed, let us homogenize the linear system
(29) by completing the matrix G by an n+ 1st column
defined as — g, and an w+lst row defined as identi-
cally zero. Then the linear eq (29) can now be for-
mulated in the homogeneous form
where
G1y1=0,
G1=G0-g
(52)
(53)
where a is any nonzero constant.
We now consider (52) as the solution of the follow-
ing least-square problem. Minimize
(54)
(55)
under the auxiliary condition
The solution of this minimum problem is the princi-
pal axis problem
Aiy1-\y1=0, (56)
where
A1=G*G1. (57)
We are particularly interested in the principal axis
associated with the smallest eigenvalue
X=0. (58)
Let us now assume that we somehow estimated
the largest eigenvalue \M of the nonnegative matrix
Av Then the matrix
A2=\MI—Al (59)
40
is a new n + 1 -dimensional nonnegative matrix whose
largest eigenvalue
X=XM (60)
corresponds to the zero eigenvalue of A\.
Now the Sylvester-Cayley asymptotic method
consists in choosing an arbitrary trial vector b0
which has to satisfy the one condition that it shall
not be deficient in the direction of the eigenvector
connected with the largest eigenvalue X .^ We now
form the sequence
JkO 7j Jd A 7v /j2 A Id A 2 h
U UQj U XJ.2UQJ U -L\.2\) JC\2UQ) . . .
and obtain asymptotically
Vi -> (61)
This method is of great theoretical importance,
even if it often converges too slowly to be useful
numerically. A proper refinement of the method,
however, will make it well adapted to our present
aims.
For the purpose of making the Sylvester-Cayley
procedure more effective, let us analyze the problem
in the reference system of the principal axes of the
matrix A2. Let us first normalize the largest eigen-
value to 1 by dividing A2 by \M>. We thus want to
operate with the matrix
A
o=^ (62)
whose eigenvalues lie between 0 and 1.
In the reference system of the principal axes the
trial vector b0 shall have the components
• , Pn (63)
assuming that the eigenvalues \t are arranged ac-
cording to increasing order on magnitude. The
operation bm=A$b0 generates the vector
Now
, ftcMT, • - • , ft,+io>C+i. (64)
Xi.+1-l, h<l ( i=l ,2 , . . . , n). (65)
Hence, as n grows to infinity, we get in the limit
( i=l ,2 , . . . ,n), (66)
while X™+1 remains constantly equal to 1. We thus
get in the limit the vector
0,0,. . . , 0, fin+10, (67)
which differs only by a factor of proportionality
from the vector
0,0,. . . , 0 , 1 . (68)
This, however, is the principal axis associated with
the largest eigenvalue Xn+1 = l.
While this method works very well in obliterating
the small eigenvalues, it becomes very inefficient
for a \iy which is near to 1.
Taking our lead from the Hamilton-Cayley
procedure we will now approach the problem from
a more general viewpoint. We go back to our
original matrix A and the given right side b0. In-
stead of considering a mere power bm, we will consider
an arbitrary polynomial Pm (A) operating on b0. For
the sake of convenience we will once more introduce
the reference system of the principal axes and we
will once more normalize the largest eigenvalue of
A to 1 by introducing the new matrix
^•0 — T A,
where X^ is the largest eigenvalue of A.
Our aim is to solve the equation
Aoy=b°,
where we have put
(69)
(70)
(71)
Instead of the exact solution we consider an
approximation y obtained by letting some polynomial
Pm(A0) operate on 6°. This leads to a residual
vector
rm+1=[l-A0Pm(A0W (72)
and our aim is to reduce rm+1 to a small quantity.
Instead of Pm(x) let us consider the polynomial of
one higher order
Apart from the boundary condition
F(0) = l
(73)
(74)
F(x) may be chosen as an arbitrary polynomial of
the order m+1 .
At this point we want to extablish a definite
measure em+i for the closeness of our approximation.
We define €m+i as the ratio of the length of the residual
vector rm+i to the length of the correct solution y:
(75)
Let us now discuss our problem in the reference
system of the principal axes. The components of
y in this system shall be denoted by
2 / 1 0 , # 2 0 , . . . , y n O
Then the components of &° become
(76)
(77)
41
while the components of the vector r become
F(\)\y1Q, • . . . y
Now by definition:
(78)
(79)
k=l
and the theorem of weighted means gives the estima-
tion
e>m&x\\kF(\k)\. (80)
Hence our aim must be to choose the polynomial
F(x) in such a fashion that the maxima of xF{x) shall
remain uniformly small in the interval between 0
and 1, which covers the entire range of the X*.
We make our choice as follows. We introduce the
Chebyshev polynomials Tn(x),7 normalized to the
range 0 to 1; [13, p. 140]. These polynomials are
defined by [19, p. 3] *
) = COS 7*0 (81)
(82)
with
We now put
1—cos 0 .
 9 6
x= ^ —
= s i n
 2*
sin2 (m + 2) 0
(m + 2)2 s in 2 1
and notice that the quantity
xFm+1(x)
1
is bounded by
(m+2)2
throughout the range 0 < x < l . Hence
1
€ w + 1
-(rn+2) 2 '
(83)
(84)
(85)
(86)
Since we have made our choice Fm+l(x), the cor-
responding approximate solution
_1-Fm+1(AO) b° (87)
is uniquely determined. We introduce the poly-
nomials
gm{x)=
(m+2)2 l -
Tm+2(x)+2(rn+2yx-l (88)
7
 The use of the Chebyshev polynomials for the solution of linear systems has
been suggested at various times [4]. The author is not aware that the specific
method here recommended has been suggested before.
which have integer coefficients. For the sake of con-
venience we list the first five gm(x) polynomials:
ffi (a;) = 6 -
(89)
- 1024a:5,
This table is actually not needed for the generation
of the successive vectors gm(A0)b°. We can obtain
these vectors much more elegantly and with smaller
rounding errors by a simple recursion scheme. We
start out with the recursion formula of the Chebyshev
polynomials, normalized to the range 0 to 1:
Tm+1(x)=2(l-2x)Tm(x)-Tm_1(x), (90)
and obtain for the polynomials gm(x) the following
recursion relation:
gm+i(x)=2(l-2x)gm(x)-gm-i
starting with
go(x)=i
(91)
In order to utilize this relation for the generation
of the vectors gm(A0)b°, we introduce the matrix
B=2I-4:AO
= 2 7 - ^ - ^ 1
and obtain the generating scheme
gm+i=Bgm-gm
starting with
(93)
(94)
(95)
The last term of (94) can be absorbed in the simpler
recursion formula
=Bgm—gm-i (96)
if we agree to operate again with a surplus column
similar to that used in our previous numerical example
(cj. the bracketed column of the numerical scheme of
section 3). We extend the matrix B by an n + l s t
42
column for which we choose the given right side b°:
B=B, b°. (97)
Similarly, we extend the vectors gm by a surplus
element, defined as the integer (m+2)2:
=9m, (m+2)2. (98)
The surplus column of the vector scheme g^ can be
filled out in advance by the squares of the integers,
starting with 4, 9, 16, . . ., in contrast to the brack-
eted column of the previous scheme which was filled
out as the scheme unfolded itself. The surplus
column of the matrix B and the surplus elements of
the vectors gm participate solely in the formation of
the product B"gm, but have no effect on the subtrac-
tion of gm-i, which is subtracted without its surplus
element.
The definition of the gm (x) polynomials shows that
the approximate solution (87) is in the following
relation to the vectors gm just generated
(99)
Moreover, if we want to find the residual vector
associated with the solution wm, we have to form
rm+l=b°—Aown
1 (100)
( S
The last equation allows the following interpreta-
tion. Let us assume that at a certain m we want to
terminate our process. We will now want to know
how much the remaining residual is. For this pur-
pose we merely add one more iteration according to
(96), then the quantities required in (100) are avail-
able with the only modification that instead of sub-
tracting gm-i we subtract 2gm. This vector, divided
by (m+2)2, gives the residual rm+!.
Numerical example. The following illustrative ex-
ample is chosen to demonstrate the operation of the
method. Our matrix A is once more the matrix of
the numerical example of section 3. The right side
is chosen as &o—0, 0, 0, 4.
Estimation of the largest eigenvalue XM. The larg-
est eigenvalue of a matrix can be estimated by the
method of GerSgorin [9], (cf. also [3] and [20]).
Even if this estimation is not always very close, it
gives a definite upper bound for \M by a very simple
test. Such an estimate is what we need since an
over estimation of \M merely makes the largest
eigenvalue smaller than 1. The only thing we have
to avoid is a \M larger than 1, because then we
would overstep the region where the Chebyshev
polynomials are bounded by unity in absolute value.
The method of Gersgorin, restricted for our case
to the estimation of the largest eigenvalue, is based
on the definition of the eigenvectors of a matrix A
by the equations
n
We consider only one equation of the given set, pick-
ing out that particular index i which belongs to the
absolutely largest xt. We now divide by xt on both
sides of the equation. Since |#a/#i|<l, we find at
once
(102)
Hence the absolute value of our chosen X is smaller
than the sum of the elements of some row (or column).
Now we can evaluate the sum of the absolute values
of all the elements for each row (or column) and se-
lect the maximum of this sequence of m numbers.
Then we know that for any \ t the absolute value of
\i cannot surpass this sum. We thus obtain the
estimate
*M<SM, (103)
where SM is the maximum among the sums of the
absolute values of all the elements of the rows 1,
2, . . ., n.
It was pointed out before that the actual genera-
tion of the symmetrized matrix A=G*G, which is a
numerically heavy load, is not demanded since all
our operations can be performed with the help of G
and G* alone. But then it becomes necessary to
estimate the largest eigenvalue of A by utilizing G
and G* only, without generating the elements of A.
We assume the general case that G has arbitrary
complex elements and conceive G as the sum of
two Hermitian matrices Gr and Gn', defined by
Or =—— (Cr—br ) (1U4;
(the symbol ~ means conjugate complex). Then
G=G'+iG"
G*=G'-iG". (105)
Hence
G*G=(G')2+(G")2+i(G'G"-G"G'). (106)
Now the largest eigenvalue of a positive definite
Hermitian matrix A can be defined as the largest
possible length of any vector Ab0, where |60| = l-
In order to find this largest length, we let the eq
(106) operate on b0. We thus obtain the estimate
or (107)
43
where \M' is the largest eigenvalue of G' and X^
the largest eigenvalue of G"'. Since X^ and X^
can be estimated by Ger&gorin's theorem, we thus
obtain an upper bound for \M, without using the
elements of the least-squared matrix A.
In our simple numerical example the given matrix
is already symmetric and positive definite. We can
thus operate directly with A. The sums of the
absolute values of each row are 3, 4, 4, 3. Hence
we can choose XM=4 as a safe estimate of the largest
eigenvalue.
We construct the matrix B according to (93),
and extend it by the column bQ=±bo=O, 0, 0, 1.
We choose m=5, and continue the scheme by one
more row to obtain the new residual. The factor
(m+2)2 is in our case 49. Hence the fifth row
has to be multiplied by 4/49 in order to obtain the
approximate solution w5, while the sixth row sQ has
to be multiplied by 1/49 in order to get the residual r6.
0 1 0 0
1 0 1 0
0 1 0 1
0 0 1 0 J
0
0
0
1
go
02
g*
0
0
0
1
4
8
0
0
1
4
9
16
0
1
4
9
16
25
1
4
9
16
25
36
4
9
16
25
36
49
sQ: 0 1 2 2
The last row was obtained by multiplying the row
5 by the matrix B and then subtracting the row 5
(and not 4) twice.
We can test the residual estimate (86) on our
scheme. According to this estimate (m+2)2ew+i
must become smaller than ym+i. If the vector gm,
multiplied by 4/(m+2)2 is a fairly good approxima-
tion of y, then the length of the vector sm+i cannot
surpass 4/(m+2)2 of the length of gm. In our case
(m=5): \g5\ =46.34, while |se|=2.24. Hence
^=0.081633. (108)
If this test fails, it is an indication that our approxi-
mation is far from the correct solution, caused by
the influence of the small eigenvalues, as we will
show presently.
The approximation w5 is obtained by multiplying
row 5 by ^=0.081633. This gives w5=0.65306,
1.30613, 2.04082, 2.93879.
The correct solution is y=0.8, 1.6, 2.4, 3.2.
What did we accomplish with this algorithm?
Let us analyze the situation in the reference system
of the principal axes. Let us plot the eigenvalues
\iy normalized to the range 0 to 1, along the abscissa,
while we plot the components of the right side b°,
associated with a certain \t, as ordinates. In the
language of physics we have a "line spectrum" since
only certain definite "frequencies" X2, namely, the
eigenvalues of A, are represented.
Whatever approximation scheme we may use,
based on iterations, we will always obtain a prelim-
inary solution yk+u which does not satisfy the equa-
tion exactly but generates a new right side in the
form of a residual vector rk+1. Hence quite generally,
for any iterative solution we will have
yk=Pk(Ao)b°, (109)
where Pk(x) is some polynominal in x. Then the
residual rk+i, associated with this solution, becomes
This residual vector is then the new "right side" of
the next approximation.
 4
The result of our approximation can now be de-
scribed as follows, if we view everything from the
reference system of the principal axes. The original
component bt 0, associated with the eigenvalue X?:,
became attenuated by the factor r(X2) where the
function T(X) is defined by
r(x)=Fk+l(x). (Ill)
In these discussions we have considered two kinds
of approximations: the purification technique dealt
with in the present section, and the method of min-
imized iterations, discussed before. Since the purifi-
cation precedes the application of the algorithm
technique given in section 3, let us call it algorithm I,
while the algorithm of section 3 shall be called
algorithm II. The attenuation obtained by these
two kinds of algorithms is based on two very different
principles. We discuss the algorithm I first.
Here we get according to (83):
with
sin2 (m + 2) |
(m + 2)2 s in 2 1
_ • 2!x — sin
 2'
(112)
(113)
The attenuation thus obtained starts with 1 and falls
off with 1/x. The factor T(X) cuts out effectively the
higher frequencies but has little influence on the small
frequencies (small X*). What we accomplish here is
44
that we put the spotlight on the small eigenvalues,
while the large eigenvalues can be eliminated to any
desired degree.
Actually this algorithm serves a double purpose.
We limit the field of vision to a relatively narrow band
of small eigenvalues. Aside from that, however, we
can make the focusing effect of the process increas-
ingly sharper. Let us limit ourselves to the case
m=5, that is, to five iterations of the type described.
We can now take the residual r6 and repeat the proc-
ess, thus obtaining a second "block" of five iterations
The attenuation factor achieved as the result of the
two blocks of iterations is the square of the previous
r(X). Generally, if the process is repeated k times,
the attenuation thus obtained is characterized by
Figure 1 plots r(X), (for ra=5) and the second,
third, and fourth powers of r(X). If our matrix A
contains a very small eigenvalue of the order of
0.0001 say, this very small eigenvalue will not be
able to compete with the larger eigenvalues, except
if the larger eigenvalues are blotted out very strongly.
At first sight we might think that from the stand-
point of such a small X* it makes no great difference
how often we repeated the process since it will
remain in the illuminated part of the spectrum for a
practically unlimited time even if k is large. How-
ever, the situation is quite different if the algorithm I
is conceived as a mere preparation to algorithm II.
Then we are reconciled to the fact that our first
efforts are unable to take out the contribution of
that small eigenvalue. We leave that task to the
second algorithm. But that second algorithm will
operate much more satisfactorily if the large eigen-
values are eliminated with great accuracy. Hence
the advantage of continuing the first algorithm to
several blocks is not so much the increased accuracy
FIGURE 1. Attenuation factors obtained by k blocks of
algorithm I.
of the solution as the proper preparation for the
second process, which will then tackle the problem
of small eigenvalues much more effectively. The
field of vision is perhaps not much reduced. But
the dim light that still spreads over the higher
portion of the spectrum is more and more sharply
eliminated.
The continuation of the (/-algorithm to a second
block can be achieved without any basic interruption
of the operations. After obtaining the residual r6,
we transfer this row to ~B as an additional sixth
column. The fifth column now remains inactive.
Consequently, the squares 4, 9, 16, . . . are now
moved over by one column. The resulting scheme,
now extended to two blocks, and omitting the first
five lines which have been obtained before, looks as
follows:
g[2)
g?
g?
g?
g?
0
1
0
so
0
l
6
19
42
73
1
0
1
0
1
6
20
48
92
150
0
1
0
1
2
11
32
68
120
187
0
0
1
0j
2
10
27
54
91
138
0
0
0
1
0
1
2
2
. . .
4
9
16
25
36
49
12
The successive blocks can be generated continuously
by one mechanized algorithm. If k blocks are
generated, the approximation becomes
49+492^493+ ••• -t-49*
In our numerical example the two contributions and
their sum becomes:
•^gi" = 0.65306 1.30613 2.04082 2.93879
4
492gf = 0.12162 0.24990 0.31154 0.22990
w(2) = 0.77468 1.55603 2.35236 3.16869
(y = 0.8 1.6 2.4 3.2) .
If we perform tha ratio test (108) once more on the
second block, we find
17.944
=
 286.08 =0.0627<0.0816.
45
Hence the inequality (86), multiplied by the factor 4,
can still be verified. We can expect that, as we come
to higher and higher blocks, the ratio test will even-
tually fail. The initial vectors of the successive
blocks become more and more purified of the larger
eigenvalues. As a consequence, the purification
process, which leaves the very small eigenvalues
untouched, becomes less and less effective. Even-
tually the polynomial gm{A) will operate on an
initial vector b^k), which contains only small eigen-
values. We will then approach the extreme case
ag -a (0) 6™- -—ym\}J) ' oQ — °o i
while sm+1 approaches
then gives
Sm4-1
\g.
(m+2)2&«>.
12
= (m+2) 2 - l
The ratio test
that is, 1/4, if m=5. This gives an upper bound for
the ratio test, which cannot be surpassed, no matter
how far the process is continued.
We now come to the analysis of the r(X)-factor
connected with algorithm II (see fig. 2). The
principle by which this process gives good attenua-
tion, is quite different from the previous one. Here
we take heed of the specific nature of the matrix A
and operate in a selective way. The polynomials
Fm+i(\) of this process have the peculiarity that
they attenuate due to the nearness of their zeros to
those X-values which are present in A. These
polynomials take advantage of the fact that the
spectrum to be attenuated is a line spectrum and not
a continuous spectrum. They work efficiently in the
neighborhood of the X* of the matrix but not for
intermediate values. They are thus associated with
the given specific matrix A and are of no use for other
matrices. If we proceed to the polynomial of nth.
order Fn(\), the zeros of this polynomial hit all the
\ t exactly, and thus make the entire residual vanish.
This analysis explains the advantages and the
disadvantages of the second algorithm. The ad-
vantage of the process is its great economy. The
b.iO
b20 X3
' 3 0
' 4 0
^50
FIGURE 2. Attenuation behavior of algorithm II.
exact solution (apart from rounding errors) is obtain-
able in n iterations; this is the minimum number of
steps for generating a polynomial that will have its
zeros at the X* of the matrix A. If the number of
components present in b° is smaller than n, then the
order of Fm(\) is correspondingly lower and the
solution is again obtained in the minimum number of
steps.
The price we have to pay is that the successive
iterations of this process are more complicated than
those of algorithm I. Instead of one new vector, a
pair of vectors has to be generated. Moreover, the
previous recurrence relation, based on the properties
of the ^-polynomials, had fixed coefficients, which
needed no adjustments throughout the procedure.
Here at every step a pair of scalars have to be eval-
uated which are needed for the generation of the new
p, q vectors. The constants of the recurrence rela-
tions have to be readjusted at each new step of the
process.
Another difficulty arises from the inevitable
accumulation of rounding errors. If we want to
maintain a long chain of interlocked operations, we
have to counteract the effect of rounding errors.
This can be done by constant reorthogonalization of
the p vectors which, however, is a lengthly process.
It is preferable not to correct for the rounding errors
but avoid them by breaking the long algorithm into
a sequence of shorter blocks. Then, however, we
lose in convergence and the number of iterations has
to be extended.
The two algorithms together complement each
other. The first algorithm succeeds in purifying the
given vector 6° of all its large eigenvalues. The
spectrum is thus effectively reduced which means
that only a relatively small number of X2 remain
practically present in the final residual. This is now
the point where the second algorithm takes over.
Because of the small number of eigenvectors still
present in 6°, a polynomial of low order will be suf-
ficient for the final elimination of the residual. The
process has thus good convergence and will be finished
after a small number of iterations. The breaking up
of the process into blocks will not be necessary since
the rounding errors will have no time to accumulate
to the point where they endanger the solution. The
small extension of the spectrum tends to reduce the
deorthogonalizing effect of the rounding errors, thus
increasing the length of a block and preventing its
premature termination. The opening of a second
block will thus but seldom be required.
6. Iterative Solution of Nearly Singular
Systems
In practical numerical work we frequently en-
counter nearly singular systems. We shall therefore *
discuss the relative merits of iterative schemes and
other matrix inversion methods with respect to such
systems.
We begin with the extreme case when the deter-
minant of the matrix G and all its minors up to a
certain order n—v vanish exactly, thus reducing the
46
rank of the matrix to n—v. In this case the linear
system (29) is generally not solvable, except if the
right side satisfies certain compatibility conditions.
The reduction of the rank from n to n—v means that
the left side of the system satisfies v independent
linear identities. The compatibility of the system,
which is the necessary and sufficient condition for
its solvability, demands that the same identities shall
be satisfied by the given right sides.
If the compatibility conditions are actually satis-
fied and the system thus solvable, then another
peculiarity arises. The solution is not unique. To
any given solution an arbitrary linear combination
of v independent vectors may be added without
disturbing the validity of the equations.
These theoretical conditions have to be translated
into practical conditions if we want to analyze the
numerical behavior of linear systems which are not
exactly but nearly singular. We can base our analy-
sis on the behavior of the eigenvalues and eigen-
vectors associated with the matrix G.
In the light of eigenvalues the lowering of the
rank of the matrix G from n to n—v means that the
matrix G possesses v vanishing eigenvalues. Such
a matrix operates in an w—^-dimensional subspace
only and blots out all the v dimensions which are
orthogonal to this subspace. Hence the linear set
(29) can only be solvable if the right side g is free
of all those dimensions which the matrix rejects. At
the same time, the solution y may contain any vector
which belongs totally to the rejected portion of the
^-dimensional space, since the operation Gy extin-
guishes this vector and thus does not disturb the
balance of the equation.
If the matrix G is not exactly but nearly singular
in v directions, this means that v of the eigenvalues,
although not exactly zero, are nevertheless very
small compared with the other eigenvalues. We
can associate such a matrix geometrically with a
strongly skew angular frame of reference which
almost collapses into a lower dimensional space. In
this interpretation we conceive the successive
columns of G as n basic vectors
Vu V2, V,, (114)
which establish an w-dimensional set of axes. The
linear system Gx=g now assumes the following
significance:
. . ,+Vnxn=g. (115)
This means that the given vector g shall be analyzed
in the reference system of the base vectors Vf.
Now the skew-angular character of a frame of axes
can be properly described by evaluating the volume
included by these axes. This again is nothing but
the determinant \G\ of the matrix G. The smaller
the included volume, the more skew-angular is the
system. However, this measure is adequate only
if the various axes of our reference system are properly
scaled. Otherwise even an orthogonal set of axes can
have a very small determinant, caused not by the
inclination of the axes, but by uneven scaling.
This uneven scaling can always be eliminated by
the following linear transformation of the variables
xt:
xi=Mry<- (1 1 6>
Then the original equation (115) now appears in the
following form
where
and thus
U1y1+U2y2+ . . .
TT-Vt ,
^ \vt
\Ui\ = l,
nf[
(117)
(118)
(119)
In matrix language the transformation (116) means
that the columns of the matrix G=(gik) are multiplied
by 8
_ 1
7i
~ '~ (120)
and the right side by
which transforms the vector # into
7i „
(121)
(122)
The consequence of this transformation on the
symmetrized matrix A is that all the diagonal ele-
ments become 1, while all the nondiagonal elements
range between ± 1. This is of great advantage from
the viewpoint of numerical operations [15].
If the original matrix is already given as a positive
definite, symmetric matrix A, then the scaling of the
matrix is performed by the transformation
(123)
We multiply all the rows, and then all the columns
by ll-y/du, which makes the resulting diagonal ele-
ments once more equal to 1. Moreover, the vector
g is transformed into the vector b by the transforma-
tion
(124)
Finally, the length of this vector is normalized to 1
by putting
{«=!.%« (125)
7 U
0Q TT7* (126)
8
 The conditions (120) and (121) need not be met with any high degree of
precision. The multipliers yt- can be rounded off to two significant figures.
47
We now consider the vector equation (117). The
smallness of the determinant \G\ associated with
the rescaled system now actually measures the
strongly skew-angular nature of our reference sys-
tem. Nevertheless, the linear equation (117) can
be considered as well adjusted if the right side g0
falls inside the narrow space included by the basic
vectors Ut. This condition is a natural counterpart
of the compatibility conditions set up for the case
that the vectors eventually collapse completely into
a lower dimensional space. If the right side lies
constantly inside the space included by the basic
vectors, then it remains coplanar with those vectors
even in the limit when the vectors do not include
any finite volume any more. Practical compatibility
includes thus the limiting case of theoretical com-
patibility. Let us examine, in what form this condi-
tion of "insidedness" comes into evidence in relation
to the least-squared matrix A and its right side So-
Let us project the vector b0 on the principal axes
of A. We obtain the components pi0. Let us divide
each one of these components by the eigenvalue
\i associated with that axis. This gives the sequence
01 0 $20
"xT'x? (127)
We pick out the absolutely largest of these num-
bers and consider
PiO (128)
as the measure of the adjustment of the given sys-
tem. No matter how small the determinant of A is,
the linear equation Ay=b can be considered as
solvable practically if /x is a reasonably small number.
The measure M does not refer in any way to the
condition of A itself. It measures the relation of
the right side of the system to the left side. The
meaning of a reasonably small n is that the near
identities which exist on the left side, lead to near
identities also on the right side.
As a consequence of (117) we have
(129)
Let us collapse the given frame of axes more and
more into a lower dimensional system, but keep /*
bounded. Then in the limit a certain number v of
\t vanish. However, as a consequence of (129), the
corresponding pi0 vanish too. This is exactly the
compatibility requirement of a singular system. The
measure IJL is thus a reasonable measure of the adjust-
ment of the given linear system.
If we are able to invert a matrix exactly, then the
smallness or largeness of /* is of no importance. If,
however, approximation techniques are employed,
then it is natural to restrict ourselves to well ad-
justed systems whose \i is not too large. We cannot
expect that any approximation procedure shall re-
main successful if M becomes arbitrarily large, since
in that case a minute change in the right side may
cause a large error in the solution. For the same rea-
son we can add at once that physical systems, whose
right sides are given as the result of observations,
must satisfy the condition of not too large p, in
order to allow any valid conclusions.
We will thus restrict ourselves to the solution of
systems that can be considered as "well adjusted" in
the sense of prescribing for /* a not too large upper
bound. The length of our approximation procedure
will depend on the magnitude of /x. If /z is too large,
then we have to abandon the use of iteration tech-
niques, or we have to employ the full technique of
minimized iterations with all its precautions, con-
tinuing to the very end of n iterations.
Singular systems, however, show a second pecu-
liarity, namely, the indeterminate character of the
solution. Let us examine what the corresponding
phenomenon is in the case of nearly singular, that is,
strongly skew-angular systems. The corresponding
phenomenon is that very small changes on the right
side cause much larger changes in the solution. The
danger exists solely in the direction of the small
eigenvalues, and is caused by the fact that the com-
ponent pto of the right side in the direction of the iih.
eigenvector has to be divided by X* in order to get yt 0.
This phenomenon is of considerable significance if
we are interested in the solution of linear systems
which arise from physical measurements. Let us
assume that we know in advance from physical
reasons that the given system is well adjusted, that
is, that \i is reasonably small, compared with the
accuracy of the measurements. Then an appearance
of a large yt 0 on account of dividing by a small \t
must be caused by experimental errors and should
be discarded. In such a situation the use of an
iteration technique for finding the solution is supe-
rior to the exact solution. The exact solution, ob-
tained by matrix inversion, would be of little help,
since it would not separate the influence of the
errors in the direction of the small X*. On the other
hand, if we use the above advocated method of
taking out first the contribution of the large eigen-
values by the ^-polynomials, then we can actually
separate the desirable part of the solution from the
undesirable part. The first approximation, which
leaves the small eigenvalues practically untouched,
does not offer any difficulty and can stand as it is.
Now we come to the second algorithm, which de-
termines the contribution of the small eigenvalues.
If in this successive approximation process a correc-
tion appears, the length of which is more than /JL
times the length of the remaining residual, we know
that we should stop at this point, since this contri-
bution comes from the errors of the data.
This analysis indicates that in the case of strongly
skew-angular but well-adjusted physical systems the
separation of the two algorithms has more than tech-
nical significance. It makes smoothing of the data
possible by discarding large errors in the solution
caused by small observational errors in the direction
of the small eigenvectors.9 The iteration technique
gives in such a case a more adequate solution than the
mathematically exact solution obtained by matrix
9
 The expression "small eigenvector" is used in the sense of "an eigenvector
associated with a small eigenvalue."
48
inversion because it capitalizes on the sluggishness
with which the small eigenvalues come into play.
The smallest eigenvalues, which essentially test the
compatibility of the system, appear last. Now the
given system is such that this test of compatibility
is not needed since we know in advance from physical
considerations that the system is well adjusted.
By omitting the contents of the last equations we
take advantage of the good part of our measurements
and reject the errors. While the uncertainty of the
result is not completely eliminated by this procedure,
it is nevertheless essentially reduced in magnitude.
7. Eigenvalue Analysis
The underlying principles of the two algorithms
discussed in the previous sections can also be
employed in the problem of finding the eigenvalues
and eigenvectors of a matrix. The general p, q,p*, <z*
algorithm gives a complete analysis of the matrix,
namely it gives all its eigenvalues and eigenvectors.
If performed with the proper care, this method gives
satisfactory results even when the eigenvalues are
closely grouped [16].
However, in many situations we are not interested
in the complete set of eigenvalues and eigenvectors.
We would welcome a technique which puts the spot-
light on a, Jew eigenvectors only, or we might want to
single out just one particular eigenvalue and its
associated eigenvector, for example, the smallest one.
The method now to be outlined should prove useful
in connection with such problems.
The preliminary purification of bQ served the
purpose of increasing the convergence of the final
algorithm by properly preparing the vector on which
it operates. We were able to effectively eliminate
all components of the original vector except those
associated with the small eigenvalues.
After the purification, the spotlight is put on the
small eigenvalues; we will therefore first obtain the
small eigenvalues and the associated eigenvectors
with great accuracy, in marked contrast to the
Sylvester-Cayley asymptotic procedure which first
obtains the absolutely largest eigenvalue and its
associated eigenvector.
In "flutter" problems we are usually interested in
the smallest eigenvalues of the given matrix. In
order to apply the asymptotic power method, we
first invert the matrix, thus transforming the smallest
eigenvalues to the largest eigenvalues of the new
matrix. If we possess a direct method for the
evaluation of the smallest eigenvalues, we might
dispense with the preliminary inversion of the
matrix, thus saving a great deal in numerical effort.
However, our previous purification procedure,
based on the properties of the Chebyshev polyno-
mials, is strictly limited to nonnegative matrices and
cannot be generalized to arbitrary complex eigen-
values, because the outstanding properties of the
Chebyshev polynomials are not preserved in the
complex range. We will now see that the general
eigenvalue problem of an arbitrary complex matrix
can always be formulated in such a way that it
becomes transformed into the determination of the
smallest eigenvalue and eigenvector of a nonnegative
Hermitian matrix.
Let us first observe that all our previous procedures
remain valid if we apply them to a nonnegative
Hermitian matrix
A*=A (130)
where A* is the transpose and A is the conjugate of
A. The quadratic form associated with a Hermitian
matrix is still real.
We consider the solution of the linear equation
Oy=g (131)
where the matrix G is a general matrix with complex
elements; the vector g has likewise complex elements.
We multiply on both sides by G* and obtain once
more the standard form
with
and
Ay=b
4=#* G
(132)
(133)
(134)
The matrix A defined by (133) is not only Hermitian
but also nonnegative.
All the characteristic features of the previous
algorithms remain the same. The largest eigenvalue
XM can once more be estimated by Ger&gorin's
theorem. The (/-algorithm carries over without any
modification, although all the vectors involved have
now complex elements.
The p, q algorithm can also be carried over with
the only modification that the adjoint vectors p*,
g* are now not identical with p, q but with p, q.
Hence the basic scalars hi and h[ of the algorithm
have to be defined as follows:
(135)
We see from these relations that the hi are again all
positive; moreover, the h'{ are all real. Actually,
the theory of the basic algorithm [14], section 6,
allows a further conclusion. The significance of the
ht and h't within the framework of this algorithm
reveals that for nonnegative Hermitian matrices not
only the ht but also the ti{ remain positive. Hence,
in spite of the complex nature of the vector elements,
the reality (and even positiveness) of the basic
scalars remains preserved.
Let us now consider the eigenvalue problem con-
nected with an arbitrary nonsymmetric and complex
matrix K:
We put
(2ST-X/) y=0.
G=K-\I,
(136)
(137)
207064—52 49
and write the equation
Gy=0
in the "least square" form
Gf*Gy=O.
This introduces the Hermitian matrix
(138)
(139)
A=G*G=K*K-(\K+\K*)+\\I. (140)
There is generally no predictable relation between
the eigenvalues of an arbitrary matrix and its "least-
square" form. Yet there is one exception, namely
the eigenvalue zero. The eigenvalue zero of G
carries over to the Hermitian matrix A. Let us now
assume that we want to operate solely with the
Hermitian matrix A and abandon the original matrix
K completely. Then we can still obtain all the
eigenvalues of K by determining all those values of X
in (140), which make the smallest eigenvalue of A
equal to zero.
We now see how we can make good use of a method
which discriminates in favor of the small eigenvalues.
Such a method can be utilized to put the emphasis
on one particular eigenvector, instead of an arbitrary
mixture of eigenvectors.
Generally, if we start the p, q algorithm with some
arbitrary b0, b* vector, we have no control over the
sequence in which the successive eigenvectors and
eigenvalues will be approximated. The particular
eigenvector in question might appear quite late in
the process. Let us assume, however, that we suc-
ceed in purifying the trial vector 60, b$ of most of
its components and emphasize strongly one particular
eigenvector in which we are interested.
Such conditions actually arise if we possess a first
approximation Xo to the desired eigenvalue X. We
can now form the Hermitian matrix (140) with this
particular X=X0 and let us assume that we can
obtain its smallest eigenvector. If Xo were the cor-
rect value for X, the smallest eigenvalue would be
zero and the associated eigenvector the correct
solution. Since Xo is only an approximation, we
still get a good vector which has a strong component
in the desired direction. This is enough for a good
start of the algorithm II.
However, our work is only half done. Since the
original matrix is not symmetric, we need the com-
plete p, q, p*, g* process. That process starts with
b0 and the adjoint b$. So far we have obtained b0
only. In order to obtain a well-suited &?, we pro-
ceed as follows. We consider the adjoint solution
(K*-7J)y*=0, (141)
which in "least-square" form leads to the new matrix
A=GG*=KK*-(\K*+\K) + \\I. (142)
The third part of this matrix is identical with the
previous third part; the second part differs from the
previous second part only in the change of i to —i.
The first part, however, is an entirely independent
new matrix, formed by multiplying the rows of K by
its rows, while previously the columns were multi-
plied by columns.
The smallest eigenvector of this new Hermitian
matrix A can now be introduced as a well-purified
b* which will strongly emphasize the desired eigen-
vector. Then two steps of the p, q algorithm will
give an improved eigenvector and a much improved
value for X. This method resembles Newton's
method of obtaining the root of an algebraic equa-
tion if a near root is given.
The problem is thus reduced to the problem of
finding the smallest eigenvector of a Hermitian
matrix. Our aim is to purify a trial vector b0 of all
its large eigenvalues, reducing it to a new vector in
which the smallest eigenvector is strongly empha-
sized.
This was accomplished before in form of the
residual of the previous ^-process. There the atten-
uation obtained was characterized by the kth
power of a certain function T(X), if k blocks of the
process were employed. As figure 1 illustrates,
increasingly strong attenuations are obtainable even
with a few blocks of five iterations. Since in our
case the solution y is of no importance but only the
residual, we can generate that residual immediately
by utilizing the Fm+1(x) polynomials. We multi-
ply by (ra+2)2, in order to get integer coefficients.
Hence we want to operate with the polynomials
(143)
These polynomials once more satisfy a simple
recurrence relation:
(144)
(145)
which agaiji leads to the previous algorithm
with the only difference that the surplus column of
the vectors fm now remains 2 throughout the process:
f f 6) (146)
The matrix B is once more defined as before, see
(93) and (97).
The termination of a block and changing over to
the next block now occurs by the following simple
procedure. We go on uninterruptedly with the
recurrences, until the last vector /m+1 is reached.
This vector is transferred to B as the new surplus
column which will be in operation during the second
block. Moreover, the last vector fm+1 becomes the
initial vector fk2) of the second block. Then the
algorithm starts over again until the new block is
finished which occurs at f%+1} and so on.
In order to demonstrate the operation of this
algorithm, we once more make use of the previous
simple matrix of fourth order and choose once more
50
m = 5 . Two blocks of six iterations are used in
accordance with our previous ^-algorithm, but now
generating directly the residuals. As trial vector
we could use the vector 1, 1, 1, 1. However, in
order not to capitalize unduly on the symmetry of
our highly simplified matrix, the trial vector is
chosen as 1, 1, 1, 0. The resulting work scheme
looks as follows:
/o
/l
h
h
u
h
/•
/i 2 )
0
1
0
.0
1
3
: 5
6
6
: 6
5
5
17
/f: 35
fP : 46
/f : 48
ft2) : 42
/?>. : 30
1
0
1
0
1
4
7
9
10
9
7
7
25
53
73
78
68
46
0
1
0
1
1
3
6
9
10
9
6
6
22
49
71
79
68
43
0 ^
0
1
o>
0
1
3
5
6
5
3
3
12
28
43
49
42
25
1
1
1
0
2
2
2
2
2
2
5
7
6
3
2
2
2
2
2
2
For checking purposes we list the first six fm(x) poly-
nomials:
= 16 - 80x + 128z2 - 64z3
(x) = 25 - 200z + 560z2 - 640z3 + 256x4
x) = 3 6 - 4 2 0 x + 1792z2-3456z3 + 3072z4- 1024z5
- 13440z3 + 19712z4-14366z5+4096z6
The last row of the scheme yields the vector that
is strongly graded in favor of the small eigenvalues.
In our numerical example the smallest eigenvalue of
the given maxtrix A is known to be
2 ( l - cos36° ) = 0.3819660.
The associated eigenvector has the components
1, 2 cos 36° , 2 cos 36° , 1
— 1, 1.6180340, 1.6180340, 1.
If the length of this vector is normalized to 1, and
the same is done with /^2), we obtain the following
comparison:
/(2)
T ^ J T = . 4 O 4 8 8 8 , .620828, .580340, .337407
\ui\
, .601501, .601501, .371748.
We notice that the approximation is not very close.
However, our aim is merely to provide a good start
to the second algorithm. If we perform two cycles,
the cycles 0 and 1, of the p, q algorithm, we obtain
the following basic scalars:
Po=-0.38506375
<ro= -0.0080299090
P l=-1.37489569.
the solutionThe first-order polynomial gives
X=—
 Po=O.385O64.
This is already a close approximation of the correct
X, which is X=0.3819660. The second-order poly-
nomial gives the quadratic equation
X2— (<ro+A>+P<)X+Po<ri = O
X2-1.76798935X+0.52942249 = 0
whose roots are Xi=0.38198259, X?= 1.38600677.
The approximation to the true Xi is already re-
markably close, the error being only 1.7 units in
the fifth decimal place. Moreover, the second
root is a very good first approximation to the next
smallest characteristic value, which is 2(1 —cos
72°) = 1.3819660.
In addition, the first two cycles allow a correction
of the first principal axis, according to the formula
This gives, if again the length is normalized to 1:
T=.3713944, .6025945, .6003686, .3721606.
The length of the error vector is 1.66-10"3. A
strong improvement compared with the error of
/e00, which was 5.57.10"2.
51
This example demonstrates that we have no diffi-
culty in improving a given first approximation Xo of an
eigenvalue; moreover, we obtain a good approxima-
tion to the eigenvector associated with that eigen-
value. Hence the problem is reduced to the question
of obtaining a good first approximation of a certain
desired X. Usually it is the X of smallest absolute
value in which we are primarily interested.
We can now proceed as follows. For a first crude
approximation we put X=0 and apply the purification
process to the Hermitian matrices A and A. The two
vectors thus obtained may be too crude to be useful
as starting vectors of the p, q algorithm. It may be
preferable to improve this approximation by a least
squares method now to be explained. If we had the
right y, we could obtain the right X from the condition
(136). Since we do not possess the right y, we can
still obtain a preliminary X by minimizing the square
of the length, that is, kk, of the vector k= (K—\I)y.
This gives one complex X. Another complex X=X* is
obtainable from the adjoint problem k*=(K*—\*I)y*
again minimizing the square of the length of this
vector. While for the correct X the two values X and
X* should coincide, this is not necessarily true for the
approximations. We now use the approximation X
as the Xo of the process above for obtaining b0 and X*
as the Xo for obtaining 6* •
If we have not been successful in our start and ob-
tained too slow a convergence in the ensuing #, q proc-
ess, we can at any point of the process speed up the
convergence by applying the purification procedure
again, but now using for Xo the absolutely smallest
root of the last characteristic equation.
The following interesting problem offers itself. Let
X=X0 be a good approximation of an eigenvalue of the
arbitrary matrix K. Then forming the Hermitian
matrices (140) and (142) with this Xo and obtaining
the smallest eigenvectors of these matrices, these vec-
tors will have a strong component in the direction of
the principal axis u, u* of the matrix K, associated
with that particular X. The first cycle of the p, q
algorithm will then bring us closer to the true value
of X, and two cycles will improve further and give a
good correction to the vector u, u*. But what can
we say about the second root of the characteristic
equation? Can we assume—in analogy with the be-
havior of symmetric matrices—that our initial vector
is not only close but also well graded, that is, that the
second root will be a good approximation of the X that
is nearest in the complex plane to the first X? This
question requires further discussion which cannot be
given here.
In this section we have merely sketched a method
for obtaining the eigenvalues of an arbitrary complex
matrix. However, no extensive numerical experi-
ments have been performed so far. The writer hopes
to go into further details about the method at some
future time.
8. Summary
The present investigation advocates a combination
of two procedures for the solution of large scale linear
systems of equations. The first procedure evaluates
the contribution of the large eigenvalues, the second
the contribution of the small eigenvalues. The first
algorithmJhas the advantage that it operates with a
constant routine which does not change throughout
the process. The second algorithm is more lengthy
and requires corrections to counteract the accumula-
tion of rounding errors. Hence it is of advantage to
cut down the length of this algorithm to a minimum;
this is achieved by the application of the preceding
algorithm.
The final work scheme can be systematized into
three distinct jphases:
(a) Rescaling of the columns of the given matrix
G by normalizing the length of each column to
approximately 1. This makes the diagonal elements
of the associated Hermitian matrix A nearly equal to
1, and all the nondiagonal elements numerically less
than 1.
(b) Purification of the given right side 60 of all
its components in the direction of the large eigen-
vectors of-A; a two-block scheme of five iterations each
eliminates practically 90 percent of the X spectrum.
An additional block of five iterations eliminates about
94 percent of the spectrum. In this algorithm every
iteration generates one new vector, by a recurrence
scheme which has fixed coefficients involving the last
vector and its penultimate.
(c) The remaining components in the direction of
the small eigenvalues are eliminated by an algorithm
which is again based on recurrences. However,
every cycle now requires the generation of a pair of
vectors, called p and q} apart from the matrix multi-
plication applied to q. Thus every cycle consists of
three vectors. The recurrence relations involve the
generation of two scalars in each cycle. In absence
of rounding errors the first vectors (called pt) of
every cycle form an orthogonal set of vectors, while
the second and third vectors are biorthogonal to each
other. In view of the deorthogonalizing effect of
rounding errors we check from time to time the
orthogonality of the vectors obtained and interrupt
the scheme if the orthogonality is no longer sufficiently
strong. We then form the residual and start an
independent second block of approximations. The
solution is obtained as a given linear combination of
the ^-vectors and can be generated along with the
other vectors, by constantly adding one more
correction.
This method is not recommended when the princi-
pal aim is the evaluation of the elements of the
inverse matrix, because it depends primarily on con-
sidering the matrix together with the given right side
as a unified system. It is true that the method of
minimized iterations can be adapted to arbitrary
right sides (which is equivalent to inverting a matrix). %
This is so in spite of the fact that the basic vectors are
obtained with the aid of one specific right side. How-
ever, the convergence of the process changes greatly
with the given right side. For an arbitrary right side
we have to assume that the process does not end
before n steps. This requires that we have to gener-
ate a complete set of basic vectors. But then con-
52
stant reorthogonalization is required which is a
lengthy procedure. The simple successive orthog-
onalization of the columns of the matrix, which also
gives the inverted matrix and does not require any
matrix multiplication, is preferable for this purpose.
In a given problem the inverted matrix will not
always be required. The number of right sides with
which we have to operate may not be too large and
thus we may prefer to repeat the algorithm for every
right side, particularly if the number of iterations
required for the given accuracy happens to be much
less than n. For example, we may imagine the
situation that a given 50X50 matrix is not too skew-
angular, to the extent that the symmetrized matrix
A has no eigenvalues below 0.1 of the maximum
eigenvalue. In this case a simple recurrence routine
of 10 iterations will give the solution with sufficient
accuracy, while the inversion of the matrix may
require a much more elaborate calculation. A fur-
ther advantage arises in the case of strongly skew-
angular bu t" well-adjusted" physical systems. Here
it is of definite advantage to separate the contribution
of the large from that of the small eigenvalues
because we can thus ameliorate the damaging in-
fluence of observational errors. These errors are
greatly magnified in the theoretically exact mathe-
matical solution, while in the iteration procedure
they come into evidence only in the latest phase of
the calculations, and that phase can be discarded.
The literature on the iterative solution of linear
equations is very extensive; (see [8] for the oldei liter-
ature, and [2] and [1] for the newer literature on the
subject). During the last few years many itera-
tive schemes have been investigated. Among those
developed at the National Bureau of Standards the
gradient method of Hestenes and its modifications
[11, 17] deserve particular attention, together with
the asymptotic acceleration technique of Forsythe
and Motzkin [7]. There is also the Monte Carlo
method of Forsythe and Leibler [6]. The latest
publication of Hestenes [10] and of Stiefel [18] is
closely related to the p, q algorithm of the present
paper, although developed independently and from
different considerations.
The present investigation is based on years of
research concerning the behavior of linear systems,
starting with the author's consulting work for the
Physical Research Unit of the Boeing Airplane Com-
pany, and continued under the sponsorship of the
National Bureau of Standards. The author is in-
debted to Miss Lillian Forthal for her excellent as-
sistance in the extensive numerical experiments that
accompanied the various phases of theoretical deduc-
tions. The author is likewise indebted to the ad-
ministration of the Institute for Numerical Analysis
and the Office of Naval Research for the generous
support of his scientific activities.
9. References
[1] W. E. Arnoldi, Quart. Applied Math. 9, 17 to 30 (1951).
[2] E. Bodewig, Koninkl. Nederland Akad. Wetenschap
Proc. 50, 930 to 941, 1104 to 1116, 1285 to 1295 (1947);
51, 53 to 64, 211 to 219 (1948).
[3] A. Brauer, Duke J. 13, 387 to 395 (1946).
[4] D. A. Flanders and G. Shortly, J. Applied Phys. 21, 1326
to 1332 (1950).
[5] G. E. Forsythe, Classification and bibliography of meth-
ods of solving linear equations.
[6] G. E. Forsythe and R. A. Leibler, MTAC 4, 127 to 129
(1950).
[7] G. E. Forsythe and T. S. Motzkin, On a gradient method
of solving linear equations; multilithed outline at
National Bureau of Standards, Los Angeles, Calif.
[8] R. A. Frazer, W. J. Duncan and A. R. Collar, Elementary
Matrizes, (Cambridge University Press, 1938); (Mac-
Millan, New York, N. Y. 1947).
[9] S. Gersgorin, Izvest. Akad. Nauk SSSR 7, 672 to 675
(1931).
[10] M. R. Hestenes, Iterative methods for solving linear
equations, NAML Report 52-9.
[11] M. R. Hestenes and M. L. Stein, The solution of linear
equations by minimization.
[12] A. S. Householder, Am. Math. Monthly 57, 453 to 459
(1950).
[13] C. Lanczos, J. Math. Phys. 17, 123 to 199 (1938).
[14] C. Lanczos, J. Research NBS 45, 255 to 282 (1950) RP
2133.
[15] J. v. Neumann and H. H. Goldstine, Bui. Am. Math.
Soc. 53, 1021 to 1099 (1947).
[16] J. B. Rosser, M. R. Hestenes, W. Karush, and C. Lanczos,
J. Research NBS 47, 291 (1951) RP2256.
[17] M. L. Stein, Gradient methods in the solution of systems
of linear equations, NAML Report 52-7.
[18] E. Stiefel, Z. ang. Math. Phys. (Zurich, Techn. Hoch-
schule) 3, 1 to 33 (1952).
[19] G. Szego, Orthogonal Polynomials (Am. Math. Soc,
New York, N. Y. 1939).
[20] O. Taussky, Am. Math. Monthly 56, 672 to 675 (1949).
Los ANGELES, September 28, 1951.
o
53
