ADAPTIVE POISSON DISORDER PROBLEM
ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Abstract. We study the quickest detection problem of a sudden change in the arrival
rate of a Poisson process from a known value to an unknown and unobservable value at an
unknown and unobservable disorder time. Our objective is to design an alarm time which
is adapted to the history of the arrival process and detects the disorder time as soon as
possible.
In previous solvable versions of the Poisson disorder problem, the arrival rate after the
disorder has been assumed a known constant. In reality, however, we may at most have some
prior information on the likely values of the new arrival rate before the disorder actually
happens, and insufficient estimates of the new rate after the disorder happens. Consequently,
we assume in this paper that the new arrival rate after the disorder is a random variable.
The detection problem is shown to admit a finite-dimensional Markovian sufficient statis-
tic if the new rate has a discrete distribution with finitely-many atoms. Furthermore, the
detection problem is cast as a discounted optimal stopping problem with running cost for a
finite-dimensional piecewise-deterministic Markov process.
This optimal stopping problem is studied in detail in the special case where the new
arrival rate has Bernoulli distribution. This is a non-trivial optimal stopping problem for a
two-dimensional piecewise-deterministic Markov process driven by the same point process.
Using a suitable single-jump operator, we solve it explicitly, describe the analytic properties
of the value function and the stopping region, and present methods for their numerical
calculation. We provide a concrete example where the value function does not satisfy the
smooth-fit principle on a proper subset of the connected, continuously differentiable optimal
stopping boundary, whereas it does on the rest.
Contents
1. Introduction and Synopsis 2
Part 1. ANALYSIS: PROBLEM DESCRIPTION, MODEL, AND
APPROXIMATION 11
Date: February 23, 2005.
2000 Mathematics Subject Classification. Primary 62L10; Secondary 62L15, 62C10, 60G40.
Key words and phrases. Poisson disorder problem, quickest detection, optimal stopping.
1
2 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
2. Problem description 11
3. Sufficient statistics for the adaptive Poisson disorder problem 12
4. Poisson disorder problem with a Bernoulli post-disorder arrival rate 16
4.1. An optimal stopping problem for the quickest detection of the Poisson disorder 18
4.2. The sample-paths of the sufficient-statistic process Φ˜ = (Φ˜(0), Φ˜(1)) 19
4.3. Case I: A “large” disorder arrival rate 20
4.4. Case II: A “small” disorder arrival rate 20
5. A family of related optimal stopping problems 21
6. A bound on the alarm time 26
7. Appendix: proofs of selected results in Part 1 30
Part 2. SYNTHESIS: SOLUTION AND NUMERICAL METHODS 37
8. The solution 37
9. The structure of the stopping regions 38
10. The boundaries of the stopping regions 44
10.1. The entrance boundary ∂Γen+1 47
10.2. The exit boundary ∂Γxn+1 48
11. Case I revisited: efficient methods for “large” post-disorder arrival rates 49
12. The smoothness of the value functions and the stopping boundaries 53
12.1. The interplay between the exit and entrance boundaries 62
12.2. The regularity of the value functions and the optimal stopping boundaries
when the disorder arrival rate λ is “large” 65
12.3. Failure of the smooth-fit principle: a concrete example 69
13. Appendix: proofs of selected results in Part 2 75
References 83
1. Introduction and Synopsis
Suppose that arrivals of certain events constitute a Poisson process N = {Nt : t ≥ 0} with
a known rate µ > 0. At some time θ, the arrival rate suddenly changes from µ to Λ. Both
the disorder time θ and the post-disorder arrival rate Λ of the Poisson process are unknown
and unobservable. Our problem is to find an alarm time τ which depends only on the past
ADAPTIVE POISSON DISORDER PROBLEM 3
and the present observations of the process N , and detects the disorder time θ as soon as
possible.
More precisely, we shall assume that θ and Λ are random variables on some probability
space (Ω,H,P), on which the process N is also defined; the variables θ, Λ are independent
of each other and of the process N . An alarm time is a stopping time τ of the history of the
process N . We shall try to choose such a stopping time so as to minimize the Bayes risk
P {τ < θ}+ c E(τ − θ)+,(1.1)
namely, the sum of the frequency P{τ < θ} of the false alarms and the expected cost
c E(τ − θ)+ of the detection delay.
We shall assume that the post-disorder ariival rate Λ has some general prior distribution
ν(·). Similarly, the disorder time θ will be assumed to have an exponential distribution
conditionally on that the disorder has not happened yet, i.e., for some pi ∈ [0, 1) and λ > 0
P{θ = 0} = pi and P{θ > t | θ > 0} = e−λt, t ≥ 0.(1.2)
The Poisson disorder problem with a known post-disorder rate (namely, Λ equals a known
constant with probability one) was studied first by Galchuk and Rozovskii (1971) and was
solved completely by Peskir and Shiryaev (2002). In the meantime, Davis (1976) noticed
that several forms of Bayes risks including (1.1) admit similar solutions. He called this class
of problems standard Poisson disorder problems, and found a partial solution. Recently,
Bayraktar and Dayanik (2003) solved the Poisson disorder problem when the detection de-
lay is penalized exponentially. Bayraktar, Dayanik and Karatzas (2004b) showed that the
exponential detection delay penalty in fact leads to another variant of standard Poisson
disorder problems if the “standards” suggested by M. Davis are restated under a suitable
reference probability measure. It was also shown that use of a suitable reference proba-
bility measure reduces the dimension of the Markovian sufficient statistic for the detection
problem, and the solution of the standard Poisson disorder problem was described fully.
We believe that unknown and unobservable post-disorder arrival rate Λ captures quite well
real-life applications of change-point detection theory. Before the onset of the new regime,
past experience may help us at most to fit an apriori distribution ν(·) on the likely values
of the new arrival rate of N after the disorder. Even after the disorder happens, we may
not have enough observations to get a reliable statistical estimate of the post-disorder rate.
Indeed, since a good alarm is expected to sound as soon as the disorder happens, we may
have very few observations of N sampled from the new regime since the disorder.
4 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Let us highlight our approach to the problem and our main results. We show that the
most general such detection problem is equivalent, under a reference probability measure,
to a discounted optimal stopping problem with a running cost for an infinite-dimensional
Markovian sufficient statistic. However, the dimension becomes finite as soon as the prior
probability distribution ν(·) of the post-disorder arrival rate Λ charges only a finite number
of atoms. This class of problems is of great interest since, in many applications, we have
typically an empirical distribution of the post-disorder arrival rate, constructed either from
finite past data or from expert opinions on the most significant likely values.
We then study in detail the case where the new arrival rate after the disorder is expected
either to increase or to decrease by the same amount. The detection problem turns in this
case into an optimal stopping problem for a two-dimensional piecewise-deterministic Markov
process, driven by the same point process. We solve this optimal stopping problem fully
by describing ε-optimal and optimal stopping times and identifing explicitly the non-trivial
shape of the optimal continuation region.
The common approach to an optimal stopping problem for a continuous-time Markov
process is to reformulate it as a free-boundary problem in terms of the infinitesimal gener-
ator of the Markov process. The free-boundary problems sometimes turn out to be quite
hard, even in one dimension; see, for example, Galchuk and Rozovskii (1971), Peskir and
Shiryaev (2002), Bayraktar and Dayanik (2003). Here the infinitesimal operator gets com-
plicated further and becomes a singular partial differential-delay operator. Moreover, it is
a non-trivial task, even in two dimensions, to guess the location, shape, and smoothness of
the free-boundary separating the continuation and stopping regions, as well as the behavior
of the value function along the boundary.
Instead, we follow a direct approach and work with integral operators rather than dif-
ferential operators. As in Gugerli (1986) and Davis (1993), we use a suitable single-jump
operator to strip the jumps off the original two-dimensional piecewise-deterministic Markov
process and turn the original optimal stopping problem into a sequence of optimal stopping
problems for a deterministic process with continuous paths. Using direct arguments, we are
able to infer from the properties of the single-jump operator the location and shape of the
optimal continuation region, as well as the smoothness of the switching boundary and the
value function.
The single-jump operator also provides a straightforward numerical method for calculating
the value function and the continuation region. The deterministic process obtained after
removing the jumps from the original Markov process has two fundamentally different types
ADAPTIVE POISSON DISORDER PROBLEM 5
of behavior. We tailor the naive numerical method to each case, by exploiting the behavior
of the paths.
We also raise the question when the value function should be a classical solution of the
relevant free-boundary problem. For a large range of configurations of parameters, both the
value function and the boundary of the continuation region turn out to be continuously dif-
ferentiable, and one may also choose to use finite-difference methods for differential-difference
equations to solve the problem numerically. For a few other cases, we cannot qualify com-
pletely the degree of the smoothness of the value function. Viscosity approaches or some
other techniques of non-smooth analysis are very likely to fill the gap, but we do not pursue
this direction here. We report one concrete example on “partial” failure of the smooth-fit
principle: in certain cases, the value function is continuously differentiable everywhere on
the state space except on a proper subset of the connected and continuously differentiable
optimal stopping boundary.
Synopsis. In Section 2, we present the model and the precise statement of the problem. A
Poisson process N with arrival rate µ and two independent random variables θ and Λ with
given prior probability distributions are introduced on a suitable probability space (Ω,H,P0).
Under a new probability measure P obtained from P0 by an absolutely continuous change
of measure, (i) the process N becomes a Poisson process whose arrival rate changes from
constant µ to the random Λ at time θ, and (ii) the random variables Λ and θ are independent
and have the same distributions as those under P0.
Working under the reference probability measure P0 turns out to be convenient. In Sec-
tion 3.1, the generalized Bayes theorem gives the new form
Rτ (pi) = 1− pi + c(1− pi)E0
[∫ τ
0
e−λt
(
Φ
(0)
t −
λ
c
)
dt
]
, τ ∈ S,(3.2)
for the Bayes risk in (1.1), this time expressed under P0 in terms of a suitable process Φ(0)
adapted to the history of N . Unfortunately, this process is not Markovian in general. In
fact, the dynamics of a family
{
Φ(k)
}
k∈N0 of adapted processes including Φ
(0) are nested as
in
dΦ
(k)
t = λ
(
m(k) + Φ
(k)
t
)
dt+
1
µ
Φ
(k+1)
t− (dNt − µdt), t > 0, Φ(k)0 =
pi
1− pi m
(k).(3.8)
Hence the Poisson disorder problem is equivalent to the minimization of the Bayes risk in (3.2)
over all stopping times τ of the infinite-dimensional Markovian sufficient statistic
{
Φ(k)
}
k∈N0 .
However, Corollary 3.3 shows that only finitely many of the Φ(k)’s (as many as the number
6 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
of atoms of the distribution ν(·)) contain all relevant information if the distribution ν(·) is
concentrated on a finite number of atoms.
In Section 4, we specialize to the detection problem where the post-disorder arrival rate
Λ is expected either to increase by one unit or to decrease by one unit; namely, µ > 1 and
ν({µ− 1, µ+1}) = 1. The sufficient statistic for the detection problem becomes the process
{Φ(0),Φ(1)} with the dynamics
dΦ
(0)
t = λ
(
1 + Φ
(0)
t
)
dt+
1
µ
Φ
(1)
t− (dNt − µdt), Φ(0)0 =
pi
1− pi ,(4.2)
dΦ
(1)
t = λ
(
m+ Φ
(1)
t
)
dt+
1
µ
Φ
(0)
t− (dNt − µdt), Φ(0)0 =
pi
1− pi m,(4.3)
where m , E0[Λ− µ] = ν({µ+ 1})− ν({µ− 1}). If we rotate the coordinate system by 45°
clockwise, then the dynamics of the sufficient statistic in the new coordinates Φ˜ , {Φ˜(0), Φ˜(1)}
dΦ˜
(0)
t =
[
(λ+ 1)Φ˜
(0)
t +
λ(1−m)√
2
]
dt− 1
µ
Φ˜
(0)
t−dNt, Φ˜
(0)
0 =
(1−m)pi√
2(1− pi) ,
dΦ˜
(1)
t =
[
(λ− 1)Φ˜(1)t +
λ(1 +m)√
2
]
dt+
1
µ
Φ˜
(1)
t−dNt, Φ˜
(1)
0 =
(1 +m)pi√
2(1− pi) .
(4.6)
are autonomous. Between consecutive jumps of the Poisson process N , the sample paths of
the process Φ˜ = {Φ˜(0), Φ˜(1)} follow the integral curves t 7→ (x(t, φ0), y(t, φ1)), t ∈ R+ of the
differential equations
d
dt
x(t, φ0) = (λ+ 1)x(t, φ0) +
λ(1−m)√
2
, x(0, φ0) = φ0,
d
dt
y(t, φ1) = (λ− 1)y(t, φ1) + λ(1 +m)√
2
, y(0, φ1) = φ1.
(4.7)
More precisely, if σ0 ≡ 0 and σn, n ∈ N is the nth jump time of the Poisson process N , then
(see also Figures 1 and 2 on page 19)
Φ˜
(0)
t = x
(
t− σn, Φ˜(0)σn
)
and Φ˜
(1)
t = y
(
t− σn, Φ˜(1)σn
)
, σn ≤ t < σn+1, n ∈ N0,(4.10)
Φ˜(0)σn =
(
1− 1
µ
)
Φ˜
(0)
σn− and Φ˜
(1)
σn =
(
1 +
1
µ
)
Φ˜
(1)
σn−, n ∈ N0.
Moreover, the detection problem reduces to the discounted optimal stopping problem
V (φ0, φ1) , inf
τ∈S
Eφ0,φ10
[∫ τ
0
e−λtg
(
Φ˜
(0)
t , Φ˜
(1)
t
)
dt
]
with g(φ0, φ1) , φ0 + φ1 − λ
c
√
2, (φ0, φ1) ∈ R2+
(4.12)
ADAPTIVE POISSON DISORDER PROBLEM 7
for the two-dimensional piecewise-deterministic Markov process Φ˜ = {Φ˜(0), Φ˜(1)} in (4.6) or
(4.10).
To solve (4.12), we introduce in Section 5 the family of optimal stopping problems
Vn(φ0, φ1) , inf
τ∈S
Eφ0,φ10
[∫ τ∧σn
0
e−λtg
(
Φ˜
(0)
t , Φ˜
(1)
t
)
dt
]
, (φ0, φ1) ∈ R2+, n ∈ N.(5.1)
We show that the sequence {Vn(·, ·)}n∈N converges to the value function V (·, ·) uniformly on
R2+. More precisely,√
2
c
·
(
µ
λ+ µ
)n
≥ Vn(φ0, φ1)− V (φ0, φ1) ≥ 0, n ∈ N, (φ0, φ1) ∈ R2+.(5.2)
Following Gugerli (1986) and Davis (1993), we define the single-jump operator J on bounded
Borel functions w : R2+ 7→ R by
Jw(t, φ0, φ1) , Eφ0,φ10
[∫ t∧σ1
0
e−λug
(
Φ˜(0)u , Φ˜
(1)
u
)
du+ 1{t≥σ1}e
−λσ1w
(
Φ˜(0)σ1 , Φ˜
(1)
σ1
)]
,(5.3)
=
∫ t
0
e−(λ+µ)u
(
g + µ · w ◦ S)(x(u, φ0), y(u, φ1))du, t ∈ [0,∞],(5.7)
where S(x, y) ,
(
(1− 1
µ
) · x, (1 + 1
µ
) · y
)
∈ R2+ for every (x, y) ∈ R2+, and
Jtw(φ0, φ1) , inf
u∈[t,∞]
Jw(u, (φ0, φ1)), t ∈ [0,∞].(5.4)
The value functions Vn, n ∈ N in (5.1) coincide with the functions vn, n ∈ N defined sequen-
tially by
vn , J0vn−1 ∀n ∈ N, and v0 ≡ 0,(5.6)
and V (·, ·) = v(·, ·) , limn vn(·, ·) on R2+. Moreover, the form of the single-jump operator
Jw(·, ·, ·) in (5.7) allows us to prove that the functions vn(·, ·) ∈ N and v(·, ·) are increasing
in each argument, uniformly bounded, and concave. This information becomes crucial later
in Section 8 as we study the shape of the continuation regions and the smoothness of the
optimal stopping boundaries.
For every n ∈ N0, the value function Vn+1(·, ·) in (5.1) is attained by the stopping time
Sn+1 defined sequentially by
rn(φ0, φ1) , inf{t > 0 : vn+1(x(t, φ0), y(t, φ1)) = 0}, n ∈ N0, (φ0, φ1) ∈ R2+,
S1 , r0
(
Φ˜0
) ∧ σ1, and Sn+1 ,
 rn
(
Φ˜0
)
, if σ1 > rn
(
Φ˜0
)
σ1 + Sn ◦ θσ1 , if σ1 ≤ rn
(
Φ˜0
)
 , n ∈ N,
8 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
where θs is the shift-operator on Ω: Nt ◦ θs = Ns+t; see Proposition 5.5 and Corollary 5.8. In
other words, the best action before the (n+ 1)st Poisson arrival is to stop if the continuous
parametric curve t 7→ (x(t,Φ(0)0 (ω)), y(t,Φ(1)0 (ω))), t ∈ R+ in (4.7) enters the (n + 1)st
“stopping region” {(x, y) ∈ R2+ : vn+1(x, y) = 0} before the arrival of the first Poisson event.
Otherwise, it is better to wait, and stop if the curve t 7→ (x(t,Φ(0)σ1(ω)(ω)), y(t,Φ(1)σ1(ω)(ω))),
t ∈ R+ enters the nth “stopping region” {(x, y) ∈ R2+ : vn(x, y) = 0} before the arrival of
the next Poisson event, and so on; see also Figure 4(b) on page 39, Section 8.
The explicit optimal stopping rules Sn for Vn(·, ·), n ∈ N and the uniform convergence in
(5.2) give an ε-optimal stopping time for V (·, ·) in (4.12). For every ε > 0,
√
2
c
·
(
µ
λ+ µ
)n
< ε =⇒ 0 ≤ Eφ0,φ10
[∫ Sn
0
e−λtg
(
Φ˜t
)
dt
]
− V (φ0, φ1) < ε, (φ0, φ1) ∈ R2+.
In Proposition 5.12 of Section 5, we also prove that the problem with the value function
V (·, ·) in (4.12) admits an optimal stopping time, and the classical stopping times Uε ,
inf{t ≥ 0 : V (Φ˜t) ≥ −ε} are ε-optimal for every ε ≥ 0.
In Section 6, we show that the optimal continuation region {(φ0, φ1) ∈ R2+ : V (φ0, φ1) < 0}
for the problem V (·, ·) in (4.12) is a bounded subset of R2+. The boundedness of this region
and the concavity of the value function V (·, ·) will help us describe explicitly the structure
of the continuation region in Section 8.
As a by-product of the results in Section 6, we obtain some simple bounds on the alarm
time. When these are tight, they may serve as good approximate detection rules. The
optimal stopping time U0 , inf{t ≥ 0 : V (Φ˜(0)t , Φ˜(1)t ) = 0} is bounded by
τC0 , inf
{
t ≥ 0 : Φ˜(0)t + Φ˜(1)t ≥
λ
c
√
2
}
≤ U0 ≤ τD , inf{t ≥ 0 : Φ˜(0)t + Φ˜(1)t ≥ ξ∗}(6.2)
for a suitable constant ξ∗. If λ ≥ λ+µ
c
(1− λ), then ξ∗ = λ+µ
c
√
2; otherwise, it is the explicit
solution of (6.10), i.e.,
ξ∗ = − λ(1 +m)√
2(λ− 1) +
[
φ∗1 +
λ(1 +m)√
2(λ− 1)
][
1 + φ∗0
√
2(λ+ 1)
λ(1−m)
]−λ−1
λ+1
>
λ+ µ
c
√
2
where
φ∗0 ,
(λ+ µ)(1− λ)− λc
c
√
2
and φ∗1 ,
(λ+ µ)(1 + λ) + λc
c
√
2
.
For small values of the ratio µ/c of the pre-disorder arrival rate and the detection delay cost
per unit time, the thresholds of the lower and upper bounds in (6.2) on the optimal stopping
time U0 are close. Then the upper bound τD may serve as a simple approximate alarm time.
ADAPTIVE POISSON DISORDER PROBLEM 9
Section 8 starts with the description of the general sequential/numerical solution method.
Each function Vn(·, ·) in (5.1) vanishes outside the region D , {(x, y) ∈ R2+ : x + y ≤ ξ∗},
where ξ∗ is defined as above. On the bounded set D, we can find the functions Vn(·, ·) ≡
vn(·, ·) by repeatedly applying the operator J0 in (5.4). In practice, the uniform convergence
in (5.2) lets us control the number of iterations needed to achieve any given level of accuracy.
The exponential rate of convergence also suggests that this sequential algorithm should be
computationally feasible and accurate.
We tailor the general method mainly to two distinct cases (see below) of the detection
problem. In the meantime, we also shed light on the structure of the solutions of the optimal
stopping problems in (4.12) and (5.1). We show that the stopping regions
Γn , {(φ0, φ1) ∈ R2+ : vn(φ0, φ1) = 0} = {(φ0, φ1) ∈ R2+ : γn(φ0) ≤ φ1}, n ∈ N,
Γ , {(φ0, φ1) ∈ R2+ : v(φ0, φ1) = 0} = {(φ0, φ1) ∈ R2+ : γ(φ0) ≤ φ1}
are convex epigraphs of some “boundary functions” γn : R+ 7→ R and γ : R+ 7→ R, re-
spectively. These boundary functions are strictly decreasing, continuous, and convex on
their compact supports. The sequence {γn(·)}n∈N is increasing and converges to γ(·); see
Figure 4(a) on page 39.
Case I: A “large” disorder arrival rate λ ≥ (1 +m)(c/2). After some preparations in
Sections 9 and 10, we prove in Section 11 that{
(x, y) ∈ R2+ : v(x, y) = vn(x, y)
}
increases to R2+, and
{x ∈ R+ : γ(x) = γn(x)} increases to R+.
Namely, every iteration vn(·, ·) of the successive approximations in (5.6) gives the exact value
function v(·, ·) on a subset of the state space R2+ which increases to the whole space as the
iterations progress. Based on this fact,Methods B and C on pages 50 and 52, respectively,
gradually calculate the value function v(·, ·) on R2+ and the optimal stopping boundary γ(·)
on R+; see also Figure 6.
In Section 12.2, we prove that the value functions vn(·, ·), n ∈ N and v(·, ·) are continu-
ously differentiable everywhere, and that the boundary functions γn(·), n ∈ N and γ(·) are
continuously differentiable on their respective supports. The value function v(·, ·)—obtained
as the limit of the successive approximations in (5.6)—satisfies the variational inequalities in
(12.1)-(12.4) associated with the optimal stopping problem (4.12) and is the unique solution
(together with the boundary ∂Γ , {(x, γ(x)) : x ∈ supp(γ)}) of the corresponding free-
boundary problem. Finally, the smooth-fit principle also holds for the function v(·, ·) across
10 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
the boundary ∂Γ. These results ensure that the value function v(·, ·) can be calculated by
using finite-difference methods for partial differential-difference equations.
Case II: A “small” disorder arrival rate 0 < λ < (1 +m)(c/2). In this case we have
to pay more attention to the structure of the boundaries ∂Γn+1 = {(x, γn+1(x)) : x ∈
supp(γn+1)} of the stopping regions Γn+1, n ∈ N0. For every n ∈ N0, the boundary ∂Γn+1 is
divided into the entrance and exit boundaries
∂Γen+1 ,
{(
x(rn(φ0, φ1), φ0), γn+1(y(rn(φ0, φ1), φ1)
)
, for some (φ0, φ1) ∈ Cn+1
}
,
∂Γxn+1 ,
{
(φ0, φ1) ∈ Γn+1 : (x(t, φ0), y(t, φ1)) ∈ Cn+1, t ∈ (0, δ] for some δ > 0
}
,
(10.10)
respectively (in Case I, we always have ∂Γn+1 ≡ ∂Γen+1 for every n ∈ N0). We show that
the value function vn+1(·, ·) and the exit boundary ∂Γxn+1 are completely determined by the
entrance boundary ∂Γen+1 (see Lemma 10.7), and the value function vn(·, ·) from the previous
iteration determines the entrance boundary ∂Γen+1. Since v0 ≡ 0 is already available, the
general solution method outlined above can be enhanced as in Method D on page 62.
For certain configurations of parameters, we are able to prove that the value functions
vn+1(·, ·), n ∈ N0 are continuously differentiable everywhere on R2+\∂Γxn+1 and are not dif-
ferentiable on the exit boundary ∂Γxn+1, n ∈ N0, see Proposition 12.17 and Section 12.3.
In Section 12.3, we give a concrete example for a case where the value function v(·, ·) of
the optimal stopping problem in (4.12) is continuously differentiable everywhere on R2+\∂Γx
and is not differentiable on the exit boundary ∂Γx of the optimal stopping region Γ. The
interesting feature of this example is that the smooth-fit principle fails on some proper subset
(namely, the exit boundary ∂Γx) of the connected and continuously differentiable optimal
stopping boundary ∂Γ, while this principle holds on the rest; see Figure 9(d).
This work is divided naturally in two parts. In Part 1, we describe the problem, formu-
late a convenient model, and develop an important approximation. In Part 2, we use the
approximation of Part 1 to develop the solution and study its properties. Both parts are
accompanied by independent appendices which are the homes for long proofs.
ADAPTIVE POISSON DISORDER PROBLEM 11
Part 1. ANALYSIS: PROBLEM DESCRIPTION, MODEL, AND
APPROXIMATION
2. Problem description
Let N = {Nt; t ≥ 0} be a homogeneous Poisson process with some rate µ > 0 on a fixed
probability space (Ω,H,P0), which also supports two random variables θ and Λ independent
of each other and of the process N . We shall denote by ν(·) the distribution of the random
variable Λ, assume that
m(k) ,
∫
R
(v − µ)kν(dv), k ∈ N0 are well-defined and finite,(2.1)
and that
P0{θ = 0} = pi and P0{θ > t} = (1− pi)e−λt, t ≥ 0(2.2)
hold for some constants λ > 0 and pi ∈ [0, 1).
Let us denote by F = {Ft}t≥0 the right-continuous enlargement with P0-null sets of the
natural filtration σ(Ns; 0 ≤ s ≤ t) of N . We also define a larger filtration G = {Gt}t≥0 by
setting Gt , Ft ∨ σ{θ,Λ}, t ≥ 0. The G-adapted, right-continuous (hence, G-progressively
measurable) process
h(t) , µ1{t<θ} + Λ1{t≥θ}, t ≥ 0(2.3)
induces the (P0,G)-martingale (see Bre´maud (1981, pp. 165-168))
Zt , exp
{∫ t
0
log
(
h(s−)
µ
)
dNs −
∫ t
0
(h(s)− µ)ds
}
, t ≥ 0.(2.4)
This martingale defines a new probability measure P on every (Ω,Gt) by
dP
dP0
∣∣∣∣
Gt
= Zt, t ≥ 0.(2.5)
Since P and P0 coincide on G0 = σ{θ,Λ}, the random variables θ and Λ are independent and
have the same distributions under both P and P0.
Under the new probability measure P the counting process N has G-progressively mea-
surable intensity given by h(·) of (2.3), namely Nt −
∫ t
0
h(s)ds, t ≥ 0 is a (P,G)-martingale.
In other words, the G-adapted process N is a Poisson process whose rate changes at time θ
from µ to Λ.
In the Poisson disorder problem, only the process N is observable, and our objective is
to detect the disorder time θ as quickly as possible. More precisely, we want to find an
12 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
F-stopping time τ that minimizes the Bayes risk
Rτ (pi) , P{τ < θ}+ cE(τ − θ)+,(2.6)
where c > 0 is a constant, and the expectation E is taken under the probability measure P.
Hence, we are interested in an alarm time τ which is adapted to the history of the process N ,
and minimizes the tradeoff between the frequency of false alarms P{τ < θ} and the expected
time of delay E(τ − θ)+ between the alarm time and the unobservable disorder time.
In the next section, we shall formulate the quickest detection problem as a problem of
optimal stopping for a suitable Markov process.
3. Sufficient statistics for the adaptive Poisson disorder problem
Let S be the collection of all F-stopping times, and introduce the F-adapted processes
Πt , P{θ ≤ t|Ft}, and Φ(k)t ,
E[(Λ− µ)k1{θ≤t}|Ft]
1− Πt , k ∈ N0, t ≥ 0.
(3.1)
Since Λ has the same distribution ν(·) under P and P0, each Φ(k), k ∈ N0 is well-defined by
(2.1). The process Π = {Πt, t ≥ 0} tracks the likelihood that a change in the intensity of
N has already occurred, given past and present observations of the process. Each Φ(k) =
{Φ(k)t , t ≥ 0}, k ∈ N may be regarded as a (weighted) odds-ratio process.
Our first lemma below shows that the minimum Bayes risk can be found by solving a
discounted optimal stopping problem, with discount rate λ and running cost function f(x) =
x − λ/c for the F-adapted process Φ(0). The calculations are considerably easier when the
process Φ(0) has the Markov property. Unfortunately, this is not true in general. However,
the explicit dynamics of Φ(0) in Lemma 3.2 reveal that the infinite-dimensional sequence
{Φ(k)}k∈N0 of the processes in (3.1) is always a sufficient Markovian statistic for the quickest
detection problem. The same result also suggests sufficient conditions for the existence of a
finite-dimensional sufficient Markovian statistic, a case amenable to concrete analysis.
3.1. Lemma. The Bayes risk in (2.6) equals
Rτ (pi) = 1− pi + c(1− pi)E0
[∫ τ
0
e−λt
(
Φ
(0)
t −
λ
c
)
dt
]
, τ ∈ S,(3.2)
where the expectation E0 is taken under the (reference) probability measure P0.
For several proofs below, the following observations will be useful. Every Zt in (2.4) can
be written as
Zt = 1{t<θ} +
Lt
Lθ
1{t≥θ}(3.3)
ADAPTIVE POISSON DISORDER PROBLEM 13
in terms of the likelihood ratio process
Lt ,
(
Λ
µ
)Nt
e−(Λ−µ)t, t ≥ 0.(3.4)
Then the generalized Bayes theorem (see, e.g., Liptser and Shiryaev (2001, Section 7.9)) and
(3.3) imply
1− Πt = E0[Zt1{θ>t}|Ft]E0[Zt|Ft] =
P0{θ > t|Ft}
E0[Zt|Ft] =
(1− pi)e−λt
E0[Zt|Ft] ,(3.5)
since θ is independent of the process N under P0 and has the distribution (2.2).
Proof of Lemma 3.1. By (3.1), the generalized Bayes theorem and (3.5), we have
Φ
(0)
t =
E[1{θ≤t}|Ft]
1− Πt =
E0[Zt1{θ≤t}|Ft]
(1− Πt)E0[Zt|Ft] =
E0[Zt1{θ≤t}|Ft]
(1− pi)e−λt , t ≥ 0,
which gives
(3.6) E[(τ − θ)+] = E
[∫ ∞
0
1{θ≤t<τ}dt
]
=
∫ ∞
0
E0[Zt1{τ>t}1θ≤t]dt
=
∫ ∞
0
E0
[
1{τ>t}E0[Zt1{θ≤t}|Ft]
]
dt = (1− pi) · E0
[∫ τ
0
e−λtΦ(0)t dt
]
, τ ∈ S.
Suppose that τ ∈ S takes countably many distinct values {tn, n ∈ N} for some tn ∈
R+ ∪ {+∞}. Then
(3.7) P{τ < θ} =
∑
n
P{tn < θ, τ = tn} =
∑
n
E0[Ztn1{tn<θ}1{τ=tn}]
=
∑
n
E0[1{θ>tn}1{τ=tn}] =
∑
n
(1− pi)e−λtn · E0[1{τ=tn}]
= (1− pi) · E0
∑
n
[(
1−
∫ tn
0
λe−λtdt
)
1{τ=tn}
]
= (1− pi)− (1− pi)λ · E0
[∫ τ
0
e−λtdt
]
.
An arbitrary stopping time τ ∈ S is the almost-sure limit of a decreasing sequence {τn}n≥1 ⊂
S of stopping times which take countably many values. For every τn, n ∈ N, (3.7) holds.
Since t 7→ 1{t<θ} and t 7→
∫ t
0
e−λsds are right-continuous and bounded, passage to limit as
n → ∞ and the bounded convergence theorem verifies (3.7) for every τ ∈ S. The sum of
(3.6) and (3.7) gives (3.2). 
3.2. Lemma. Let m(k), k ∈ N0 be defined as in (2.1) Then every Φ(k), k ∈ N0 in (3.1)
satisfies the equation
dΦ
(k)
t = λ
(
m(k) + Φ
(k)
t
)
dt+
1
µ
Φ
(k+1)
t− (dNt − µdt), t > 0, Φ(k)0 =
pi
1− pi m
(k).(3.8)
14 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Proof. For every k ∈ N0, let us introduce the function
F (k)(t, x) ,
∫ (
v
µ
)x
(v − µ)ke−(v−µ)tν(dv), t ∈ R+, x ∈ R.(3.9)
The generalized Bayes theorem, (3.5), and the independence of the random variables θ, Λ
and the process N under P0 imply
Φ
(k)
t =
E0
[
(Λ− µ)kZt1{θ≤t}|Ft
]
(1− Πt)E0[Zt|Ft] =
E0
[
(Λ− µ)k
(
Lt1{θ=0} + LtLθ 1{0<θ≤t}
) ∣∣Ft]
(1− pi)e−λt
=
pieλt
1− piF
(k)(t, Nt) + λ
∫ t
0
eλ(t−s)F (k)(t− s,Nt −Ns)ds = U (k)t + V (k)t
(3.10)
for every k ∈ N0 and t ∈ R+, where
U
(k)
t ,
pieλt
1− piF
(k)(t, Nt) and V
(k)
t , λ
∫ t
0
eλ(t−s)F (k)(t− s,Nt −Ns)ds.(3.11)
Every F (k)(t, x), k ∈ N0 in (3.9) is continuously differentiable, and
∂
∂t
F (k)(t, x) = −F (k+1)(t, x), t > 0, x ∈ R, k ∈ N0.(3.12)
The change of variable formula for jump processes gives
F (k)(t, Nt) = F
(k)(0, 0) +
∫ t
0
∂F (k)
∂t
(s,Ns)ds+
∫ t
0
∂F (k)
∂x
(s,Ns−)dNs
+
∑
0<s≤t
[
F (k)(s,Ns)− F (k)(s,Ns−)− ∂F
(k)
∂x
(s,Ns−)∆Ns
]
= m(k) −
∫ t
0
F (k+1)(s,Ns)ds+
∑
0<s≤t
[
F (k)(s,Ns)− F (k)(s,Ns−)
]
,
(3.13)
where ∆Ns , Ns −Ns− ∈ {0, 1} for every s > 0, and the last equality follows from (3.12),
F (k)(0, 0) = m(k), k ∈ N0, and
∫ t
0
∂F (k)
∂x
(s,Ns−)dNs =
∑
0<s≤t
∂F (k)
∂x
(s,Ns−)∆Ns.
However, F (k)(s,Ns)− F (k)(s,Ns−) is equal to∫ (
v
µ
)Ns−+∆Ns
(v − µ)ke−(v−µ)sν(dv)−
∫ (
v
µ
)Ns−
(v − µ)ke−(v−µ)sν(dv)
=
∫ (
v
µ
)Ns− [(v
µ
)∆Ns
− 1
]
(v − µ)ke−(v−µ)tν(dv)
=
∆Ns
µ
∫ (
v
µ
)Ns−
(v − µ)k+1e−(v−µ)tν(dv) = 1
µ
F (k+1)(s,Ns−)∆Ns,
ADAPTIVE POISSON DISORDER PROBLEM 15
since [(v/µ)∆Ns − 1] = (∆Ns/µ)(v − µ). The last displayed equation and (3.13) imply
F (k)(t, Nt) = m
(k) −
∫ t
0
F (k+1)(s,Ns)ds+
∑
0<s≤t
1
µ
F (k+1)(s,Ns−)∆Ns
= m(k) +
∫ t
0
1
µ
F (k+1)(s,Ns−)(dNs − µds), t ∈ R+, k ∈ N0.
(3.14)
This identity will help us derive the dynamics of U (k) and V (k) in (3.11). Note that
d
(
1− pi
pi
U
(k)
t
)
= d
(
eλtF (k)(t, Nt)
)
= eλtF (k)(t, Nt)λdt+ e
λtdF (k)(t, Nt)
= λ
1− pi
pi
U
(k)
t dt+
eλt
µ
F (k+1)(t, Nt−)(dNt − µdt).
Therefore,
dU
(k)
t = λU
(k)
t +
1
µ
U
(k+1)
t (dNt − µdt), t > 0, U (k)0 =
pi
1− pi m
(k).(3.15)
The derivation of the dynamics of V (k) is trickier. For every fixed s ∈ [0, t), let us define
N
(s)
u , Ns+u −Ns, 0 ≤ u ≤ t− s. This is also a Poisson process under P0. As in (3.14),
F (k)
(
t− s,N (s)t−s
)
= m(k) +
∫ t−s
0
1
µ
F (k+1)
(
u,N
(s)
u−
) (
dN (s)u − µdu
)
.
Changing the variable of integration and substituting N
(s)
• = Ns+• − Ns into this equality
gives
F (k)(t− s,Nt −Ns) = m(k) + 1
µ
∫ t
s
F (k+1)(v − s,Nv− −Ns)(dNv − µdv).
Let us plug this identity into V
(k)
t in (3.11), multiply both sides by e
−λt, and change the
order of integration. Then
e−λtV (k)t =
∫ t
0
λe−λs
(
m(k) +
1
µ
∫ t
0
F (k+1)(v − s,Nv− −Ns)(dNv − µdv)
)
ds
= m(k)
∫ t
0
λe−λsds+
λ
µ
∫ t
0
(∫ v
0
e−λsF (k+1)(v − s,Nv −Ns)ds
)
(dNv − µdv)
= m(k)
∫ t
0
λe−λsds+
1
µ
∫ t
0
e−λvV (k+1)v (dNv − µdv).
Differentiating both sides and rearranging terms, we obtain
dV
(k)
t = λ
(
m(k) + V
(k)
t
)
dt+
1
µ
V
(k+1)
t (dNt − µdt), t > 0, V (k)0 = 0.(3.16)
Adding (3.15) and (3.16) as in (3.10) gives the dynamics (3.8) of the process Φ(k). 
16 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Lemma 3.2 shows that the process Φ(0) does not have the Markov property in general.
This is because, as (3.8) shows, Φ(0) depends on Φ(1), then Φ(1) depends on Φ(2), and so
on ad infinitum. However, a finite-dimensional sufficient Markovian statistic emerges if the
system of stochastic differential equations in (3.8) is closeable, namely, if the process Φ(k) can
be expressed in terms of the processes Φ(0), . . . ,Φ(k−1), for some k ∈ N0. Our next corollary
shows that this is true if Λ takes finitely many distinct values.
3.3. Corollary. Suppose that ν({λ1, · · · , λk}) = 1 for some positive numbers λ1, . . . , λk.
Consider the polynomial
p(v) ,
k∏
i=1
(v − λi + µ) ≡ vk +
k−1∑
i=0
ci v
i, v ∈ R
for suitable real numbers c0, . . . , ck−1. Then {Φ(0),Φ(1), · · · ,Φ(k−1)} is a k-dimensional suffi-
cient Markov statistic, with Φ(k) = −∑k−1i=0 ciΦ(i).
Proof. Under the hypothesis, the random variable p(Λ− µ) = (Λ− µ)k +∑k−1i=0 ci(Λ− µ)i is
equal to zero almost surely. Therefore, (3.1) implies
Φ
(k)
t +
k−1∑
i=0
ciΦ
(i)
t =
E
[
p(Λ− µ)1{θ≤t}|Ft
]
1− Πt = 0, P-a.s., for every t ≥ 0.
The process on the lefthand side has right-continuous sample paths, by (3.8). Therefore,
Φ
(k)
t +
∑k−1
i=0 ciΦ
(i)
t = 0 for all t ∈ R+ almost surely, i.e., the process Φ(k) is the linear
combination of the processes Φ(0), . . . ,Φ(k−1) outside a null set. 
In applications, one may construct an a prior distribution for the random variable Λ
by using empirical distributions obtained from past data, if available, and/or from expert
opinions in the field. Therefore, it is reasonable to expect that a prior distribution for Λ will
typically be discrete with finite support. In such a case, we can set up the detection problem
in the form of an optimal stopping problem for a finite-dimensional Markov process, thanks
to Corollary 3.3. In the remainder of the paper, we shall study the case where the arrival
rate of the observations after the disorder has a Bernoulli prior distribution.
4. Poisson disorder problem with a Bernoulli post-disorder arrival rate
We shall assume henceforth µ > 1 and that the random variable Λ has Bernoulli distribu-
tion
ν({µ− 1, µ+ 1}) = 1.(4.1)
ADAPTIVE POISSON DISORDER PROBLEM 17
Namely, the rate of the Poisson processN is expected to increase or decrease by one unit after
the disorder. Corollary 3.3 implies that Φ(2) = Φ(0), and the sufficient statistic (Φ(0),Φ(1)) is
a Markov process. According to Lemma 3.2, the pair satisfies
dΦ
(0)
t = λ
(
1 + Φ
(0)
t
)
dt+
1
µ
Φ
(1)
t− (dNt − µdt), Φ(0)0 =
pi
1− pi ,(4.2)
dΦ
(1)
t = λ
(
m+ Φ
(1)
t
)
dt+
1
µ
Φ
(0)
t− (dNt − µdt), Φ(1)0 =
pi
1− pi m,(4.3)
where, as in (2.1), we set
m ≡ m(1) = E0[Λ− µ] = P{Λ = µ+ 1} − P{Λ = µ− 1}.(4.4)
The dynamics of the processes Φ(0) and Φ(1) in (4.2) and (4.3) are interdependent. However,
if we define a new process
Φ˜ ≡
[
Φ˜(0)
Φ˜(1)
]
, 1√
2
[
Φ(0) − Φ(1)
Φ(0) + Φ(1)
]
,(4.5)
then each of the new processes Φ˜(0) and Φ˜(1) is autonomous:
dΦ˜
(0)
t =
[
(λ+ 1)Φ˜
(0)
t +
λ(1−m)√
2
]
dt− 1
µ
Φ˜
(0)
t−dNt, Φ˜
(0)
0 =
(1−m)pi√
2(1− pi) ,
dΦ˜
(1)
t =
[
(λ− 1)Φ˜(1)t +
λ(1 +m)√
2
]
dt+
1
µ
Φ˜
(1)
t−dNt, Φ˜
(1)
0 =
(1 +m)pi√
2(1− pi) .
(4.6)
The new coordinates Φ˜(0) and Φ˜(1) are in fact the conditional odds-ratio processes as in
Φ˜
(0)
t =
√
2 · P{Λ = µ− 1, θ ≤ t|Ft}
P{θ > t|Ft} and Φ˜
(1)
t =
√
2 · P{Λ = µ+ 1, θ ≤ t|Ft}
P{θ > t|Ft} .
Therefore, both Φ˜(0) and Φ˜(1) are nonnegative processes.
Note that m ∈ [−1, 1] in (4.4). The cases m = ±1 degenerate to Poisson disorder
problems with known post-disorder rates, and were studied by Bayraktar, Dayanik, and
Karatzas (2004b). Therefore, we will assume that m ∈ (−1, 1) in the remainder.
4.1. Remark. For every φ0 ∈ R and φ1 ∈ R, let us denote by x(t, φ0), t ∈ R and y(t, φ1),
t ∈ R the solutions of the differential equations
d
dt
x(t, φ0) = (λ+ 1)x(t, φ0) +
λ(1−m)√
2
, x(0, φ0) = φ0,
d
dt
y(t, φ1) = (λ− 1)y(t, φ1) + λ(1 +m)√
2
, y(0, φ1) = φ1,
(4.7)
18 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
respectively. These solutions are given by
x(t, φ0) = − λ(1−m)√
2(λ+ 1)
+ e(λ+1)t
[
φ0 +
λ(1−m)√
2(λ+ 1)
]
, t ∈ R,
y(t, φ1) =

− λ(1 +m)√
2(λ− 1) + e
(λ−1)t
[
φ1 +
λ(1 +m)√
2(λ− 1)
]
, λ 6= 1
φ1 +
1 +m√
2
t, λ = 1
 , t ∈ R.
(4.8)
Both x(·, φ0) and y(·, φ1) have the semi-group property, i.e., for every t ∈ R and s ∈ R
x(t+ s, φ0) = x(s, x(t, φ0)) and y(t+ s, φ1) = y(s, y(t, φ1)).(4.9)
Note from (4.6) and (4.7) that
Φ˜
(0)
t = x
(
t− σn, Φ˜(0)σn
)
and Φ˜
(1)
t = y
(
t− σn, Φ˜(1)σn
)
, σn ≤ t < σn+1, n ∈ N0.(4.10)
4.1. An optimal stopping problem for the quickest detection of the Poisson dis-
order. In terms of the new sufficient statistics Φ˜(1) and Φ˜(0) in (4.5, 4.6), the Bayes risk of
(2.6, 3.2) can be rewritten as
Rτ (pi) = 1− pi + c(1− pi)√
2
· E0
[∫ τ
0
e−λt
(
Φ˜
(0)
t + Φ˜
(1)
t −
λ
c
√
2
)
dt
]
, τ ∈ S.
Therefore, the minimum Bayes risk U(pi) , infτ∈S Rτ (pi), pi ∈ [0, 1) is given by
U(pi) = 1− pi + c(1− pi)√
2
· V
(
(1−m)pi√
2(1− pi) ,
(1 +m)pi√
2(1− pi)
)
, pi ∈ [0, 1),(4.11)
where m is as in (4.4), the function V (·, ·) is the value function of the optimal stopping
problem
V (φ0, φ1) , inf
τ∈S
Eφ0,φ10
[∫ τ
0
e−λtg
(
Φ˜
(0)
t , Φ˜
(1)
t
)
dt
]
,
g(φ0, φ1) , φ0 + φ1 − λ
c
√
2, (φ0, φ1) ∈ R2+,
(4.12)
and Eφ0,φ10 is the conditional P0-expectation given that Φ˜
(0)
0 = φ0 and Φ˜
(1)
0 = φ1. Moreover,
an optimal stopping time for (4.12) is a minimum Bayes alarm time.
It is clear from (4.12) that it is never optimal to stop before the process Φ˜ leaves the
region
C0 ,
{
(φ0, φ1) ∈ R2+ : φ0 + φ1 <
λ
c
√
2
}
.(4.13)
ADAPTIVE POISSON DISORDER PROBLEM 19
(a) Φ˜(1) with 0 < λ < 1
φd0
t
0
t
Φ˜(1)0
t
Φ˜(1)t (ω)
Φ˜(0)
(c) Φ˜(0) with λ > 0
Φ˜(1)
Φ˜(0)t (ω)
Φ˜(1)t (ω)
(b) Φ˜(1) with λ ≥ 1
Figure 1. The sample-paths of the processes Φ˜(1) and Φ˜(0).
In the next subsection we shall discuss the pathwise behavior of the process Φ˜; this will give
insight into the solution of the optimal stopping problem in (4.12).
4.2. The sample-paths of the sufficient-statistic process Φ˜ = (Φ˜(0), Φ˜(1)). The process
Φ˜(0) jumps downwards and increases between jumps; see (4.6) and Figure 1(c). On the other
hand, the process Φ˜(1) jumps upwards, and its behavior between jumps depends on the sign
of 1 − λ. If λ ≥ 1, then the process Φ˜(1) increases between jumps; see Figure 1(b). If
0 < λ < 1, then Φ˜(1) reverts to the (positive) “mean-level”
φd ,
λ(1 +m)
(1− λ)√2(4.14)
between jumps; it never visits φd unless it starts there; and in this latter case, it stays at φd
until the first jump and never comes back to φd later (i.e., φd > 0 is an entrance boundary
for Φ˜(1)); see Figure 1(a). Finally, note that φd and 1− λ 6= 0 have the same signs.
As for the solution of the optimal stopping problem in (4.12), it is worth waiting if the
process Φ˜ is in the region C0 of (4.13), or is likely to return to C0 shortly. The sample-paths
of the process Φ˜ are deterministic between jumps, and tend towards, or away from, the region
20 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
C0
Φ˜(1)
Φ˜t(ω)
α
αα = 90◦
α
Φ˜(0)0
Φ˜(1)
Φ˜(0)0
Φ˜t(ω)
φd
λ
c
√
2
λ
c
√
2
λ
c
√
2
λ
c
√
2
(a) Case I: λ ≥ 1 or 0 < (λ/c)√2 ≤ φd (b) Case II: 0 < φd < (λ/c)
√
2
C0
Figure 2. The sample-paths of Φ˜
C0. These two cases are described separately below. In both cases, however, the process
Φ˜ jumps in the same direction relative to its position before the jump. A jump at (φ0, φ1)
is an instantaneous displacement (1/µ)[−φ0 φ1]T in Φ˜. Therefore, the jump direction is
away from (respectively, towards) the region C0 if φ0 < φ1 (respectively, φ0 > φ1). Along
a quarter of a circle in Figure 2(a), the directions of jumps at an equal distance from the
origin are illustrated by the arrows. Note also that, along any fixed half-ray in R2+, the jump
direction (namely, the angle α in Figure 2(a)) does not change, but the size of the jump
does.
4.3. Case I: A “large” disorder arrival rate. Suppose that λ ≥ 1 or 0 < (λ/c)√2 ≤ φd.
Equivalently, λ ≥ [1− (1 +m)(c/2)]+ is “large”. Between jumps, the process Φ˜ gets farther
away from the region C0. It may return to C0 by jumps only, and only if the jump originates
in the region L , {(φ0, φ1) : φ0 > φ1}; see Figure 2(a). But, if Φ˜(1) reaches at or above
(λ/c)
√
2, then Φ˜ will never return to C0.
4.4. Case II: A “small” disorder arrival rate. Now suppose that 0 < φd < (λ/c)
√
2.
Equivalently, 0 < λ < 1− (1+m)(c/2) is “small”. If the process Φ˜ finds itself in a very close
neighborhood of the upper-left corner of the triangular region C0, then it will drift into C0
ADAPTIVE POISSON DISORDER PROBLEM 21
before the next jump with positive probability. Otherwise, the behavior of the sample-paths
of Φ˜ relative to C0 is very similar to that in Case I; see Figure 2(b).
5. A family of related optimal stopping problems
Let us introduce the family of optimal stopping problems
Vn(φ0, φ1) , inf
τ∈S
Eφ0,φ10
[∫ τ∧σn
0
e−λtg
(
Φ˜
(0)
t , Φ˜
(1)
t
)
dt
]
, (φ0, φ1) ∈ R2+, n ∈ N,(5.1)
obtained from (4.12) by stopping the process Φ˜ at the nth jump time σn of the process N .
Since g(·, ·) in (4.12) is bounded from below by the constant −(λ/c)√2, the expectation in
(5.1) is well-defined for every stopping time τ ∈ S. In fact, −√2/c ≤ Vn ≤ 0 for every n ∈ N.
Since the sequence (σn)n≥1 of jump times of the process N is increasing almost surely, the
sequence (Vn)n≥1 is decreasing. Therefore, limn→∞ Vn exists everywhere. It is also obvious
that Vn ≥ V , n ∈ N.
5.1. Proposition. As n → ∞, the sequence Vn(φ0, φ1) converges to V (φ0, φ1) uniformly in
(φ0, φ1) ∈ R2+. In fact, for every n ∈ N and (φ0, φ1) ∈ R2+, we have
√
2
c
·
(
µ
λ+ µ
)n
≥ Vn(φ0, φ1)− V (φ0, φ1) ≥ 0.(5.2)
Proof. Let us fix (φ0, φ1) ∈ R2+. For every τ ∈ S and n ∈ N, we have
Eφ0,φ10
[∫ τ
0
e−λsg(Φ˜s)ds
]
= Eφ0,φ10
[∫ τ∧σn
0
e−λsg(Φ˜s)ds
]
+Eφ0,φ10
[
1{τ≥σn}
∫ τ
σn
e−λsg(Φ˜s)ds
]
≥ Eφ0,φ10
[∫ τ∧σn
0
e−λsg(Φ˜s)ds
]
− λ
c
√
2 · Eφ0,φ10
[
1{τ≥σn}
∫ τ
σn
e−λsds
]
≥ Eφ0,φ10
[∫ τ∧σn
0
e−λsg(Φ˜s)ds
]
−
√
2
c
· Eφ0,φ10
[
e−λσn
] ≥ Vn(φ0, φ1)− √2
c
·
(
µ
λ+ µ
)n
We have used the bound g(φ0, φ1) ≥ −(λ/c)
√
2 from (4.12), as well as the fact that N is
a Poisson process with rate µ under P0, and σn is the n-th jump time of N . Taking the
infimum over τ ∈ S gives the first inequality in (5.2). 
We shall try to calculate now the functions Vn(·) of (5.1), following a method of Gugerli
(1986) and Davis (1993). Let us start by defining on the collection of bounded Borel functions
22 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
w : R2+ 7→ R the operators
Jw(t, φ0, φ1) , Eφ0,φ10
[∫ t∧σ1
0
e−λug
(
Φ˜(0)u , Φ˜
(1)
u
)
du+ 1{t≥σ1}e
−λσ1w
(
Φ˜(0)σ1 , Φ˜
(1)
σ1
)]
,(5.3)
Jtw(φ0, φ1) , inf
u∈[t,∞]
Jw(u, φ0, φ1) for every t ∈ [0,∞].(5.4)
The special structure of the stopping times of jump processes (see Lemma 7.1 below) implies
J0w(φ0, φ1) = inf
τ∈S
Eφ0,φ10
[∫ τ∧σ1
0
e−λtg
(
Φ˜
(0)
t , Φ˜
(1)
t
)
dt+ 1{τ≥σ1}e
−λσ1w
(
Φ˜(0)σ1 , Φ˜
(1)
σ1
)]
.(5.5)
By relying on the strong Markov property of the process N at its first jump time σ1, one
expects that the value function V of (4.12) satisfies the equation V = J0V . Below, we show
that this is indeed the case. In fact, if we define vn : R2+ 7→ R, n ∈ N0 sequentially by
v0 ≡ 0, and vn , J0vn−1 ∀n ∈ N,(5.6)
then every vn is bounded and identical to Vn of (5.1), limn→∞ vn exists and equals the value
function V in (4.12).
Under P0, the first jump time σ1 of the process N has exponential distribution with rate
µ. Using the Fubini theorem and (4.10), we can write (5.3) as
Jw(t, φ0, φ1) =
∫ t
0
e−(λ+µ)u
(
g + µ · w ◦ S)(x(u, φ0), y(u, φ1))du, t ∈ [0,∞],(5.7)
where x(·, φ0) and y(·, φ1) are the solutions (4.8) of the ordinary differential equations in
(4.7), and S : R2+ 7→ R2+ is the linear mapping
S(φ0, φ1) ,
((
1− 1
µ
)
φ0,
(
1 +
1
µ
)
φ1
)
.(5.8)
5.2. Remark. Using µ > 1 and the explicit forms of x(u, φ0) and y(u, φ1) in (4.8), it is easy
to check that the integrand in (5.7) is absolutely integrable on R+. Therefore,
lim
t→∞
Jw(t, φ0, φ1) = Jw(∞, φ0, φ1) <∞,
and the mapping t 7→ Jw(t, φ0, φ1) : [0,∞] 7→ R is continuous. The infimum Jtw(φ0, φ1) in
(5.4) is attained for every t ∈ [0,∞].
5.3. Lemma. For every bounded Borel function w : R2+ 7→ R, the mapping J0w is bounded.
If we define ||w|| , sup(φ0,φ1)∈R2+ |w(φ0, φ1)| <∞, then
−
(
λ
λ+ µ
·
√
2
c
+
µ
λ+ µ
· ||w||
)
≤ J0w(φ0, φ1) ≤ 0, (φ0, φ1) ∈ R2+.(5.9)
ADAPTIVE POISSON DISORDER PROBLEM 23
If the function w(φ0, φ1) is concave, then so is J0w(φ0, φ1). If w1 ≤ w2 are real-valued and
bounded Borel functions defined on R2+, then J0w1 ≤ J0w2.
5.4. Corollary. Every vn, n ∈ N0 in (5.6) is bounded and concave, and −
√
2/c ≤ . . . ≤
vn ≤ vn−1 ≤ v1 ≤ v0 ≡ 0. The limit
v(φ0, φ1) , lim
n→∞
vn(φ0, φ1), (φ0, φ1) ∈ R2+(5.10)
exists, and is also bounded and concave.
Both vn : R2+ 7→ R, n ∈ N and v : R2+ 7→ R are continuous, increasing in each of their
arguments, and their left and right partial derivatives are bounded on every compact subset
of R2+.
5.5. Proposition. For every n ∈ N, the functions vn of (5.6) and Vn of (5.1) coincide. For
every ε ≥ 0, let
rεn(φ0, φ1) , inf
{
s ∈ (0,∞] : Jvn
(
s, φ0, φ1
) ≤ J0vn(φ0, φ1) + ε} , n ∈ N0, (φ0, φ1) ∈ R2+,
Sε1 , rε0
(
Φ˜0
) ∧ σ1, and Sεn+1 ,
 r
ε/2
n
(
Φ˜0
)
, if σ1 > r
ε/2
n
(
Φ˜0
)
σ1 + S
ε/2
n ◦ θσ1 , if σ1 ≤ rε/2n
(
Φ˜0
)
 , n ∈ N,
where θs is the shift-operator on Ω: Nt ◦ θs = Ns+t. Then
Eφ0,φ10
[∫ Sεn
0
e−λtg
(
Φ˜t
)
dt
]
≤ vn(φ0, φ1) + ε, ∀n ∈ N, ∀ ε ≥ 0.(5.11)
5.6. Proposition. We have v(φ0, φ1) = V (φ0, φ1) for every (φ0, φ1) ∈ R2+. Moreover, V is
the largest nonpositive solution U of the equation U = J0U .
5.7. Lemma. Let w : R2+ 7→ R be a bounded function. For every t ∈ R+ and (φ0, φ1) ∈ R2+,
Jtw(φ0, φ1) = Jw(t, φ0, φ1) + e
−(λ+µ)t J0w
(
x(t, φ0), y(t, φ1))
)
.(5.12)
5.8. Corollary. Let
rn(φ0, φ1) = inf
{
s ∈ (0,∞] : Jvn
(
s, (φ0, φ1)
)
= J0vn(φ0, φ1)
}
(5.13)
be the same as rεn(φ0, φ1) in Proposition 5.5 with ε = 0. Then
rn(φ0, φ1) = inf
{
t > 0 : vn+1
(
x(t, φ0), y(t, φ1)
)
= 0
}
(inf ∅ ≡ ∞).(5.14)
Proof. Let us fix (φ0, φ1) ∈ R2+, and denote rn(φ0, φ1) by rn. By Remark 5.2, we have
Jvn(rn, φ0, φ1) = J0vn(φ0, φ1) = Jrnvn(φ0, φ1).
24 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Suppose first that rn <∞. Since J0vn = vn+1, taking t = rn and w = vn in (5.7) gives
Jvn(rn, φ0, φ1) = Jrnvn(φ0, φ1) = Jvn(rn, φ0, φ1) + e
−(λ+µ)rnvn+1(x(rn, φ0), y(rn, φ1)).
Therefore, vn+1(x(rn, φ0), y(rn, φ1)) = 0.
If 0 < t < rn, then Jvn(t, φ0, φ1) > J0vn(φ0, φ1) = Jrnvn(φ0, φ1) = Jtvn(φ0, φ1) since
u 7→ Juvn(φ0, φ1) is nondecreasing. Taking t ∈ (0, rn) and w = vn in (5.7) imply
J0vn(φ0, φ1) = Jtvn(φ0, φ1) = Jvn(t, φ0, φ1) + e
−(λ+µ)tvn+1(x(t, φ0), y(t, φ1)).
Therefore, vn+1(x(t, φ0), y(t, φ1)) < 0 for every t ∈ (0, rn), and (5.14) follows.
Suppose now that rn =∞. Then we have vn+1(x(t, φ0), y(t, φ1)) < 0 for every t ∈ (0,∞)
by the same argument in the last paragraph above. Hence, {t > 0 : vn+1(x(t, φ0), y(t, φ1)) =
0} = ∅, and (5.14) still holds. 
5.9. Remark. For every t ∈ [0, rn(φ0, φ1)], we have Jtvn(φ0, φ1) = J0vn(φ0, φ1) = vn+1(φ0, φ1).
Then substituting w(·, ·) = vn(·, ·) in (5.12) gives the dynamic programming equation for the
family {vk(·, ·)}k∈N0 : for every (φ0, φ1) ∈ R2+ and n ∈ N0
vn+1(φ0, φ1) = Jvn(t, φ0, φ1) + e
−(λ+µ)tvn+1(x(t, φ0), y(t, φ1)), t ∈ [0, rn(φ0, φ1)].(5.15)
5.10. Remark (Dynamic Programming Equation for V (·, ·)). Since V (·, ·) is bounded, and
V = J0V by Proposition 5.6, Lemma 5.7 gives
JtV (φ0, φ1) = JV (t, φ0, φ1) + e
−(λ+µ)t V
(
x(t, φ0), y(t, φ1))
)
, t ∈ R+(5.16)
for every (φ0, φ1) ∈ R2+; and if we define
r(φ0, φ1) , inf{t > 0 : JV (t, φ0, φ1) = J0V (φ0, φ1)}, (φ0, φ1) ∈ R2+,(5.17)
then arguments similar to those in the proof of Corollary 5.8, and (5.16), give
r(φ0, φ1) = inf{t > 0 : V (φ0, φ1) = 0}, (φ0, φ1) ∈ R2+,(5.18)
as well as the Dynamic Programming equation
V (φ0, φ1) = JV (t, φ0, φ1) + e
−(λ+µ)tV (x(t, φ0), y(t, φ1)), t ∈ [0, r(φ0, φ1)](5.19)
for the function V (·, ·) of (4.12). Because t 7→ Jw(t, (φ0, φ1)) and t 7→ Jtw(φ0, φ1) are
continuous for every bounded w : R2+ 7→ R (see, e.g., (5.7)), the identity (5.16) implies
that t 7→ V (x(t, φ0), y(t, φ1)) is continuous. Therefore, every realization of t 7→ V (Φ˜t) is
right-continuous and has left-limits.
ADAPTIVE POISSON DISORDER PROBLEM 25
Let us define the F-stopping times
Uε , inf{t ≥ 0 : V (Φ˜t) ≥ −ε}, ε ≥ 0.(5.20)
By Remark 5.10, we have
V
(
Φ˜Uε
) ≥ −ε on the event {Uε <∞} .(5.21)
5.11. Proposition. Let Mt , e−λtV (Φ˜t) +
∫ t
0
e−λsg(Φ˜s)ds, t ≥ 0. For every n ∈ N, ε ≥ 0,
and (φ0, φ1) ∈ R2+, we have Eφ0,φ10 [M0] = Eφ0,φ10 [MUε∧σn ], i.e.,
V (φ0, φ1) = Eφ0,φ10
[
e−λ(Uε∧σn)V (Φ˜Uε∧σn) +
∫ Uε∧σn
0
e−λsg(Φ˜s)ds
]
.(5.22)
5.12. Proposition. For every ε ≥ 0, the stopping time Uε in (5.20) is ε-optimal for the
problem (4.12), i.e.,
Eφ0,φ10
[∫ Uε
0
e−λsg(Φ˜s)ds
]
≤ V (φ0, φ1) + ε, for every (φ0, φ1) ∈ R2+.
Proposition 5.5 above shows how we can calculate the Vn’s sequentially; it also identifies
explicitly ε-optimal times for every optimal stopping problem in (5.1). Together with Propo-
sition 5.1, it suggests a way to calculate ε-optimal alarm times: Let ε1 > 0 and ε2 ≥ 0 be
two arbitrary numbers such that ε1 + ε2 = ε > 0. Let us choose n ∈ N such that
√
2
c
·
(
µ
λ+ µ
)n
< ε1.(5.23)
Then we have Vn(φ0, φ1) − ε1 ≤ V (φ0, φ1) ≤ Vn(φ0, φ1) for every (φ0, φ1) ∈ R2+, and the
stopping time Sε2n of Proposition 5.5 is an ε-optimal stopping time for our original optimal
stopping problem (4.12) in the sense that
V (φ0, φ1) ≤ Eφ0,φ10
[∫ Sε2n
0
e−λtg
(
Φ˜t
)
dt
]
< V (φ0, φ1) + ε, (φ0, φ1) ∈ R2+.
Since n ∈ N satisfies (5.23), and Sε2n (ω) ≤ σn(ω) for all ω ∈ Ω, setting the alarm at the nth
jump of the process N (if this has not been triggered by Sε2n (ω) earlier) is not in error more
than ε.
In this section, we showed that the (more classical) stopping times Uε of (5.20) are also
ε-optimal for (4.12); especially the stopping time U0 is optimal, see Proposition 5.12.
26 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
6. A bound on the alarm time
We shall show that the optimal continuation region C = {(φ0, φ1) ∈ R2+ : V (φ0, φ1) < 0}
is contained in some set
D = {(φ0, φ1) ∈ R2+ : φ0 + φ1 < ξ∗} for a suitable ξ∗ ∈
[
λ+ µ
c
√
2,∞
)
.(6.1)
Therefore, the region C has compact closure; this will be very useful in proving in the next
section that C has a strictly decreasing convex boundary.
Recall from Section 4.1 that it is not optimal to stop before the process Φ˜ leaves the region
C0 in (4.13). Thus the optimal stopping time U0 of Proposition 5.12 is bounded by
τC0 , inf
{
t ≥ 0 : Φ˜(0)t + Φ˜(1)t ≥
λ
c
√
2
}
≤ U0 ≤ τD , inf{t ≥ 0 : Φ˜(0)t + Φ˜(1)t ≥ ξ∗}(6.2)
the exit times τC0 and τD of the process Φ˜ from the regions C0 and D, respectively. The
constant threshold ξ∗ in (6.1) is essentially determined by the number (λ + µ)
√
2/c (see
(6.5), (6.9) and (6.11)), and our calculations below suggest that they are close. Therefore,
the bounds in (6.2) may prove useful in practice. The difference [(λ+ µ)/c]
√
2− (λ/c)√2 =
(µ/c)
√
2 between the thresholds that determine the latest and the earliest alarm times is also
meaningful. It increases as µ/c increases: waiting longer is encouraged if the new information
arrives at a higher rate than we pay for detection delay per unit time when the disorder has
already happened.
Finally, we prove in Lemma 6.1 that τD in (6.2) has finite expectation. Therefore,
Eφ0,φ10 [U0] ≤ Eφ0,φ10 [τD] <∞ for every (φ0, φ1) ∈ R2+.
Let τ ∈ S be any F-stopping time. By Lemma 7.1, there is a constant t ≥ 0 such that
τ ∧ σ1 = t ∧ σ1 almost surely. Therefore
(6.3) Eφ0,φ10
[∫ τ
0
e−λsg
(
Φ˜s
)
ds
]
= Eφ0,φ10
[∫ τ∧σ1
0
e−λsg
(
Φ˜s
)
ds
]
+ Eφ0,φ10
[
1{τ≥σ1}
∫ τ
σ1
e−λsg
(
Φ˜s
)
ds
]
≥ Eφ0,φ10
[∫ t
0
1{s≤σ1}e
−λsg
(
x(s, φ0), y(s, φ1)
)
ds
]
−
√
2
c
· Eφ0,φ10
[
1{t≥σ1}e
−λσ1]
=
∫ t
0
e−(λ+µ)s
[
g
(
x(s, φ0), y(s, φ1)
)− µ
c
√
2
]
ds.
The inequality follows from g(φ0, φ1) ≥ g(0, 0) = −(λ/c)
√
2, see (4.12). The functions
x(·, φ0) and y(·, φ1) are the solutions of (4.7) (see Remark 4.1), and σ1 has exponential
ADAPTIVE POISSON DISORDER PROBLEM 27
λ+µ
c
√
2λc
√
2
λ
c
√
2 (x(t, φ0), y(t, φ1))
(φ0, φ1)
λ
c
√
2
λ
c
√
2
λ+µ
c
√
2 λ+µc
√
2λc
√
2
ξ∗
000
(φ∗0, φ
∗
1)
y
x
y y
x x
(x(−t∗, φ∗0), y(−t∗, φ∗1))
(c) 0 < λ1−λ < λ+µc (λ < 1)(b) λ1−λ ≥ λ+µc (λ < 1)(a) λ ≥ 1 (φd ≤ 0)
C0
`
C0C0
λ
c
√
2
φd
`
λ+µ
c
√
2
φd
λ
1−λ
√
2
(x(t, φ0), y(t, φ1))
(φ0, φ1)
λ+µ
c
√
2 λ+µ
c
√
2
λ
1−λ
√
2
R2+\D1 R2+\D1 R2+\D2
ξ∗
Figure 3. Region D
distribution with rate µ under P0. Clearly, if
0 < g
(
x(s, φ0), y(s, φ1)
)− µ
c
√
2 = x(s, φ0) + y(s, φ1)− λ+ µ
c
√
2, 0 < s <∞,(6.4)
then (6.3) implies that Eφ0,φ10
[∫ τ
0
e−λsg
(
Φ˜s
)
ds
]
> 0 for every F-stopping time τ 6= 0 almost
surely (since the filtration F is right-continuous, the probability of {τ ≥ 0} ∈ F0 equals zero
or one). Thus, “stopping immediately” is optimal at every (φ0, φ1) where (6.4) holds.
If λ ≥ 1, then s 7→ x(s, φ0) and s 7→ y(s, φ1) are increasing for every (φ0, φ1) ∈ R2+, see
(4.7) and Figure 3(a). Therefore, x(s, φ0) + y(s, φ1) > x(0, φ0) + y(0, φ1) = φ0+ φ1 for every
0 < s <∞. Hence, (6.4) holds, and therefore it is optimal to stop immediately outside the
region
D1 ,
{
(φ0, φ1) ∈ R2+ : φ0 + φ1 <
λ+ µ
c
√
2
}
if λ ≥ 1.(6.5)
Suppose now that 0 < λ < 1; equivalently, φd of (4.14) is positive. Then s 7→ x(s, φ0)
is increasing for every φ0 ∈ R+. For φ1 = φd, the derivative dy(s, φd)/ds in (4.7) vanishes
for every 0 < s < ∞. The mapping s 7→ y(s, φ1) is increasing if φ1 ∈ [0, φd), decreasing if
φ1 ∈ (φd,∞), and φ(s, φd) = φd for every 0 ≤ s < ∞; see (4.7) and Figures 3(b,c). The
derivative
d
dt
[
x(t, φ0) + y(t, φ1)
]
= (λ+ 1)x(t, φ0) + (λ− 1)y(t, φ1) + λ
√
2(6.6)
28 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
of the righthand side of (6.4) (see also (4.7)) vanishes if the curve s 7→ (x(s, φ0), y(s, φ1))
meets at s = t the line
` : (λ+ 1)x+ (λ− 1)y + λ
√
2 = 0, or y =
1 + λ
1− λ x+
λ
1− λ
√
2.(6.7)
Since m ∈ (−1, 1), the “mean-level” φd in (4.14) and the y-intercept of the line ` in (6.7) are
related as in
φd =
λ
1− λ ·
1 +m√
2
<
λ
1− λ
√
2.
Because ` is increasing, this relationship implies that the line ` in contained in R+× (φd,∞)
(see Figures 3(b,c)). However, every curve t 7→ (x(t, φ0), y(t, φ1)) starting at some (φ0, φ1) in
R+ × (φd,∞) is “decreasing”, and the derivative in (6.6) is increasing. Therefore, any curve
t 7→ (x(t, φ0), y(t, φ1)), (φ0, φ1) ∈ R2+ may meet ` at most once, and
if t 7→ (x(t, φ0), y(t, φ1)) meets the line ` at t` = t`(φ0, φ1), then t 7→ x(t, φ0) +
y(t, φ1) is decreasing (resp., increasing) on [0, t`] (resp., on [t`,∞)). Otherwise,
t 7→ x(t, φ0) + y(t, φ1) is increasing on [0,∞).
 .(6.8)
 Consider now the first of two possible cases: the line ` does not meet D1 of (6.5); i.e.,
λ/(1−λ) ≥ (λ+µ)/c, as in Figure 3(b). Then φ0+φ1 ≥ (λ+µ)
√
2/c for every (φ0, φ1) ∈ `.
Therefore, (6.8) implies that (6.4) holds, i.e., it is optimal to stop immediately, outside
D1 =
{
(φ0, φ1) ∈ R2+ : φ0 + φ1 <
λ+ µ
c
√
2
}
if
λ
1− λ ≥
λ+ µ
c
.(6.9)
 In the second case, the line ` of (6.7) meets the region D1, i.e., 0 < λ/(1− λ) < (λ+ µ)/c,
see Figure 3(c). Let us denote by (φ∗0, φ
∗
1) the point at the intersection of the line ` and the
boundary x+ y − (λ+ µ)√2/c = 0 of the region D1. By running the time “backwards”, we
can find ξ∗ (and t∗) such that
(0, ξ∗) =
(
x(−t∗, φ∗0), y(−t∗, φ∗1)
)
.(6.10)
Indeed, using (4.8), we can obtain first t∗ ≥ 0 by solving 0 = x(−t∗, φ∗0), and then ξ∗ ,
y(−t∗, φ∗1) . By the semi-group property (4.9), we have
x(t∗, 0) = x
(
t∗, x(−t∗, φ∗0)
)
= x(t∗ + (−t∗), φ∗0) = x(0, φ∗0) = φ∗0,
y(t∗, ξ∗) = y
(
t∗, y(−t∗, φ∗1)
)
= y(t∗ + (−t∗), φ∗1) = y(0, φ∗1) = φ∗1.
ADAPTIVE POISSON DISORDER PROBLEM 29
Hence, the curve t 7→ (x(t, 0), y(t, ξ∗)), t ≥ 0 meets ` at (φ∗0, φ∗1), and t` in (6.8) equals t∗,
see Figure 3(c). Therefore, (6.8) implies that
x(t, 0) + y(t, ξ∗) ≥ x(t∗, 0) + y(t∗, ξ∗) = φ∗0 + φ∗1 =
λ+ µ
c
√
2, 0 ≤ t <∞.
In particular, ξ∗ = 0 + ξ∗ = x(0, 0) + y(0, ξ∗) ≥ (λ + µ)√2/c. We are now ready to show
that it is optimal to stop immediately outside the region
D2 ,
{
(φ0, φ1) ∈ R2+ : φ0 + φ1 < ξ∗
}
if 0 <
λ
1− λ <
λ+ µ
c
,(6.11)
where ξ∗ is as in (6.10). The curve t 7→ (x(t, 0), y(t, ξ∗)) divides R2+ into two connected
components each containing the region D1 of (6.5) and
M ,
(
R2+\D2
) ∩ {(x, y) ∈ R2+ : (λ+ 1)x+ (λ− 1)y + λ√2 < 0} ,
respectively (see (6.7)). Every curve t 7→ (x(t, φ0), y(t, φ1)), t ≥ 0 starting at (φ0, φ1) in M
will stay in the same component as M . Therefore, the curve intersects the line ` away from
D1, and (6.8) implies that (6.4) is satisfied for every (φ0, φ1) ∈M .
For (φ0, φ1) ∈ (R2+\D2) ∩
{
(x, y) ∈ R2+ : (λ+ 1)x+ (λ− 1)y + λ
√
2 ≥ 0}, the curve t 7→(
x(t, φ0), y(t, φ1)
)
, t ≥ 0 does not meet `; therefore, t 7→ x(t, φ0) + y(t, φ1) increases by (6.8)
and
x(t, φ0) + y(t, φ1) > x(0, φ0) + y(0, φ1) = φ0 + φ1 ≥ ξ∗ ≥ λ+ µ
c
√
2, 0 < s <∞.
Thus, the sufficient condition (6.4) for the optimality of immediate stopping holds for every
(φ0, φ1) ∈ R2+\D2.
6.1. Lemma. Let τD be the exit time of the process Φ˜ from the region D in (6.1). Then
Eφ0,φ10 [τD] is finite for every (φ0, φ1) ∈ R2+.
Proof. Let f(φ0, φ1) , φ0 + φ1, (φ0, φ1) ∈ R2+. Using the explicit form of the infinitesimal
generator A˜ of the process Φ˜ in (7.4), we obtain
(6.12) A˜f(φ0, φ1) = (λ+ 1)φ0 + λ(1−m)√
2
+ (λ− 1)φ1 + λ(1 +m)√
2
+ µ
[(
1− 1
µ
)
φ0 +
(
1 +
1
µ
)
φ1 − (φ0 + φ1)
]
= λ(φ0 + φ1 +
√
2) ≥ λ
√
2
30 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
for every (φ0, φ1) ∈ R2+. Since f(·, ·) is bounded on D of (6.1) and τD ∧ t, t ≥ 0 is a bounded
F-stopping time, (7.3) holds for τ = τD ∧ t. Then we have
(6.13) ξ∗
(
1 +
1
µ
)
≥ Eφ0,φ10 f
(
Φ˜τD∧t
)
= f(φ0, φ1) + Eφ0,φ10
[∫ τD∧t
0
A˜f(Φ˜t)dt] ≥ λ√2Eφ0,φ10 [τD ∧ t], t ≥ 0.
The process Φ˜may leave the regionD in (6.1) continuously or by a jump. Since f(S(φ0, φ1)) =
(1+ 1/µ)φ0+ (1− 1/µ)φ1 ≤ (1+ 1/µ)(φ0+φ1) = (1+ 1/µ)f(φ0, φ1) ≤ (1+ 1/µ)ξ∗ for every
(φ0, φ1) ∈ D, and this upper bound is larger than ξ∗, the first inequality in (6.13) follows.
The second inequality is due to (6.12). Finally, the monotone convergence theorem and
(6.13) imply that Eφ0,φ10 [τD] is finite. 
7. Appendix: proofs of selected results in Part 1
The P0-infinitesimal generator A˜ of the process Φ˜ in (4.5). Let us denote by A˜ the infin-
itesimal generator under P0 of the process Φ˜ =
[
Φ˜(0) Φ˜(1)
]T
in (4.5). For every function
f ∈ C1,1(R+ × R+), we have
(7.1) f(Φ˜t) = f(Φ˜0) +
∑
0<s≤t
[
f(Φ˜s)− f(Φ˜s−)
]
+
∫ t
0
{
Dφ0f(Φ˜s)
[
(λ+ 1)Φ˜(0)s +
λ(1−m)√
2
]
+Dφ1f(Φ˜s)
[
(λ− 1)Φ˜(1)s +
λ(1 +m)√
2
]}
ds
and∑
0<s≤t
[
f(Φ˜s)− f(Φ˜s−)
]
=
∫ t
0
[
f
((
1− 1
µ
)
· Φ˜(0)s−,
(
1 +
1
µ
)
· Φ˜(1)s−
)
− f
(
Φ˜
(0)
s−, Φ˜
(1)
s−
)]
dNs.
Note that {Nt − µt; t ≥ 0} is a (P0,F)-martingale. Then for every F-stopping time τ such
that
Eφ0,φ10
∣∣∣f(Φ˜τ)∣∣∣ <∞ and
Eφ0,φ10
[∫ τ
0
∣∣∣∣f ((1− 1µ
)
· Φ˜(0)s−,
(
1 +
1
µ
)
· Φ˜(1)s−
)
− f
(
Φ˜
(0)
s−, Φ˜
(1)
s−
)∣∣∣∣ ds] <∞,(7.2)
we have
E0f(Φ˜τ ) = f(Φ˜0) + E0
∫ τ
0
A˜f(Φ˜s)ds, t ≥ 0,(7.3)
ADAPTIVE POISSON DISORDER PROBLEM 31
and
A˜f(φ0, φ1) = Dφ0f(φ0, φ1)
[
(λ+ 1)φ0 +
λ(1−m)√
2
]
+Dφ1f(φ0, φ1)
[
(λ− 1)φ1 + λ(1 +m)√
2
]
+ µ
[
f
((
1− 1
µ
)
φ0,
(
1 +
1
µ
)
φ1
)
− f(φ0, φ1)
]
, (φ0, φ1) ∈ R+ × R+.(7.4)
Proof of Lemma 5.3. Let w : R2+ 7→ R be a bounded Borel function. Since g(·, ·) ≥
g(0, 0) = −λ√2/c in (4.12) is bounded from below, the function J0w is well-defined. By
(5.7),
Jw(t, φ0, φ1) ≥ −
(
λ
c
√
2 + µ||w||
)∫ ∞
0
e−(λ+µ)udu = −
(
λ
c
√
2 + µ||w||
)
1
λ+ µ
for every t ∈ [0,∞]. Since we also have J0w(φ0, φ1) ≤ Jw(0, φ0, φ1) = 0, we obtain (5.9).
Suppose now that w is also concave. For every u ∈ R, the functions φ0 7→ x(u, φ0)
and φ1 7→ y(u, φ1) in (4.8) are linear. The mappings (φ0, φ1) 7→ S(φ0, φ1) in (5.8) and
(φ0, φ1) 7→ g(φ0, φ1) in (4.12) are also linear. Therefore, the integrand in (5.7), namely
(φ0, φ1) 7→ e−(λ+µ)u
(
g + µ · w ◦ S)(x(u, φ0), y(u, φ1)) is concave for every u ∈ [0,∞).
Thus, the mappings (φ0, φ1) 7→ Jw(t, (φ0, φ1)), t ∈ [0,∞] in (5.7) are concave. Then
J0w(φ0, φ1) = inft∈[0,∞] Jw(t, φ0, φ1) is a lower envelope of concave mappings, and there-
fore, is a concave function of (φ0, φ1) ∈ R2+. Finally, it is clear from (5.7) that w1 ≤ w2
implies that J0w1 ≤ J0w2. 
Proof of Corollary 5.4. The function v0 ≡ 0 has all of the properties. The proof of the
lemma now follows from an induction and the properties of concave functions. 
For the proof of Proposition 5.5, we shall need the following result on the characterization
of F-stopping times, see Bre´maud (1981, Theorem T33, p. 308), Davis (1993, Lemma A2.3,
p. 261).
7.1. Lemma. For every F-stopping time τ and every n ∈ N0, there is an Fσn-measurable
random variable Rn : Ω 7→ [0,∞] such that τ ∧ σn+1 = (σn + Rn) ∧ σn+1 holds P0-a.s. on
{τ ≥ σn}.
Proof of Proposition 5.5. First, we shall establish the inequality
Eφ0,φ10
∫ τ∧σn
0
e−λtg
(
Φ˜t
)
dt ≥ vn(φ0, φ1), τ ∈ S, (φ0, φ1) ∈ R2+(7.5)
32 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
for every n ∈ N0, by proving inductively on k = 1, . . . , n+ 1 that
(7.6) Eφ0,φ10
∫ τ∧σn
0
e−λtg
(
Φ˜t
)
dt
≥ Eφ0,φ10
[∫ τ∧σn−k+1
0
e−λtg
(
Φ˜t
)
dt+ 1{τ≥σn−k+1}e
−λσn−k+1vk−1
(
Φ˜σn−k+1
)]
=: RHSk−1.
Observe that (7.5) follows from (7.6) when we set k = n+ 1.
If k = 1, then the inequality (7.6) is satisfied as an equality since v0 ≡ 0. Suppose that
(7.6) holds for some 1 ≤ k < n+1. We shall prove that it must also hold when k is replaced
with k + 1. Let us denote the righthand side of (7.6) by RHSk−1, and rewrite it as
(7.7) RHSk−1 = RHS
(1)
k−1 +RHS
(2)
k−1 , E
φ0,φ1
0
[∫ τ∧σn−k
0
e−λtg
(
Φ˜t
)
dt
]
+ Eφ0,φ10
[
1{τ≥σn−k}
(∫ τ∧σn−k+1
σn−k
e−λtg
(
Φ˜t
)
dt+ 1{τ≥σn−k+1}e
−λσn−k+1vk−1
(
Φ˜σn−k+1
))]
where we used
∫ τ∧σn−k+1
0
=
∫ τ∧σn−k
0
+
∫ τ∧σn−k+1
τ∧σn−k =
∫ τ∧σn−k
0
+1{τ≥σn−k}
∫ τ∧σn−k+1
τ∧σn−k , as well as
1{τ≥σn−k}1{τ≥σn−k+1} = 1{τ≥σn−k+1}. By Lemma 7.1, there is an Fσn−k-measurable random
variable Rn−k such that
τ ∧ σn−k+1 = (σn−k +Rn−k) ∧ σn−k+1 holds P0-almost surely on {τ ≥ σn−k}.
Therefore, the second expectation, denoted by RSH
(2)
k−1, in (7.7) becomes
Eφ0,φ10
{
1{τ≥σn−k}
[∫ (σn−k+Rn−k)∧σn−k+1
σn−k
e−λtg
(
Φ˜t
)
dt+ 1{σn−k+Rn−k≥σn−k+1}·
e−λσn−k+1vk−1
(
Φ˜σn−k+1
)]}
= Eφ0,φ10
{
1{τ≥σn−k}e
−λσn−kfn−k(Rn−k, Φ˜σn−k)
}
by the strong Markov property of N , where
fk−1(r, φ0, φ1) , Eφ0,φ10
[∫ r∧σ1
0
e−λtg
(
Φ˜t
)
dt+ 1{r≥σ1}e
−λσ1vk−1
(
Φ˜σ1
)]
= Jvk−1(r, (φ0, φ1)) ≥ J0vk−1(φ0, φ1) = vk(φ0, φ1).
The (in)equalities follow from (5.3), (5.4) and (5.6), respectively. Thus
RHS
(2)
k−1 ≥ Eφ0,φ10
[
1{τ≥σn−k}e
−λσn−kvk
(
Φ˜σn−k
)]
.
ADAPTIVE POISSON DISORDER PROBLEM 33
From (7.6) and (7.7), we finally obtain
Eφ0,φ10
∫ τ∧σn
0
e−λtg
(
Φ˜t
)
dt ≥ RHSk−1 = Eφ0,φ10
[∫ τ∧σn−k
0
e−λtg
(
Φ˜t
)
dt
]
+RHS
(2)
k−1
≥ Eφ0,φ10
[∫ τ∧σn−k
0
e−λtg
(
Φ˜t
)
dt+ 1{τ≥σn−k}e
−λσn−kvk
(
Φ˜σn−k
)]
= RHSk.
This completes the proof of (7.6) by induction on k, and (7.5) follows by setting k = n + 1
in (7.6). When we take the infimum of both sides in (7.5), we obtain Vn ≥ vn, n ∈ N.
The reverse inequality Vn ≤ vn, n ∈ N follows immediately from (5.11) since every F-
stopping time Sεn is less than or equal to σn, P0-a.s by construction. Therefore, we only need
to establish (5.11). We will prove it by induction on n ∈ N. For n = 1, the lefthand side of
(5.11) becomes
Eφ0,φ10
∫ Sε1
0
e−λtg
(
Φ˜t
)
dt = Eφ0,φ10
∫ rε0(φ0,φ1)∧σ1
0
e−λtg
(
Φ˜t
)
dt = Jv0(r
ε
0(φ0, φ1), φ0, φ1).
Since Jv0(r
ε
0(φ0, φ1), φ0, φ1) ≤ J0v0(φ0, φ1) + ε by Remark 5.2, (5.11) holds for n = 1.
Suppose that (5.11) holds for every ε > 0 for some n ∈ N. We will prove that it also holds
when n is replaced with n+ 1. Since Sεn+1 ∧ σ1 = rε/2n
(
Φ˜0
) ∧ σ1, P0-a.s., we have
Eφ0,φ10
[∫ Sεn+1
0
e−λtg
(
Φ˜t
)
dt
]
= Eφ0,φ10
[∫ Sεn+1∧σ1
0
e−λtg
(
Φ˜t
)
dt+ 1{Sεn+1≥σ1}
∫ Sεn+1
σ1
e−λtg
(
Φ˜t
)
dt
]
= Eφ0,φ10
[∫ rε/2n (φ0,φ1)∧σ1
0
e−λtg
(
Φ˜t
)
dt
]
+ Eφ0,φ10
[
1{rε/2n (φ0,φ1)≥σ1}e
−λσ1fn
(
Φ˜σ1
)]
by the strong Markov property of N , where
fn(φ0, φ1) , Eφ0,φ10
[∫ Sε/2n
0
e−λtg
(
Φ˜t
)
dt
]
≤ vn(φ0, φ1) + ε/2
by the induction hypothesis. Therefore,
(7.8) Eφ0,φ10
[∫ Sεn+1
0
e−λtg
(
Φ˜t
)
dt
]
≤ Eφ0,φ10
[∫ rε/2n (φ0,φ1)∧σ1
0
e−λtg
(
Φ˜t
)
dt+
1{rε/2n (φ0,φ1)≥σ1}e
−λσ1vn
(
Φ˜σ1
)]
+ ε/2 = Jvn(r
ε/2
n (φ0, φ1), (φ0, φ1)) + ε/2.
Since Jvn(r
ε/2
n (φ0, φ1), (φ0, φ1)) ≤ vn+1(φ0, φ1)+ε/2 by Remark 5.2, this inequality and (7.8)
prove (5.11) when n is replaced with n. 
34 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Proof of Proposition 5.6. Corollary 5.4 and Propositions 5.5 and 5.1 imply that v(φ0, φ1) =
limn→∞ vn(φ0, φ1) = limn→∞ Vn(φ0, φ1) = V (φ0, φ1) for every (φ0, φ1) ∈ R2+. Next, let us
show that V = J0V . Since (vn)n≥1 is a decreasing sequence, for every (φ0, φ1) ∈ R2+
V (φ0, φ1) = lim
n→∞
vn(φ0, φ1) = inf
n≥1
vn(φ0, φ1) = inf
n≥1
J0vn−1(φ0, φ1).(7.9)
Since (Jvn)n≥1 is a decreasing sequence, and {vn}n∈N are uniformly bounded, the dominated
convergence theorem and (7.9) imply that V (φ0, φ1) = infn≥1 J0vn−1(φ0, φ1) = J0v(φ0, φ1) =
J0V (φ0, φ1). Finally, since U ≤ 0, we have U ≤ vn for every n by induction, and U ≤
limn→∞ vn = V . 
Proof of Lemma 5.7. Let us fix a constant u ≥ t and (φ0, φ1) ∈ R2+. Then
(7.10) Jw(u, φ0, φ1) = Eφ0,φ10
[∫ u∧σ1
0
e−λsg(Φ˜s)ds+ 1{u≥σ1}e
−λσ1w(Φ˜σ1)
]
= Eφ0,φ10
[∫ t∧σ1
0
e−λsg(Φ˜s)ds+ 1{u≥σ1}e
−λσ1w(Φ˜σ1)
]
+ Eφ0,φ1
[
1{σ1>t}
∫ u∧σ1
t
e−λsg(Φ˜s)ds
]
.
On the event {σ1 > t}, we have u∧σ1 = [t+(u− t)]∧ [t+(σ1 ◦ θt)] = t+ [(u− t)∧ (σ1 ◦ θt)].
Therefore, the strong Markov property of N applied to the last integral above, gives
(7.11) Eφ0,φ10
[
1{σ1>t}
∫ u∧σ1
t
e−λsg(Φ˜s)ds
]
= Eφ0,φ10
[
1{σ1>t}
∫ u∧σ1−t
0
e−λ(s+t)g(Φ˜s+t)ds
]
= Eφ0,φ10
[
1{σ1>t}e
−λtEeΦt0
[∫ (u−t)∧σ1
0
e−λsg(Φ˜s)ds
]]
= Eφ0,φ10
[
1{σ1>t}e
−λt
(
Jw(u− t, Φ˜t)− EeΦt0
[
1{u−t≥σ1}e
−λσ1w(Φ˜σ1)
])]
= e−(λ+µ)t Jw
(
u− t, (x(t, φ0), y(t, φ0))
)− Eφ0,φ10 [1{σ1>t}1{u≥σ1}e−λσ1w(Φ˜σ1)] .
The third equality follows from the definition of Jw in (5.3) and the last from (4.10) and
the strong Markov property. Substituting (7.11) into (7.10) and simplifying the rest give
Jw(u, φ0, φ1) = Jw(t, (φ0, φ1)) + e
−(λ+µ)t Jw
(
u− t, (x(t, φ0), y(t, φ0))
)
.
Finally, taking the infimum of both sides over u ∈ [t,+∞] gives (5.12). 
ADAPTIVE POISSON DISORDER PROBLEM 35
Proof of Proposition 5.11. First, let us show (5.22) for n = 1. Fix ε ≥ 0 and (φ0, φ1) ∈
R2+. By Lemma 7.1, there exists a constant u ∈ [0,∞] such that Uε ∧ σ1 = u ∧ σ1. Then
(7.12) Eφ0,φ10 MUε∧σ1 = E
φ0,φ1
0
[
e−λ(u∧σ1)V (Φ˜u∧σ1) +
∫ u∧σ1
0
e−λsg(Φ˜s)ds
]
= Eφ0,φ10
[∫ u∧σ1
0
e−λsg(Φ˜s)ds+ 1{u≥σ1}e
−λσ1V (Φ˜σ1)
]
+ Eφ0,φ10
[
1{u<σ1}e
−λuV (Φ˜u)
]
= JV (u, (φ0, φ1)) + e
−(λ+µ)uV
(
x(u, φ0), y(u, φ1)
)
= JuV (φ0, φ1),
where the third equality follows from (5.3) and (4.10), and the fourth from (5.16).
Fix any t ∈ [0, u). By (5.16) and (4.10) once again, we have
JV (t, φ0, φ1) = JtV (φ0, φ1)− e−(λ+µ)tV (x(t, φ0), y(t, φ1))
≥ J0V (φ0, φ1)− e−(λ+µ)tV (x(t, φ0), y(t, φ1)) = J0V (φ0, φ1)− Eφ0,φ10
[
1{σ1>t}e
−λtV (Φ˜t)
]
.
On the event {σ1 > t}, we have Uε > t (otherwise, Uε ≤ t < σ1 would imply Uε = u ≤ t,
which contradicts with our initial choice of t < u). Thus, V (Φ˜t) < −ε on {σ1 > t}. Hence,
JV (t, φ0, φ1) ≥ J0V (φ0, φ1) + εEφ0,φ10
[
1{σ1>t}e
−λt] ≥ J0V (φ0, φ1) + ε e−(λ+µ)u
for every t ∈ [0, u). Therefore, J0V (φ0, φ1) = JuV (φ0, φ1), and (7.12) implies
Eφ0,φ10 [MUε∧σ1 ] = JuV (φ0, φ1) = J0V (φ0, φ1) = V (φ0, φ1) = E
φ0,φ1
0 [M0].
This completes the proof of (5.22) for n = 1.
Now suppose that (5.22) holds for some n ∈ N, and let us show the same equality for
n+ 1. Note that
Eφ0,φ10 [MUε∧σn+1 ] = E
φ0,φ1
0 [1{Uε<σ1}MUε ] + E
φ0,φ1
0
[
1{Uε≥σ1}
∫ σ1
0
e−λsg(Φ˜s)ds
]
+ Eφ0,φ10
[
1{Uε≥σ1}
{∫ Uε∧σn+1
σ1
e−λsg(Φ˜s)ds+ e−λ(Uε∧σn+1)V (Φ˜Uε∧σn+1)
}]
.
Since Uε ∧ σn+1 = σ1 + [(Uε ∧ σn) ◦ θσ1 ] on the event {Uε ≥ σ1}, the strong Markov property
of Φ˜ at the stopping time σ1 will complete the proof. 
Proof of Proposition 5.12. Note that the sequence of random variables∫ Uε∧σn
0
e−λsg(Φ˜s)ds+ e−λ(Uε∧σn)V (Φ˜Uε∧σn) ≥ −2
∫ ∞
0
e−λs
λ
c
√
2 ds = −2
√
2
c
36 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
is bounded from below, see (4.12). By (5.22) and Fatou’s Lemma, we have
V (φ0, φ1) ≥ Eφ0,φ10
[
lim
n→∞
(∫ Uε∧σn
0
e−λsg(Φ˜s)ds+ e−λ(Uε∧σn)V (Φ˜Uε∧σn)
)]
≥ Eφ0,φ10
[∫ Uε
0
e−λsg(Φ˜s)ds
]
− ε Eφ0,φ10
[
1{Uε<∞}e
−λUε] ≥ Eφ0,φ10 [∫ Uε
0
e−λsg(Φ˜s)ds
]
− ε
for every (φ0, φ1) ∈ R2+. The second inequality follows from (5.21). 
ADAPTIVE POISSON DISORDER PROBLEM 37
Part 2. SYNTHESIS: SOLUTION AND NUMERICAL METHODS
8. The solution
In Proposition 5.1, we showed that the value function V (φ0, φ1) of our original optimal
stopping problem in (4.12) is approximated uniformly in (φ0, φ1) ∈ R2+ by the decreasing
sequence {Vn(φ0, φ1)}n∈N of the value functions of the optimal stopping problems in (5.1).
The value functions Vn(·, ·) = vn(·, ·), n ∈ N can be calculated sequentially by setting v0 ≡ 0,
and
vn+1(φ0, φ1) = J0vn(φ0, φ1) = inf
t∈[0,∞]
Jvn(t, φ0, φ1), (φ0, φ1) ∈ R2+,(8.1)
where the operator J is defined in (5.3); see Proposition 5.5.
Finding the infimum in (8.1) is not as formidable as it may look. By Proposition 5.5,
the infimum in (8.1) is always attained (i.e., the case ε = 0 in (5.11)). By Corollary 5.8,
it is attained at the exit time rn(φ0, φ1) of the deterministic and continuous curve t 7→
(x(t, φ0), y(t, φ1)) in (4.7) from the set{
(φ0, φ1) ∈ R2+ : vn+1(φ0, φ1) < 0
} ⊆ {(φ0, φ1) ∈ R2+ : v(φ0, φ1) < 0} ⊆ D,
where D is the triangular region in (6.1), and the last inclusion is proven in Section 6.
Therefore, the search for the infimum in (8.1) can be confined for every n ∈ N to
Jvn(t, φ0, φ1) =
∫ t
0
e−(λ+µ)u[g + µ · vn ◦ S](x(u, φ0), y(u, φ1))du, t ∈ [0, r(φ0, φ1)](8.2)
over the interval t ∈ [0, r(φ0, φ1)], where
r(φ0, φ1) , inf{t ≥ 0 : x(t, φ0) + y(t, φ1) ≥ ξ∗}, (φ0, φ1) ∈ R2+
is the (bounded) exit time of the curve t 7→ (x(t, φ0), y(t, φ1)) out of the region D in (6.1).
Finally, the error in approximating V (·, ·) of (4.12) by {vn(·, ·)}n∈N in (8.1) can be con-
trolled. For every ε > 0,
√
2
c
(
µ
λ+ µ
)n
< ε =⇒ −ε ≤ V (φ0, φ1)− vn(φ0, φ1) ≤ 0, ∀ (φ0, φ1) ∈ R2+,(8.3)
by Propositions 5.1 and 5.5. The exponential rate of the uniform convergence of {vn(·, ·)}n∈N
to V (·, ·) on R2+ in (8.3) may also reduce the computational burden by allowing relatively
small number of iterations in (8.1).
In the remainder, we draw attention to certain special cases where the exact value function
V (·, ·) can be calculated gradually at each iteration in (8.1); see Proposition 9.3. In the
38 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
meantime, we will give a precise geometric description of the stopping regions
Γn , {(φ0, φ1) ∈ R2+ : vn(φ0, φ1) = 0}, Cn , R2+\Γn, n ∈ N,(8.4)
Γ , {(φ0, φ1) ∈ R2+ : v(φ0, φ1) = 0}, C , R2+\Γ,(8.5)
and describe the optimal stopping strategies.
9. The structure of the stopping regions
By Proposition 5.12, the set Γ is the optimal stopping region for the problem (4.12).
Namely, stopping at the first hitting time U0 = inf{t ∈ R+ : Φ˜t ∈ Γ} of the process
Φ˜ = (Φ˜(0), Φ˜(1)) to the set Γ is optimal for (4.12).
Similarly, we shall call each set Γn, n ∈ N a stopping region for the family of optimal
stopping problems in (5.1). However, unlike the case above, we need the first n stopping
regions, Γ1, . . . ,Γn, in order to describe an optimal stopping time for the problem in (5.1).
Using Corollary 5.8, the optimal stopping time Sn ≡ S0n in Proposition 5.5 for Vn of (5.1)
may be described as follows: Stop if the process Φ˜ hits Γn before N jumps. If N jumps before
Φ˜ reaches Γn, then wait, and stop if Φ˜ hits Γn−1 before the next jump of N , and so on. If
the rule is not met before (n− 1)st jump of N , then stop at the earliest of the hitting time of
Γ1 and the next jump time of N . See Figure 4(b) for three realizations of the stopping time
S2.
We shall call each Cn , R2+\Γn, n ∈ N a continuation region for the family of optimal
stopping problems in (5.1), and C , R2+\Γ the optimal continuation region for (4.12). The
stopping regions are related by
R2+\D ⊂ Γ ⊂ · · · ⊂ Γn ⊂ Γn−1 ⊂ · · ·Γ1 ⊂ R2+\C0, and Γ =
∞⋂
n=1
Γn,(9.1)
since the sequence of nonpositive functions {vn}n∈N is decreasing, and v = limn→∞ ↓ vn
by Lemma 5.4. The sets D and C0 are defined in (6.1) and (4.13), respectively. Since vn,
n ∈ N and v are concave and continuous mappings from R2+ into (−∞, 0] by Lemma 5.4,
the stopping regions Γn, n ∈ N and Γ are convex and closed. Let us define the functions
γn : R+ 7→ R+, n ∈ N and γ : R+ 7→ R+ by (see, also, Figure 4(a))
γn(x) , inf{y ∈ R+ : (x, y) ∈ Γn}, x ∈ R+,
γ(x) , inf{y ∈ R+ : (x, y) ∈ Γ}, x ∈ R+,
ADAPTIVE POISSON DISORDER PROBLEM 39
γn(0)
γ(0)
ξ∗ξξnξn−10
ξ∗
C0
λ
c
√
2
γn−1(·)
γn(·)
γ(·)
λ
c
√
2
Γ
Γn
Γn−1
γn−1(0)
0
Φ˜(ω1)
Φ˜(ω2)
Γ1 Γ2 Γ
Φ˜(ω3)
Φ˜S2(ω)(ω)
Φ˜U0(ω)(ω)
(a) (b)
R2+\D
Figure 4. (a) The stopping regions (each arrow at the boundary of a region points toward
the interior of that region), and (b) three sample paths and the optimal stopping times S2
and U0 for the optimal stopping problems V2 in (5.1) and V in (4.12), respectively.
and the numbers
ξn , inf{x ∈ R+ : γn(x) = 0}, n ∈ N and ξ , inf{x ∈ R+ : γ(x) = 0}.
Then the stopping regions Γn, n ∈ N and Γ are the convex and closed epigraphs of the
functions γn(·), n ∈ N and γ(·), respectively. Therefore, γn(·), n ∈ N and γ(·) are convex
and continuous mappings from R+ into R+.
By the set-inclusions in (9.1), we have (λ/c)
√
2 ≤ ξn−1 ≤ ξn ≤ ξ ≤ ξ∗ for the same
ξ∗ ∈ R+ in the description (6.1) of the set D. Since vn, n ∈ N and v vanish on R+\D =
{(φ0, φ1) ∈ R2+ : φ0+φ1 ≥ ξ∗} by (9.1), the functions γn(·), n ∈ N and γ(·) vanish on [ξ∗,∞).
However, ξn and ξ are the smallest zeros of the continuous functions γn(·), n ∈ N and γ(·),
respectively. Since both functions are also nonnegative and convex, the function γn(·), n ∈ N
(resp. γ(·)) equals zero on [ξn,∞) (resp. on [ξ,∞)) and is strictly decreasing on [0, ξn] (resp.
on [0, ξ]). For future reference, we now summarize our results.
9.1. Proposition. There are decreasing, convex and continuous mappings γn : R+ 7→ R+,
n ∈ N and γ : R+ 7→ R+ such that
Γn = {(φ0, φ1) ∈ R2+ : φ1 ≥ γn(φ0)}, n ∈ N and Γ = {(φ0, φ1) ∈ R2+ : φ1 ≥ γ(φ0)}.
40 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
The sequence {γn(φ0)}n∈N is increasing and γ(φ0) = lim ↑ γn(φ0) for every φ0 ∈ R+. There
are some numbers
λ
c
√
2 ≤ ξ1 ≤ · · · ≤ ξn−1 ≤ ξn ≤ · · · ≤ ξ < ξ∗ <∞(9.2)
such that γn(·), n ∈ N (resp., γ(·)) is strictly decreasing on [0, ξn], n ∈ N (resp., [0, ξ]), and
equals zero on [ξn,∞), n ∈ N (resp., [ξ,∞)). Moreover,
λ
c
√
2 ≤ γ1(0) ≤ · · · ≤ γn−1(0) ≤ γn(0) ≤ · · · ≤ γ(0) < ξ∗ <∞.(9.3)
The number ξ∗ is the same as in the definition of the set D in (6.1).
9.2. Notation. Let S : R2+ 7→ R2+ be the same linear map as in (5.8).
(N1) For any subset R ⊆ R2+,
S−(n+1)(R) , S−1(S−n(R)), n ∈ N, S−1(R) , {(x, y) ∈ R2+ : S(x, y) ∈ R},
Sn+1(R) , S(Sn(R)), n ∈ N, S(R) , {S(x, y) ∈ R2+ : (x, y) ∈ R},
and S0(R) = S(S−1(R)) = S−1(S(R)) = R.
(N2) For every singleton {(x, y)} ⊆ R2+, we write
Sm({x, y}) = Sm(x, y) =
((
1− 1
µ
)m
x,
(
1 +
1
µ
)m
y
)
, m ∈ Z.
(N3) For any function g : R+ 7→ R+, we define the function Sn[g] : R+ 7→ R+, n ∈ Z by
Sn[g](x) , inf{y ∈ R+ : (x, y) ∈ Sn(epi(g))}, x ∈ R+.
That is, Sn[g] is the function whose epigraph is the set Sn(epi(g)). Note that we use
Sn(·) and Sn[·] to distinguish the sets and the functions.
(N4) For every subset R of R2+, we denote by cl(R) its closure in R2+ and by int(R) its
interior. We shall denote the support of a function g : R+ 7→ R+ by
supp(g) = cl({x ∈ R+ : g(x) > 0}).
The process Φ˜ jumps into the region Γ (resp., S−n(Γ), n ∈ N) if the process N jumps
while Φ˜ is in the region S−1(Γ) (resp., S−(n+1)(Γ), n ∈ N). Clearly, if the process Φ˜ can
never leave the region S−1(Γ) before a jump, then the value functions V (·, ·) and V1(·, ·) in
(5.1) must coincide on the region S−1(Γ).
9.3. Proposition. Suppose that
∀n ∈ N : (φ0, φ1) ∈ S−n(Γ) =⇒ (x(t, φ0), y(t, φ1)) ∈ S−n(Γ), t ∈ [0,∞)(9.4)
ADAPTIVE POISSON DISORDER PROBLEM 41
( µµ+1)
nγ(0)
ξn(
µ
µ−1)
nξ
Γn
S−n
Cn
Γ ξ∗
ξ∗
( µµ+1)
nγn(0)
B
= γ1(0)
S−n[γn](·)
γn(0)
C
S−n
ξn (
µ
µ−1)
nξn
(b)
S−n[γ](·)
(a)
γ(0)
0 ξ 0 ξ
(xn, γ(xn))
S−n(xn, γ(xn))
γ(·) = γn(·) on [0, xn]
γ(·)
xn
γn(·)
(xn, γn(xn))
xn
γn(·) γ(·)
V (·, ·) = vn(·, ·) on R+ × [B,∞)
S−n(Γ) ∩ Γ = S−n(Γ) ∩ Γn
S−n(Γ) ∩ C = S−n(Γ) ∩ Cn
Figure 5. Here we assume that (9.4) holds. In (a), the dashed curve is the non-zero part
of the boundary function S−n[γ](·) of the region S−n(Γ), see (9.6). The region S−n(Γ) is
obtained by “shifting” Γ to “down and right.” At x = xn, the functions γ(·) and S−n[γn](·)
meet for the first time, see (9.9). By Proposition 9.3, the value functions V (·, ·) and Vn(·, ·)
are equal on S−n(Γ). Therefore, the boundry functions γ(·) and γn(·) coincide on [0, xn] (the
thick continuous curve). The image under S−n of the common part stretches beyond [0, xn]
(the thick dashed curve). Hence, the triangular region on [0, xn] belongs to both S
−n(Γ)∩C
and S−n(Γn)∩Cn. In (b), we describe how to calculate V (·, ·) on R+× [B,∞) for any B > 0
by the three-step method on page 43. Here, the number n is the smallest satisfying (9.9).
holds. Then for every n ∈ N, we have

V (φ0, φ1) = Vn(φ0, φ1) = Vn+1(φ0, φ1) = · · · for every (φ0, φ1) ∈ S−n(Γ)
S−n(Γ) ∩ Γ = S−n(Γ) ∩ Γn = S−n(Γ) ∩ Γn+1 = · · ·
S−n(Γ) ∩C = S−n(Γ) ∩Cn = S−n(Γ) ∩Cn+1 = · · ·
 .(9.5)
Since Γ and Γn are convex and closed, and S(·, ·) is a linear mapping, the sets S−n(Γ)
and S−n(Γn), n ∈ N are convex and closed. The sets Γ and Γn, n ∈ N are the epigraphs of
the continuous functions γ(·) and γn(·), n ∈ N in Proposition 9.3, respectively. Therefore,
S−n(Γ) = {(x, y) ∈ R2+ : y ≥ S−n[γ](x)} and
S−n(Γn) = {(x, y) ∈ R2+ : y ≥ S−n[γn](x)}
(9.6)
42 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
are the epigraphs of the functions S−n[γ](·) and S−n[γn](·) for every n ∈ N0. These functions
are decreasing, continuous and convex. In fact,
S−n[γ](x) =
(
µ
µ+ 1
)n
γ
((
µ− 1
µ
)n
x
)
, x ∈ R+, n ∈ Z,(9.7)
and the function S−n[γn](·) is obtained by replacing γ with γn in (9.7). The support of the
functions S−n[γ](·) and S−n[γn](·) are
supp(S−n[γ]) =
[
0,
(
µ
µ− 1
)n
ξ
]
and supp(S−n[γn]) =
[
0,
(
µ
µ− 1
)n
ξn
]
(9.8)
respectively, for every n ∈ Z. By Proposition 9.1, the functions S−n[γ](·) and S−n[γn](·) are
strictly decreasing on their supports; see Figure 5.
Since S−n[γ](0) = (µ/(µ+ 1))nγ(0) < γ(0) and S−n[γ](ξ) > 0 = γ(ξ) for every n ∈ N, the
functions S−n[γ](·) and γ(·) intersect, and
xn(γ) , min{x ∈ R+ : S−n[γ](x) = γ(x)} ∈ (0,∞), n ∈ N.(9.9)
9.4. Corollary. Suppose that (9.4) holds. Then xn ≡ xn(γ) = xn(γk), k ≥ n ∈ N, and
S−n(Γ) ∩C ∩ ([0, xn]× R+) = S−n(Γk) ∩Ck ∩ ([0, xn]× R+) , k ≥ n, n ∈ N.(9.10)
Particularly, we have γ(x) = γn(x) for every x ∈ [0, xn], and
V (x, y) = Vn(x, y) for every (x, y) ∈ S−n(Γn) ∩Cn ∩ ([0, xn]× R+), n ∈ N.(9.11)
Proof. Let us fix any k ≥ n ∈ N. Since the value functions V (·, ·) and Vk(·, ·) are equal on
the region S−n(Γ) by Proposition 9.3, the boundaries of the regions Γ and Γk coincide in
the region S−n(Γ). Particularly, we have
γ(x) = γk(x) for every x ∈ [0, xn(γ)](9.12)
since S−n[γ](x) < γ(x) for every x ∈ [0, xn(γ)). Therefore,
S−n[γ](x) = S−n[γk](x) for every x ∈
[
0,
(
µ
µ− 1
)n
xn(γ)
]
⊃ [0, xn(γ)],(9.13)
Now, (9.12) and (9.13) imply that xn(γ) = xn(γk) , and (9.6) implies that (see also Figure 5
for the case k = n)
S−n(Γ) ∩C ∩ ([0, xn(γ)]× R+) = S−n(Γk) ∩Ck ∩ ([0, xn(γ)]× R+) .
The equality (9.11) follows immediately from Proposition 9.3. 
ADAPTIVE POISSON DISORDER PROBLEM 43
The identity in (9.11) suggests that, in a finite number of iterations of (8.1), we can find the
restrictions of the value function V (·, ·) and the continuation region C to the set R+× [B,∞)
for any B > 0, when the condition (9.4) holds:
Step A.1: Calculate the value function v1(0, y) for every y ∈ [0, ξ∗], and determine
γ(0) = γ1(0) = inf{y ∈ R+ : v1(x, y) = 0} ∈ (0, ξ∗); see (9.3), Corollary 9.4 and
Figure 5.
Step A.2: Given any B > 0, find the smallest n ∈ N such that
B >
(
µ
µ+ 1
)n
γ1(0) =
(
µ
µ+ 1
)n
γn(0) = S
−n[γn](0).(9.14)
Because every S−m[γm](·), m ∈ N is decreasing, this implies R+× [B,∞) ⊂ S−n(Γn);
see (9.6). We also have n ≤ min{m ∈ N : B > (µ/(µ+ 1))mξ∗} since γ1(0) ∈ (0, ξ∗).
Step A.3: Calculate vn(φ0, φ1) for every (φ0, φ1) ∈ R+\D by (8.1), where D is as in
(6.1). By (9.1), D ⊆ Γn and vn ≡ 0 on D.
Then the value functions V (·, ·) and vn(·, ·) are equal on R+×[B,∞) and (R+×[B,∞))∩C =
(R+ × [B,∞)) ∩Cn. See also Figure 5(b).
The next lemma implies that we can calculate the exact value function V (·, ·) under
condition (9.4) on the set R+ × (0,∞) along an increasing sequence of sets R+ × [Bn,∞),
and on R+ × {0} by the continuity of the function V (·, ·) on R2+.
9.5. Lemma. Suppose that (9.4) holds. Let ξ∗ be the same number as in the definition of
the region D in (6.1). Then limn→∞ S−n(Γ) = R+ × (0,∞), and
R+ ×
[(
µ
1 + µ
)n
· ξ∗,+∞
)
⊆ S−n(Γ), n ∈ N.(9.15)
Proof. Recall from (9.1) that R+ × [ξ∗,∞) ⊂ R2+\D ⊂ Γ. The rectangle on the lefthand
side in (9.15) is the same set as S−n
(
R+ × [ξ∗,∞)
) ⊂ S−n(Γ). But, (9.15) implies that
R+ × (0,∞) ⊆ limn→∞ S−n(Γ).
On the other hand, for every x ∈ R+, there exists number N(x) such that Sn(x, 0) =
((1 − 1/µ)nx, 0) /∈ Γ, n ≥ N(x). Then (x, 0) /∈ S−n(Γ) for every n ≥ N(x). This implies
that limn→∞ S−n(Γ) ⊆ R+ × (0,∞). 
9.6. Remark. Every set S−n(Γ), n ∈ N is separated from its complement by the strictly
decreasing, convex and continuous function S−n[γ](x), x ∈ [0, (µ/(µ − 1))nξ]. Therefore,
the condition (9.4) will be satisfied, for example, if the mappings t 7→ x(t, φ0), t ∈ R+ and
t 7→ y(t, φ1), t ∈ R+ are increasing for every (φ0, φ1) ∈ R2+. We have seen on page 20 that
this is always the case when λ is “large”.
44 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Thus, if λ is “large”, then there is a sequence of sets R+ × [Bn,∞), n ∈ N, increasing to
R+ × (0,∞) in the limit, such that V (·, ·) = vn(·, ·) on R+ × [Bn,∞) for every n ∈ N. See
also page 49 below.
10. The boundaries of the stopping regions
We shall show that the optimization in (8.1) can be avoided in principle, and v1, v2, . . .
can be calculated by integration.
Note that we obtain Jvn(t, φ0, φ1) in (8.1) by integrating the function [g + µ · vn ◦ S](·, ·)
along the curve u 7→ (x(u, φ0), y(u, φ1)) on u ∈ [0, t]; see (5.3). Therefore, the infimum in
(8.1) is determined by the the excursions of u 7→ (x(u, φ0), y(u, φ1)), u ∈ R+ into the regions
where the sign of the continuous mapping [g + µ · vn ◦ S](·, ·) is negative and positive.
10.1. Lemma. For every n ∈ N, we have
An , {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) < 0} ⊆ Cn+1.(10.1)
Proof. Let (φ0, φ1) ∈ An. Since the function u 7→ [g + µ · vn ◦ S](x(u, φ0, ), y(u, φ1)) is
continuous, there exists some t = t(φ0, φ1) > 0 such that
Jvn(t, φ0, φ1) =
∫ t
0
e−(λ+µ)u[g + µ · vn ◦ S](x(u, φ0, ), y(u, φ1))du < 0.
Therefore, vn+1(φ0, φ1) = J0vn(φ0, φ1) ≤ Jvn(t, φ0, φ1) < 0, and (φ0, φ1) ∈ Cn+1. 
For certain cases, the regions An and Cn+1 coincide, that is, the continuation region Cn+1
for vn+1(·, ·) can be found immediately when the value function vn(·, ·) is available. Then
vn+1 ≡ 0 on Γn+1 = R2+\Cn+1, and we calculate vn+1(·, ·) on Cn+1 by the integration
(10.2) vn+1(φ0, φ1) = Jvn(t, φ0, φ1)
∣∣∣
t=rn(φ0,φ1)
=
∫ rn(φ0,φ1)
0
e−(λ+µ)u[g + µ · vn ◦ S](x(u, φ0, ), y(u, φ1))du, (φ0, φ1) ∈ Cn+1,
of the function [g+µ·vn◦S](·, ·) over the curve (x(·, φ0), y(·, φ1)) until the exit time rn(φ0, φ1),
see (5.14), of the continuous (and deterministic) curve u 7→ (x(u, φ0), y(u, φ1)), u ∈ R+ from
the continuation region Cn+1.
The region An in (10.1) has properties very similar to those of the continuation region
Cn+1, compare Lemma 10.2 and Proposition 9.1. For example, both sets are separated from
their complements by a strictly decreasing, convex and continuous function which stays flat
on the x-axis for all large x values.
ADAPTIVE POISSON DISORDER PROBLEM 45
For every n ∈ N, let us define the function an : R+ 7→ R+ by
an(x) , inf{y ∈ R+ : (x, y) ∈ R2+\An} = inf{y ∈ R+ : [g + µ · vn ◦ S](x, y) ≥ 0}.(10.3)
The function an(·) is finite since, given any x ∈ R+, we have [g + µ · vn ◦ S](x, y) > 0 for
every large y ∈ R+. Recall that the function vn(·, ·) equals zero outside the bounded region
Cn. The linear mapping S : R2+ 7→ R2+ in (5.8) is increasing in both x and y. The affine
mapping g : R2+ 7→ R in (4.12) is also increasing and grows unboundedly in both x and y.
Similarly, given any large x ∈ R+, [g + µ · vn ◦ S](x, y) ≥ 0 for every y ∈ R+. Therefore,
an(x) = 0 for every x ∈ [M,∞) for some M ∈ R+, and the smallest number M
αn , inf{x ≥ 0 : an(x) = 0} is finite.(10.4)
The set R2+\An = {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) ≥ 0} is convex and closed since
vn(·, ·) is concave and continuous, S(·, ·) is linear, and g(·, ·) is affine. Because R2+\An is the
epigraph of an(·), this implies that an(·) is a convex and continuous mapping from R+ into
R+.
The function an(·) does not vanish identically on R+; in particular, an(0) > 0 since the
continuous function [g + µ · vn ◦ S](x, y) is strictly negative at (x, y) = (0, 0):
[g + µ · vn ◦ S](0, 0) = g(0, 0) + µ · vn(0, 0) ≤ g(0, 0) = −λ
c
√
2 < 0.
Because an(·) is continuous, this implies that the number αn in (10.4) is strictly positive.
Since an(·) is convex and vanishes for every large x ∈ R+, it is strictly decreasing on [0, αn),
and equals zero on [αn,∞).
10.2. Lemma. For every n ∈ N, there are a number αn ∈ (0,∞) and a strictly decreasing,
convex and continuous mapping an : [0, αn] 7→ R+ such that an(αn) = 0, and
{(x, an(x)); x ∈ [0, αn]} =
{
(x, y) ∈ R2+; [g + µ · vn ◦ S](x, y) = 0
}
.(10.5)
Moreover, the continuous mapping (x, y) 7→ [g+ µ · vn ◦ S](x, y), n ∈ N is strictly increasing
in each argument, and for every n ∈ N
{(x, y) ∈ [0, αn)× R+; y < an(x)} =
{
(x, y) ∈ R2+; [g + µ · vn ◦ S](x, y) < 0
} ≡ An.(10.6)
Next, we shall relate the regions An in (10.1) and Cn+1, and their boundaries an(·) and
γn+1(·), respectively, for every n ∈ N.
46 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Using the characterization of the stopping regions Γn, n ∈ N in Proposition 9.1 in terms
of the switching curves γn(·), the exit time rn(·, ·) in Corollary 5.8 can be expressed as
rn(φ0, φ1) = inf
{
t > 0 : y(t, φ1) = γn+1
(
x(t, φ0)
)}
, (φ0, φ1) ∈ R2+,(10.7)
since the functions x(·, φ0) and y(·, φ1) in (4.8) are continuous. Because every γn+1(·), n ∈ N
is bounded, the function rn(·, ·) is finite. Thus
0 < rn(φ0, φ1) <∞ for every (φ0, φ1) ∈ Cn+1.
Therefore, the (smallest) minimizer rn(φ0, φ1) of the function t 7→ Jvn(t, φ0, φ1), see (5.13),
is an interior point of (0,∞] for every (φ0, φ1) ∈ Cn+1, and the derivative ∂Jvn(t, φ0, φ1)/∂t
vanishes at t = rn(φ0, φ1). Using (5.3) and (10.7) gives
0 =
[
g + µ · vn ◦ S
](
x(t, φ0), y(t, φ1)
))∣∣∣
t=rn(φ0,φ1)
=
[
g + µ · vn ◦ S
](
x(t, φ0), γn+1
(
x(t, φ0)
))∣∣∣
t=rn(φ0,φ1)
, (φ0, φ1) ∈ Cn+1.
(10.8)
Let us denote the boundary of Γn+1 by
∂Γn+1 , {(x, γn+1(x)) : x ∈ [0, ξn+1]},(10.9)
and define the entrance and exit boundaries of Γn+1 by
∂Γen+1 ,
{(
x(rn(φ0, φ1), φ0), γn+1(y(rn(φ0, φ1), φ1)
)
, for some (φ0, φ1) ∈ Cn+1
}
,
∂Γxn+1 ,
{
(φ0, φ1) ∈ Γn+1 : (x(t, φ0), y(t, φ1)) ∈ Cn+1, t ∈ (0, δ] for some δ > 0
}
,
(10.10)
respectively. The paths t 7→ (x(t, φ0), y(t, φ1)) starting at some (φ0, φ1) ∈ Cn+1 enters
the region Γn+1 (for the first time) at the entrance boundary ∂Γ
e
n+1. Similarly, for every
(φ0, φ1) ∈ ∂Γxn+1, the path t 7→
(
x(t, φ0), y(t, φ1))
)
exits Γn+1 immediately.
10.3. Remark. By Lemma 10.5 below, the entrance boundary ∂Γen+1 is a subset of the
boundary ∂An of the region An in (4.13). Clearly, the curve t 7→ (x(t, φ0), (y, φ1)) starting
at any (φ0, φ1) ∈ ∂Γen+1 ⊆ ∂An cannot return immediately into the region An (otherwise
Jvn(t, φ0, φ1) < 0 for some t > 0 and (φ0, φ1) ∈ Cn+1). In the theory of Markov processes,
every element of ∂Γen+1 (resp., ∂Γ
x
n+1) is a regular boundary point of the domain An (resp.,
the interior of Γn+1) with respect to the process Φ˜.
10.4. Remark. Observe that for every (φ0, φ1) ∈ ∂Γxn+1, the quantity rn(φ0, φ1) in (5.14) is
the return time of the curve t 7→ (x(t, φ0), y(t, φ1)) to the stopping region Γn+1 and is also
ADAPTIVE POISSON DISORDER PROBLEM 47
strictly positive. Therefore, the first order necessary optimality condition in (10.11) must
also hold on the exit boundary ∂Γxn+1. Thus,
0 =
[
g + µ · vn ◦ S
](
x(t, φ0), γn+1
(
x(t, φ0)
))∣∣∣
t=rn(φ0,φ1)
, (φ0, φ1) ∈ Cn+1 ∪ ∂Γxn+1.
(10.11)
10.1. The entrance boundary ∂Γen+1. Since all of the functions in (10.11) are continuous,
(10.11) and the definition of the entrance boundary ∂Γen+1 in (10.10) imply
[g + µ · vn ◦ S] (x, y) = 0, (x, y) ∈ ∂Γen+1.(10.12)
The next lemma immediately follows from (10.12), Lemma 10.2 and the continuity of the
function [g + µ · vn ◦ S](·, ·).
10.5. Lemma. For every n ∈ N, let αn ∈ R+ and an : R+ 7→ R+ be the same as in
Lemma 10.2. Then cl(∂Γen+1) ⊆ {(x, an(x)) : x ∈ [0, αn]}, n ∈ N.
10.6. Corollary. For any n ∈ N, if the equality ∂Γn+1 = cl(∂Γen+1) holds, then
∂Γn+1 = {(x, an(x)) : x ∈ [0, αn]}.(10.13)
In other words, ξn+1 = αn, and γn+1(x) = an(x) for every x ∈ [0, ξn+1] ≡ [0, αn], and
Cn+1 = {(x, y) : [g + µ · vn ◦ S](x, y) < 0}.(10.14)
Proof. By Lemma 10.5, {(x, γn+1(x)) : x ∈ [0, ξn+1]} = ∂Γn+1 ⊆ {(x, an(x)) : x ∈ [0, αn]}.
Since both γn+1(·) and an(·) are strictly decreasing, continuous functions which equal zero
at the righthand point of their domains, they must be identical. Finally,
Cn+1 = R2+\Γn+1 = {(x, y) ∈ [0, ξn+1)× R+ : y < γn+1(x)}
= {(x, y) ∈ [0, αn)× R+ : y < an(x)} = {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) < 0},
where the last equality follows from (10.6). 
If the disorder arrival rate λ is large, then every point on the stopping boundary ∂Γn+1 of
the stopping region Γn+1 belongs to the entrance boundary ∂Γ
e
n+1, see Section 11. There-
fore, the stopping boundary ∂Γn+1 for the value function vn+1(·, ·) is determined as in Corol-
lary 10.6, as soon as the value function vn(·, ·) is calculated. Using this observation, the main
solution method described at the beginning of Section 8 can be tailored into a more efficient
algorithm, see Section 11 and Figure 6.
48 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
The exit boundary ∂Γxn+1 may not always be nonempty. If it is nonempty, it is also deter-
mined by the entrance boundary ∂Γen+1, and the general solution method can be similarly
enhanced in this case, see Section 12.
10.2. The exit boundary ∂Γxn+1. Using the semigroup property in (4.9) of the functions
x(·, ·) and y(·, ·), and a change of variable, we obtain
Jvn(t, φ0, φ1) = −e−(λ+µ)tJvn(−t, x(t, φ0), y(t, φ1)), t ∈ R+, (φ0, φ1) ∈ R2+.(10.15)
Substituting in (5.12) w(·, ·) = vn(·, ·) and the identity above give
(10.16) Jtvn(φ0, φ1) = e
−(λ+µ)t [vn+1(x(t, φ0), y(t, φ1))− Jvn(−t, x(t, φ0), y(t, φ1))] ,
t ∈ R+, (φ0, φ1) ∈ R2+.
Since Jrn(φ0,φ1)vn(φ0, φ1) = vn+1(φ0, φ1), and vn+1(x(rn(φ0, φ1), φ0), y(rn(φ0, φ1), φ1)) = 0,
evaluating the equality in (10.16) at t = rn(φ0, φ1) gives
vn+1(φ0, φ1) =
[−e−(λ+µ)tJvn(−t, x(t, φ0), y(t, φ1))] ∣∣∣
t=rn(φ0,φ1)
, (φ0, φ1) ∈ R2+.(10.17)
Recall from Section 10.1 that (x(rn(φ0, φ1), φ0), y(rn(φ0, φ1), φ1)) ∈ ∂Γen+1 for every (φ0, φ1) ∈
Cn+1 ∪ ∂Γxn+1. Therefore, (10.17) implies that we can both calculate the value function
vn+1(·, ·) and find the continuation region Cn+1 by backtracking the curves t 7→ (x(−t, φ0),
y(−t, φ1)) from every point (φ0, φ1) ∈ ∂Γen+1 on the entrance boundary. Let us define
{
r̂(φ0, φ1) , inf{t ≥ 0 : (x(−t, φ0), y(−t, φ1)) 6∈ R2+}
r̂n(φ0, φ1) , inf{t ∈ (0, r̂(φ0, φ1)] : −Jvn(−t, φ0, φ1) ≥ 0}
}
, (φ0, φ1) ∈ R2+, n ∈ N0,
(10.18)
where the infimum of an empty set is infinity. Since the mapping t 7→ Jvn(t, φ0, φ1) is
continuous, we have Jvn(−r̂n(φ0, φ1), φ0, φ1) = 0 if 0 < r̂n(φ0, φ1) <∞.
10.7. Lemma. The entrance boundary ∂Γen+1 determines the exit boundary ∂Γ
x
n+1, the con-
tinuation region Cn+1, and the value function vn+1(·, ·) on Cn+1:
∂Γxn+1 =
{
(x(−t, φ0), y(−t, φ1))
∣∣∣
t=brn(φ0,φ1)
; (φ0, φ1) ∈ ∂Γen+1 and r̂n(φ0, φ1) ≤ r̂(φ0, φ1)
}
,
Cn+1 =
{
(x(−t, φ0), y(−t, φ1)); (φ0, φ1) ∈ ∂Γen+1 and t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)]
} \∂Γxn+1,
and for every (φ0, φ1) ∈ ∂Γen+1
vn+1(x(−t, φ0), y(−t, φ1)) = −e−(λ+µ)tJvn(−t, φ0, φ1), t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)].
ADAPTIVE POISSON DISORDER PROBLEM 49
11. Case I revisited: efficient methods for “large” post-disorder arrival
rates
This is Case I on page 20 where λ ≥ [1− (1 +m)(c/2)]+ is “large”, and the sample-paths
of both components of the process Φ˜ = [Φ˜(0), Φ˜(1)]T increase between the jumps; see also
Figure 2(a). By the relation (4.10), the deterministic functions t 7→ x(t, φ0), t ∈ R+ and
t 7→ y(t, φ1)), t ∈ R+ are strictly increasing for every (φ0, φ1) ∈ R2+.
By Remark 9.6, the identity in (9.11) between the value functions V (·, ·) and Vn(·, ·) on
the set S−n(Γn) ∩ Cn ∩ ([0, xn]× R+) holds for every n ∈ N. Thus, we can find the value
function V (·, ·) by calculating Vn(·, ·), n ∈ N using steps 1-3 on page 43. This method can
be improved further. We shall show that the optimization in each iteration of (8.1) can be
avoided, and the value function V (·, ·) may be calculated in one pass over the continuation
region C; see Figure 6.
Since the boundary ∂Γn+1 of the stopping region Γn+1 is a (strictly) decreasing continuous
curve, every point in the set ∂Γn+1∩int(R2+) is accessible from some point in the continuation
region Cn+1. Therefore, we have ∂Γn+1 = cl(∂Γ
e
n+1) for every n ∈ N0. By Corollary 10.6,
the set An in (10.1) and the continuation region Cn+1 (and their boundaries) coincide for
every n ∈ N0.
If the value function vn(·, ·) ≡ Vn(·, ·) for some n ∈ N0 is already calculated, then the
boundary of the continuation region Cn+1 becomes immediately available as in (10.13). In
fact, (10.5) and (10.14) imply
S(Cn+1) =
{
(x, y) ∈ R2+ : vn(x, y) <
[
− 1
µ
· g ◦ S−1
]
(x, y)
}
,(11.1)
S(∂Γn+1) =
{
(x, y) ∈ R2+ : vn(x, y) =
[
− 1
µ
· g ◦ S−1
]
(x, y)
}
.(11.2)
The set on the righthand side in (11.2) is a strictly decreasing, convex and continuous curve
in R2+, and it is the same as
S(∂Γn+1) = S(the boundary of the set epi(γn+1) ∩ ([0, ξn+1]× R+))
= the boundary of the set epi(S[γn+1]) ∩
([
0,
µ− 1
µ
ξn+1
]
× R+
)
=
{
(x, S[γn+1](x));x ∈
[
0,
µ− 1
µ
ξn+1
]}
.
(11.3)
50 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
If we know vn(·, ·), then we can determine the set in (11.2) of all points (x, y) ∈ R2+ satisfying
vn(x, y) =
[
− 1
µ
· g ◦ S−1
]
(x, y) ≡ − x
µ− 1 −
y
µ+ 1
+
λ
cµ
√
2,(11.4)
and obtain the boundary function γn+1(·) after the transformation of this set by S−1. Then
we can calculate the (smallest) minimizer rn(·, ·) of (8.1) by the relation (10.7), and the
value function vn+1(·, ·) by (10.2). We can continue in this manner to find the value func-
tions vn+2(·, ·), vn+3(·, ·), · · · . This method saves us from an explicit search for the solution
rn(φ0, φ1) of the minimization problem in (8.1) for every (φ0, φ1) ∈ Cn+1:
Step B.0: Initialize n = 0, v0(·, ·) ≡ 0.
Step B.1: Find the region
Bn ,
{
(x, y) ∈ R2+ : vn(x, y) < −
x
µ− 1 −
y
µ+ 1
+
λ
cµ
√
2
}
, n ∈ N.(11.5)
Step B.2: Determine the continuation region Cn+1 = S
−1(Bn) by the transformation
of Bn under S
−1.
Step B.3: Calculate the value function vn+1(·, ·) on Cn+1 by using (10.2) and (10.7).
Step B.4: Set n to n+ 1 and go to Step B.1.
In fact, we can do much better than this. After n ∈ N iterations, we find both vn(·, ·) and
V (·, ·), vn+1(·, ·), vn+2(·, ·), · · · on the subset
Qn , S−n(Γn) ∩Cn ∩ ([0, xn]× R+)(11.6)
=
{
(x, y) ∈ [0, xn]× R+ : S−n[γn](x) ≤ y < γn(x)
}
, n ∈ N(11.7)
of Cn+1 by Corollary 9.4; see also Figure 5. Therefore, we need to determine only the set
Rn+1 , Qn+1\Qn, n ∈ N (R1 ≡ Q1)(11.8)
in Step B.2, and calculate the value function vn+1(·, ·) only on this set in Step B.3. By
Lemma 9.5, this modified method calculates V (·, ·) (and all Vn(·, ·), n ∈ N simultaneously)
on any given set R+ × (0, B), B > 0 in finite number of iterations. We shall describe this
modified method on page 52 after establishing a few facts below. Several steps of the method
is also illustrated in Figure 6.
Since v0 ≡ 0, setting n = 0 in (11.4) gives a straight line; substituting (x, y) = (x, S[γ1](x))
and comparing this with S(∂Γ1) in (11.3) give
S[γ1](x) = −µ+ 1
µ− 1 x+
µ+ 1
µ
· λ
c
√
2, x ∈ supp(S[γ1]) =
[
0,
µ− 1
µ
· λ
c
√
2
]
,(11.9)
ADAPTIVE POISSON DISORDER PROBLEM 51
(x1, γ1(x1))
S−2[γ2](·)C1
Γ1 = R2+\C1
[
− 1µg ◦ S−1
]
(·, ·) = v1(·, ·)
S−1
0 x1 ξ1 ( µµ−1 )ξ1
ξ2 ( µµ−1 )
2ξ2
γ2(·)γ1(·)
S−1[γ1](·)
( µµ+1 )γ1(0)
γ1(0)
( µµ+1 )
2γ2(0)
(i) S[γ2](·) is obtained by solving
R2
(ii)
(iii)
γ1(·) = γ(·) on [0, x1]
(x1, γ1(x1))
C1
x10
S−1[γ1](·)
ξ1 ( µµ−1 )ξ1
γ1(·)
( µµ+1 )γ1(0)
γ1(0)
Γ1 = R2+\C1
v1(·, ·) = V (·, ·) on R1
C.1C.2
C2
[
− 1µg ◦ S−1
]
(·, ·) = v2(·, ·)
0 ξ2
γ2(·) γ3(·) S−3[γ3](·)
( µµ−1 )
3ξ3ξ3 ( µµ−1 )
2ξ2
(x2, γ2(x2))
x2
S−2[γ2](·)
S−1
(x1, γ1(x1))
( µµ+1 )γ1(0)
γ1(0)
( µµ+1 )
2γ2(0)
( µµ+1 )
3γ3(0)
Γ2 = R2+\C2
S[γ2](·)
R1
R3
(ii)
(iii)
(i) S[γ3](·) is obtained by solving
γ1(0)
(x1, γ1(x1))
C2
x10
( µµ+1 )γ1(0)
S−2[γ2](·)
(x2, γ2(x2))
ξ2x2
γ2(·) = γ(·) on [0, x2]
γ2(·)
( µµ−1 )
2ξ2
( µµ+1 )
2γ2(0)
Γ2 = R2+\C2
v2(·, ·) = V (·, ·) on R1 ∪R2
C.1C.2
(b) n = 1 in Step C.1.
(a) n = 0 in Step C.1
Figure 6. Case I: λ is “large.” The illustration of Method C, see page 52: Steps C.1 and
C.2 when (a) n = 0, and for (b) n = 1 in Step C.1.
52 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
and ξ1 = (λ/c)
√
2. Using (9.7), we find
γ1(x) = S
−1[S[γ1]](x) = −x+ λ
c
√
2, x ∈ [0, ξ1] =
[
0,
λ
c
√
2
]
.(11.10)
The function S−1[γ1](·) is affine, and intersects with γ1(·) at x1 ≡ x1(γ1) = (λ/c)(
√
2/2), see
(9.9). By Corollary 9.4 and Remark 9.6, the boundary of the stopping region on [0, x1] is
γ(x) = γ1(x) = −x+ λ
c
√
2, x ∈ [0, x1] ≡
[
0,
λ
√
2
c
· 1
2
]
;(11.11)
see the inset in Figure 6(a). Hence, the boundaries of the optimal stopping region Γ and
the stopping regions Γn, n ∈ N stick on the upper half of the hypotenuse of the rectangular
triangle {(x, y) ∈ R2+ : g(x, y) ≤ 0} ≡ cl(C0).
11.1. Proposition. Fix any n ∈ N. The functions in Sn ,
(
S−k[γk]
)n
k=1
do not intersect
inside the continuation region Cn = {(x, y) ∈ R2+ : y < γn(x)}. The function S[γn+1](·)
intersects with each function in Sn ∪ {γn} pairwise exactly once.
The same conclusions hold when every γk, k = 1, . . . , n in the proposition is replaced with
γ; this can be verified using the elementary properties of convex functions and the affine
structure of the boundary function γ(·) in (11.11); see Bayraktar, Dayanik and Karatzas
(2004a) for the details. Then the proof of Proposition 11.1 follows easily from Corollary 9.4.
We are now ready to give a better version of method B on page 50 to calculate each v(·, ·)
and the boundary function γ(·). Recall that S−n[γn](·) and xn(γn) are defined by (9.7) and
(9.9). The steps C.1 and C.2 below are illustrated in Figure 6 for n = 0 and n = 1.
Step C.0: Initialize n = 0, x0 = 0, v0(·, ·) ≡ 0 and the region R1 as in (11.8).
Step C.1: Calculate the value function V (φ0, φ1) = vn+1(φ0, φ1) for every (φ0, φ1) ∈
Rn+1 using (10.2) and (10.7).
Step C.2: Set n to n+ 1.
(i) Determine the set{
(x, y) ∈ Rn : vn(x, y) = − x
µ− 1 −
y
µ+ 1
+
λ
cµ
√
2
}
(11.12)
of points in Rn which satisfy (11.4). This is the intersection of the set in (11.3)
and Rn. Namely, it is the section of the strictly decreasing, convex and contin-
uous curve x 7→ S[γn+1](x) contained in Rn.
(ii) Find the subset of Rn enclosed between the vertical y-axis and the curve in
(11.12). This is the intersection Rn ∩Bn of the sets Rn and Bn in (11.5).
ADAPTIVE POISSON DISORDER PROBLEM 53
(iii) Find the set Rn+1 = S
−1(Rn ∩ Bn) in (11.8) by applying the transformation
S−1(·, ·) to the set found in (ii).
The region Rn+1 is enclosed between the y-axis from left, the S
−1-transformation
of the curve in (11.12) from right. This right boundary of Rn+1 extends the
boundary γ(·) ≡ γn+1(·) from the previous iteration into the region S−(n+1)(Γ) ≡
S−(n+1)(Γn+1).
(iv) Go to Step C.1.
12. The smoothness of the value functions and the stopping boundaries
The general method described at the beginning of this section evaluates the integrals
Jvn(·, φ0, φ1) in (8.2) of the function [g + µ · vn ◦ S](·, ·) along the curves (x(·, φ0), y(·, φ1))
in R2+ in order to calculate the value function vn+1(φ0, φ1) as in (8.1). For an accurate
implementation of this method, it may be useful to know how smooth the integrand, or
essentially the value function vn(·, ·) is.
The smoothness of the value function V (·, ·) may also allow us to formulate the original
optimal stopping problem in (4.12) as a free-boundary problem. Then, in principle, we can
calculate the value function V (·, ·) directly, by solving a partial differential equation, as the
next proposition suggests.
12.1. Proposition. Suppose that there is a bounded and continuous function w : R2+ 7→
(−∞, 0] which is continuously differentiable on R2+\∂Γ, and whose first-order derivatives are
locally bounded near the boundary ∂Γ = {(x, γ(x)) : x ∈ [0, ξ]}. Moreover,
(A˜ − λ)w(x, y) + g(x, y) = 0, (x, y) ∈ C,(12.1)
w(x, y) = 0, (x, y) ∈ Γ,(12.2)
(A˜ − λ)w(x, y) + g(x, y) > 0, (x, y) ∈ Γ\∂Γ,(12.3)
w(x, y) < 0, (x, y) ∈ C,(12.4)
where A˜ is the infinitesimal generator in (7.4) of the process Φ˜ acting on the continuously
differentiable functions.
Suppose also that the sample-paths of the process Φ˜ = (Φ˜(0), Φ˜(1)) spend zero time on the
boundary ∂Γ almost surely, i.e.,
Eφ0,φ10
[∫ ∞
0
1∂Γ(Φ˜t)dt
]
= 0, (φ0, φ1) ∈ R2+.(12.5)
If the convex function γ(·) is also Lipschitz continuous on [0, ξ], then w(·, ·) = V (·, ·) on R2+.
54 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Proof. Similar to the proof of Theorem 10.4.1 in Øksendal(1998, p. 215). 
Under certain conditions, we are able to show that the bounded, concave and continuous
value functions vn(·, ·), n ∈ N and V (·, ·) are continuously differentiable on R2+\∂Γxn+1 and
R2+\∂Γx, respectively, and are not differentiable on the exit boundaries ∂Γxn and ∂Γx in
(10.10), respectively. The exit boundaries ∂Γxn, n ∈ N and ∂Γx, and the entrance boundaries
∂Γen and ∂Γ
e are connected subsets of R2+, and
∂Γn = ∂Γ
x
n ∪ cl(∂Γen), n ∈ N and ∂Γ = ∂Γx ∪ cl(∂Γe).(12.6)
Moreover, the boundary functions γn(·) and γ(·) are continuously differentiable on their
support.
The hypotheses of Proposition 12.1 are satisfied with w(·, ·) , v(·, ·) in (5.10). Thus,
the function v(·, ·) ≡ V (·, ·) may be obtained by solving the variational inequalities (12.1)-
(12.4). This may be a challenging problem since, as we already pointed out above, the
smooth-fit principle is guaranteed not to hold on some part of the free-boundary. We shall
not investigate the variational problem here, but give a concrete example with this interesting
boundary behavior, and describe our solution method for it.
The main result is Proposition 12.17 below, and it is proven by induction. Here, we shall
study the basis of the induction by breaking it down in several lemmas. The proof of the
induction hypothesis is very similar, and later we will point out only the major differences.
Let us introduce the continuous mapping Gn : R3+ 7→ R defined by
Gn(t, φ0, φ1) ,
[
g + µ · vn ◦ S
]
(x(t, φ0), y(t, φ1)), (t, φ0, φ1) ∈ R3+, n ∈ N0,(12.7)
Note that (10.2) gives
vn+1(φ0, φ1) =
∫ rn(φ0,φ1)
0
e−(λ+µ)t Gn(t, φ0, φ1) dt, (φ0, φ1) ∈ Cn+1, n ∈ N0.(12.8)
Using (10.11), (10.12) and Lemmas 10.2 and 10.5, we obtain
(12.9) (x(t, φ0), y(t, φ1))
∣∣∣
t=rn(φ0,φ1)
∈ ∂Γen+1 ⊆ {(x, an(x)) : x ∈ [0, αn]}
≡ {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) = 0} , (φ0, φ1) ∈ Cn+1 ∪ ∂Γxn+1.
Therefore, for every n ∈ N
0 = Gn(t, φ0, φ1)
∣∣∣
t=rn(φ0,φ1)
, (φ0, φ1) ∈ Cn+1 ∪ ∂Γxn+1.(12.10)
ADAPTIVE POISSON DISORDER PROBLEM 55
Under certain local smoothness and nondegeneracy conditions on the function Gn(·, ·, ·),
the implicit function theorems guarantee that the equation Gn(t, φ0, φ1) = 0 implicitly deter-
mines t = tn(φ0, φ1) in an open neighborhood of every point (rn(φ0, φ1), φ0, φ1) in R+×Cn+1,
as a smooth function of the variables (φ0, φ1). Since the continuation region Cn+1 has com-
pact closure, a finite subcovering of these open neighborhoods exists. Patching the solutions
tn(·, ·) in the finite subcovering gives the global solution, which is smooth and must coincide
with the function rn(·, ·) on Cn+1.
In the remainder, we shall use the following version of the implicit function theorem; see,
e.g., Protter and Murray (1991, Chapter 14), and also Conjecture 12.18 below.
12.2. Theorem (Implicit Function Theorem). Let A ⊆ Rm be an open set, F : A 7→ R be a
continuously differentiable function, and (t, x) ∈ R× Rm−1 be a point in A such that
F (t, x) = 0, and
∂
∂t
F (t, x)
∣∣∣
(t,x)=(t,x)
6= 0.
Then there exist an open set B ⊆ Rm−1 containing the point x, and a unique continuously
differentiable function f : B 7→ R such that t = f(x) and F (f(x), x) = 0 for all x ∈ B.
Since v0(·, ·) ≡ 0, we have
G0(t, φ0, φ1) = x(t, φ0) + y(t, φ1)− λ
c
√
2, (t, φ0, φ1) ∈ R+ ×C1.(12.11)
The function G0(·, ·, ·) is continuously differentiable on R+ ×C1. By (6.8) in Section 6, its
partial derivative
DtG0(t, φ0, φ1) =
d
dt
[x(t, φ0) + y(t, φ1)](12.12)
with respect to t-variable may vanish at most once; if this happens, this derivative is strictly
negative before and strictly positive after the derivative vanishes; otherwise, it is strictly
positive everywhere (see, also, Figure 3). Namely, the function t 7→ G0(t, φ0, φ1) has at most
one local minimum for every (φ0, φ1) ∈ R2+.
12.3. Lemma. Fix any (φ0, φ1) ∈ R2+. The function t 7→ G0(t, φ0, φ1) from R+ into R has
at most one local minimum. It is strictly increasing if there is no local minimum. If there
is a local minimum, then the function G0(·, φ0, φ1) is strictly decreasing before the minimum
and strictly increasing after the minimum.
12.4. Lemma. The smallest minimizer r0(φ0, φ1) in (5.13) is continuously differentiable at
every (φ0, φ1) ∈ C1.
56 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Proof. The result will follow from Theorem 12.2 applied to the function G0(·, ·, ·) on R×C1
at the point (t, x) = (r0(φ0, φ1), φ0, φ1) ∈ R×C1. We only need to establish that
DtG0(t, φ0, φ1)
∣∣∣
t=r0(φ0,φ1)
6= 0, (φ0, φ1) ∈ C1.
Let us fix (φ0, φ1) ∈ C1 and assume that DtG0(r0(φ0, φ1), φ0, φ1) = 0. Then the function
t 7→ G0(t, φ0, φ1) is strictly decreasing on t ∈ [0, r0(φ0, φ1)] by Lemma 12.3, and
G0(t, φ0, φ1) > G0(r0(φ0, φ1), φ0, φ1) = 0, t ∈ [0, r0(φ0, φ1)).
Therefore, (12.8) implies that v1(φ0, φ1) > 0. This contradicts with our choice of (φ0, φ1) in
the continuation region C1, as well as, the bound v1(·, ·) ≤ 0. 
12.5. Corollary. The the value function v1(φ0, φ1) is continuously differentiable at every
(φ0, φ1) ∈ C1. For every (φ0, φ1) ∈ C1,
Dφ0v1(φ0, φ1) =
∫ r0(φ0,φ1)
0
e−(λ+µ)tDφ0G0(t, φ0, φ1) dt =
1− e−(µ−1)r0(φ0,φ1)
µ− 1 ,
Dφ1v1(φ0, φ1) =
∫ r0(φ0,φ1)
0
e−(λ+µ)tDφ1G0(t, φ0, φ1) dt =
1− e−(µ+1)r0(φ0,φ1)
µ+ 1
.
(12.13)
Proof. By (12.8) and Lemma 12.4, the value function v1(·, ·) is continuously differentiable.
Using (12.9) after applying the chain-rule to (12.8) with n = 0 gives the integrals in (12.13).
These integrals can be calculated explicitly by using (4.7) or (4.8). 
12.6. Corollary. The the entrance boundary ∂Γe1 in (10.10) is connected. More precisely,
∂Γe1 = {(x, γ1(x)) : x ∈ (ξe1, ξ1)} for some 0 ≤ ξe1 < ξ1,(12.14)
where ξ1 is the same as in [0, ξ1] = supp(γ1), the support of the boundary function γ1(·), see
Proposition 9.1.
12.7. Corollary. The restriction of the boundary function γ1(·) to the interval (ξe1, ξ1) is
continuously differentiable. In fact,
γ1(x) = a0(x), x ∈ [ξe1, ξ1], and [0, ξ1] ≡ supp(γ1) = supp(a0) ≡ [0, α0],
where
a0(x) =

− x+ λ
c
√
2, x ∈
[
0,
λ
c
√
2
)
0, elsewhere
(12.15)
is the continuously differentiable boundary function of the region A0 = {(x, y) ∈ R2+ : [g+µ ·
v0 ◦ S](x, y) < 0} in (10.6).
ADAPTIVE POISSON DISORDER PROBLEM 57
Proof. The function a0(·) in (12.15) is continuously differentiable on its support supp(a0) =
[0, α0], and the result follows from Lemma 10.5 and Corollary 12.6. 
The entrance boundary ∂Γe1 always exists. However, the exit boundary ∂Γ
x
1 may not exist
all the time. Next we shall identify the geometry of the exit boundary ∂Γx1 when it exists.
12.8. Lemma. For every (φ0, φ1) ∈ ∂Γx1, we have [g + µ · v0 ◦ S](φ0, φ1) > 0. Therefore,
cl(∂Γe1) ∩ ∂Γx1 = ∅.
Proof. Suppose that (φ0, φ1) ∈ ∂Γx1 . Let us assume that [g + µ · v0 ◦ S](φ0, φ1) ≤ 0. Then
G0(0, φ0, φ1) = [g + µ · v0 ◦ S](φ0, φ1) ≤ 0 = G0(r0(φ0, φ1), φ0, φ1), and Lemma 12.3 implies
G0(t, φ0, φ1) = [g + µ · v0 ◦ S](x(t, φ0), y(t, φ1)) < 0, t ∈ (0, r0(φ0, φ1)).
This inequality and (12.8) for n = 0 give
v1(φ0, φ1) =
∫ r0(φ0,φ1)
0
e−(λ+µ)tG0(t, φ0, φ1)dt < 0,
which contradicts with v1(φ0, φ1) = 0. This proves that [g + µ · v0 ◦ S](φ0, φ1) > 0 for every
(φ0, φ1) ∈ ∂Γx1 . Since the mapping (x, y) 7→ [g + µ · v0 ◦ S](x, y) is continuous, we have
[g + µ · v0 ◦ S](φ0, φ1) = 0 for every (φ0, φ1) ∈ cl(∂Γe1) by Lemmas 10.2 and 10.5. Therefore,
cl(∂Γe1) ∩ ∂Γx1 = ∅. 
The next corollary is helpful in determining the point (ξe1, γ1(ξ
e
1)) ≡ (ξe1, a0(ξe1)). The
region An was introduced in Section 10.
12.9. Corollary. The parametric curve
C1 , R2+ ∩ {(x(t, ξe1), y(t, a0(ξe1))) : t ∈ R}(12.16)
is the smallest among all the parametric curves R2+ ∩ {(x(t, φ0), y(t, φ1)) : t ∈ R}, (φ0, φ1) ∈
R2+ that majorize the boundary function a0(·) of the region A0 = {(x, y) : [g+µ·v0◦S](x, y) <
0} = {(x, y) ∈ R2+ : y < a0(x)}.
The curve C1 and the boundary ∂A0 = {(x, a0(x)) : x ∈ [0, α0]} touch exactly at (ξe1, a0(ξe1)) ≡
(ξe1, a0(ξ
e
1)) and nowhere else.
Proof. By Corollary 12.7 and Lemma 12.8, we have (ξe1, a0(ξ
e
1)) = (ξ
e
1, γ1(ξ
e
1)) ∈ cl(∂Γ1)\∂Γe1.
Therefore (ξe1, a0(ξ
e
1)) 6∈ ∂Γe1 ∪ ∂Γx1 . Hence there exists some δ > 0 such that
(x(t, ξe1), y(t, a0(ξ
e
1))) ∈ Γ1 ⊆ R2+\A0, t ∈ (−δ,+δ).(12.17)
58 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Recall from (10.18) that r̂(ξe1, a0(ξ
e
1)) is the exit time of the curve (x(−t, ξe), y(−t, a0(ξe1))),
t ∈ R+ from R2+. Then the function
t 7→ G0(t, ξe1, a0(ξe1)) ≡ [g + µ · v0 ◦ S](x(t, ξe1), y(t, a0(ξe1))), t ∈ [−r̂(ξe1, a0(ξe1)),∞)
has a zero at t = 0, and is nonnegative for every t ∈ (−δ, δ) by (12.17). Hence it has a local
minimum at (ξe1, a0(ξ
e
1)). By Lemma 12.3, the function G0(t, ξ
e
1, a0(ξ
e
1)) is strictly positive
for every t 6= 0. Therefore, y(t, a0(ξe1)) > a0(x(t, ξe1)) for every t 6= 0. 
12.10. Remark. Since C1 ⊂ R2+\A0, we have Jv0(t, φ0, φ1) > 0 for every t > 0 and (φ0, φ1) ∈
C1. This implies v1(φ0, φ1) = 0 for every (φ0, φ1) ∈ C1. Therefore, C1 ⊂ Γ1.
The curve C1 divides R2+ into two components. Since the continuation region C1 is con-
nected and contains A0, the region C1 is contained in the (lower) component which lays
between the curve C1 and x-axis. Thus the boundary ∂Γ1 is completely below the curve C1,
and they touch at the point (ξe1, a0(ξ
e
1)) ≡ (ξe1, γ1(ξe1)). See Figure 8(a).
Next corollary shows that no points on the boundary {(x, a0(x)) : x ∈ [0, ξe1)} over the
interval [0, ξe0) of the region A0 = {(x, y) ∈ R2+ : [g+µ ·v0 ◦S](x, y) < 0} is a boundary point
for the stopping region Γ1.
12.11. Corollary. For every x ∈ [0, ξe1), we have γ1(x) > a0(x) and [g+µ·v0◦S](x, γ1(x)) > 0.
Proof. If [0, ξe1) = ∅, then there is nothing to prove. Otherwise, fix any φ0 ∈ [0, ξe1). As-
sume that (φ0, a0(φ0)) ∈ ∂Γ1. By Corollary 12.6 and Lemma 12.8, we have (φ0, a0(φ0)) /∈
∂Γe1∪∂Γx1 . The same arguments as in the proof of Corollary 12.9 with (φ0, a0(φ0)) instead of
(ξe1, a0(ξ
e
1)) gives that the parametric curve {(x(−t, φ0), y(−t, a0(φ0)) : t ∈ [−r̂(φ0, a0(φ0)),∞)}
is the smallest majorant of the boundary function a0(·), and both curves touch at the point
(φ0, a0(φ0)). But this implies φ0 = ξ
e
1, a contradiction with our choice of φ0. 
12.12. Corollary. If φ0 ∈ [0, ξe1), then (φ0, γ1(φ0)) ∈ ∂Γx1 has an open neighborhood, on the
intersection with the continuation region C1 of which the function r0(·, ·) is bounded and
bounded away from zero.
On the other hand, the function r0(·, ·) is continuous on the entrance boundary ∂Γe1: for
every (φ0, φ1) ∈ ∂Γe1 and every sequence {(φ(n)0 , φ(n)1 )}n∈N ⊆ C1 converging to the boundary
point (φ0, φ1), we have limn→∞ r0(φ
(n)
0 , φ
(n)
1 ) = 0.
12.13. Lemma. If ξe1 = 0, then ∂Γ1 = cl(∂Γ
e
1). If ξ
e
1 > 0, then the exit boundary ∂Γ
x
1 is not
empty, and ∂Γ1 = ∂Γ
x
1 ∪ cl(∂Γe1).
ADAPTIVE POISSON DISORDER PROBLEM 59
φd
γ1(·)a0(·)
ξx1ξ
e
1
C1
λ
c
√
2
γ1(0)
Γ1
0
B1
ξ1 = λc
√
2
The exit boundary ∂Γx1
of the stopping region Γ1 :
r̂0(x, a0(x)) < r̂(x, a0(x))
for every x ∈ [ξe1, ξx1 )
φd
γ1(·)a0(·)
ξx1ξ
e
1
C1
ξ1 = λc
√
2
λ
c
√
2
γ1(0)
A0
Γ1
0
(a) (b)
Figure 7. (a) The exit boundary ∂Γx1 of the stopping region Γ1 is found by backtracing the
parametric curves t 7→ (x(t, φ0), y(t, φ1)) from every point (φ0, φ1) on the entrance boundary
∂Γe1 = {(x, a0(x)) : x ∈ (ξe1, ξ1)}. In (b), the region B1 defined in the proof of Lemma 12.14
is sketched.
If ∂Γ1 6= cl(∂Γe1), then ξe1 > 0, and the exit boundary ∂Γx1 is not empty by Lemma 12.13.
The characterization of the exit boundary in Lemma 10.7 can be expressed better. It can
be easily shown that there exists some ξx1 ∈ (ξe1, ξ1) such that
Γx1 =
{(
x(−t, φ0), y(−t, a0(φ0))
)∣∣∣
t=br0(φ0,a0(φ0))
: φ0 ∈ (ξe1, ξx1 ]
}
.
More precisely,
ξx1 = inf{φ0 ∈ [ξe1, ξ1] : r̂0(φ0, γ1(x)) ≤ r̂(φ0, γ1(x))}.(12.18)
For every φ0 ∈ (ξe1, ξx1 ], we have r̂0(φ0, γ1(φ0)) ≤ r̂(φ0, γ1(φ0). See (10.18) and Figure 7(a).
Our next result shows that, the exit boundary ∂Γx1 = {(φ0, γ1(φ0)) : φ0 ∈ [0, ξe1)} is on a
continuously differentiable curve, if it is not empty.
12.14. Lemma. The restriction of the boundary function γ1(·) to the interval [0, ξe1) is con-
tinuously differentiable.
If the value function v1(·, ·) were continuously differentiable on the exit boundary ∂Γx1 ,
then the result would follow from an application of the implicit function theorem to the
identity v1(φ0, φ1) = 0 near the point (φ0, φ1) = (φ0, γ1(φ0)).
60 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Unfortunately, v1(·, ·) is not differentiable on ∂Γx1 ; see Lemma 12.16. Therefore, we shall
first extend the restriction to the set C1∪∂Γx1 of the value function v1(·, ·) to a new function
v˜1(·, ·) on an open set B1 ⊃ C1 ∪ Γx1 such that v˜1(·, ·) is continuously differentiable on B1.
We shall then use the identity v˜1(φ0, γ1(φ1)) = 0 as described above.
12.15. Lemma. The boundary function γ1(·) is continuously differentiable on the interior of
its support [0, ξ1].
The next result shows that the value function is not differentiable on the exit boundary
∂Γx1 . In fact, as the proof reveals, the left and right partial derivatives are different along
the exit boundary. Therefore, the smooth-fit principle does not apply to the value function
v1(·, ·) along (some part of) the boundary if the exit boundary ∂Γx1 is not empty.
12.16. Lemma. The value function v1(·, ·) is continuously differentiable on the entrance
boundary ∂Γe1, but is not differentiable on the exit boundary ∂Γ
x
1.
The techniques used above in the analysis of the value function v1(·, ·) and the bound-
ary function γ1(·) can be extended inductively to every function vn(·, ·) and the boundary
function γn(·) if the followings are true for every n ∈ N:
A1(n): For every (φ0, φ1) ∈ R2+, the function t 7→ Gn(t, φ0, φ1) in (12.7) from R+ into R
has at most one local minimum. It is strictly increasing if there is no local minimum.
If there is a local minimum, then the function Gn(·, φ0, φ1) is strictly decreasing before
the minimum and strictly increasing after the minimum.
A2(n): The function (x, y) 7→ [g+µ · vn ◦S](x, y) is (continuously) differentiable on the
entrance boundary ∂Γen+1 of the stopping region Γn+1 = {(x, y) : vn+1(x, y) = 0}.
12.17. Proposition. If A1(k) and A2(k) above are true for every 0 ≤ k ≤ n, then the
followings hold.
ADAPTIVE POISSON DISORDER PROBLEM 61
(1) The value function vn+1(·, ·) is continuously differentiable everywhere on R2+\Γxn+1,
everywhere except the exit boundary ∂Γxn+1. For every (φ0, φ1) ∈ Cn+1
Dφ0vn+1(φ0, φ1) =
∫ rn(φ0,φ1)
0
e−(λ+µ)tDφ0Gn(t, φ0, φ1)du
=
∫ rn(φ0,φ1)
0
e−(λ+µ)t [1 + (µ− 1)Dφ0vn ◦ S] (x(u, φ0), y(u, φ1))du,
Dφ1vn+1(φ0, φ1) =
∫ rn(φ0,φ1)
0
e−(λ+µ)tDφ1Gn(t, φ0, φ1)du
=
∫ rn(φ0,φ1)
0
e−(λ+µ)t [1 + (µ+ 1)Dφ1vn ◦ S] (x(u, φ0), y(u, φ1))du.
(2) The entrance boundary ∂Γen+1 is connected. More precisely,
∂Γen+1 = {(x, an(x)) : x ∈ (ξen+1, ξn+1)} for some ξen+1 ∈ [0, ξn+1).
The boundary function an(·) of the region An = {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) <
0} is continuously differentiable on (ξen+1, ξn+1). Therefore, the boundary function
γn+1(·) ≡ an(·) on (ξen+1, ξn+1) is continuously differentiable.
(3) The parametric curve
Cn+1 , R2+ ∩ {(x(t, ξen+1), y(t, an(ξen+1))) : t ∈ R}
is the smallest among all the parametric curves R2+ ∩ {(x(t, φ0), y(t, φ1)) : t ∈ R},
(φ0, φ1) ∈ R2+ that majorize the boundary function an(·) of the region An = {(x, y) :
[g + µ · vn ◦ S](x, y) < 0} = {(x, y) ∈ R2+ : y < an(x)}.
The curve Cn+1 and the boundary ∂An = {(x, an(x)) : x ∈ [0, αn]} touch exactly at
(ξen+1, an(ξ
e
n+1)) ≡ (ξen+1, an(ξen+1)) and nowhere else.
(4) If ξen+1 = 0, then ∂Γn+1 = cl(∂Γ
e
n+1). If ξ
e
n+1 > 0, then the exit boundary ∂Γ
x
n+1 is
not empty, and ∂Γn+1 = ∂Γ
x
n+1 ∪ cl(∂Γen+1).
(5) The boundary function γn+1(·) is continuously differentiable on the interior of its
support [0, ξn+1].
The proof of the proposition is by induction on n ∈ N0. The suppositions A1(0) and A2(0)
are always correct; see Lemma 12.3, and note that [g+µ · v0 ◦S](·, ·) ≡ g(·, ·) is continuously
differentiable everywhere. All of the claims above are proved for the basis of the induction
n = 0 before the statement of the proposition. For n ≥ 1, the proofs are the same with
obvious changes, with the exception of the differentiability of an(·) in (2).
62 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
For n = 0, the differentiability of a0(x) = −x + (λ/c)
√
2, x ∈ (0, ξ1) was obvious. For
n ≥ 1, the function an(·) is not available explicitly. But
[g + µ · vn ◦ S](x, an(x)) = 0, x ∈ [0, ξn+1].
ByA2(n), the function [g+µ·vn◦S](·, ·) is continuously differentiable on ∂Γen+1 = {(x, an(x)) :
x ∈ (ξen+1, ξn+1)}. Since y 7→ [g + µ · vn ◦ S](x, y) is strictly increasing for every x ∈ R+, we
have
∂
∂y
[g + µ · vn ◦ S](x, y)
∣∣∣
(x,y)=(x,an(x))
> 0, x ∈ (ξen+1, ξn+1).
Thus, the function an(·) is continuously differentiable on (ξen+1, ξn+1) by the implicit function
theorem.
12.1. The interplay between the exit and entrance boundaries. Unfortunately, we
were unable to identify fully all the cases where the hypotheses A1(n) and A2(n) on page
60 are satisfied for every n ∈ N (see, though, Section 12.2 for the important case of “large”
disorder arrival rate λ and Section 12.3 for another interesting example, where they are
satisfied). However, they are the sufficient conditions for Proposition 12.17 to hold, and
Proposition 12.17 shows the crucial interplay between the exit and entrance boundaries. We
would like to illustrate this interplay briefly; it may be very useful in designing efficient
detection algorithms for general Poisson disorder problems. Later, we shall point out how
the gap may be closed as an interesting research problem.
In Section 10.2, we showed that both the value functions and the exit boundaries are deter-
mined by the entrance boundaries, see Lemma 10.7. More explicitly, if the entrance boundary
∂Γen+1 is obtained somehow, then one can calculate the value function vn+1(·, ·) and the exit
boundary ∂Γxn+1 by running backwards in time the parametric curves t 7→ (x(t, φ0), y(t, φ1))
from every point (φ0, φ1) on the entrance boundary ∂Γ
e
n+1 and by evaluating the explicit
expressions of Lemma 10.7 along the way. On the other hand, the entrance boundary ∂Γen+1
can be found when the value function vn(·, ·) is already calculated. Since v0 ≡ 0 is read-
ily available, the following iterative algorithm will give us every vn(·, ·), n ∈ N0 and the
boundary functions γn(·), see also Figure 8:
Step D.0: Initialize n = 0, v0(·, ·) ≡ 0 on R2+. Let a0(·) be the boundary function of
the region A0 = {(φ0, φ1) ∈ R2+ : [g + µ · v0 ◦ S](φ0, φ1) < 0}; see (12.15).
ADAPTIVE POISSON DISORDER PROBLEM 63
ξe1 ξ1
Exit boundary ∂Γx1
Entrance
boundary ∂Γe1:
γ1(·) = a0(·)
on [ξe1, ξ1]
of the stopping region Γ1:
Γ1
γ1(·) > a0(·) on [0, ξe1)
satisfying the equation[
1
µ · g ◦ S−1
]
(·, ·) = v1(·, ·)
a0(·)
γ1(·)
S[a1]: The locus of points
Γ1
A0A0
C1: tangent to a0(·) at ξe1
Exit boundary ∂Γxn+1
of the stopping region Γn+1:
Γn+1
γn+1(·) > an(·) on [0, ξen+1)
Γn+1
An
ξn+1ξ
e
n+1
Cn+1: tangent to an(·) at ξen+1
Entrance
boundary ∂Γen+1:
γn+1(·) = an(·)
on [ξen+1, ξn+1]
S[an+1]: The locus of points
γn+1(·)
an(·)
An
satisfying the equation[
1
µ · g ◦ S−1
]
(·, ·) = vn+1(·, ·)
(a)
(c)
(b)
(d)
Figure 8. For all values of the disorder arrival rate (large or small), both the value functions
vn+1(·, ·), n ∈ N0 and the exit boundaries ∂Γxn+1, n ∈ N0 are determined by the entrance
boundaries ∂Γen+1, n ∈ N0; see Lemma 10.7. Figures (a) and (b) illustrate Steps D.1 and
D.2 of Method D page 62 for n = 0, and Figures (c) and (d) for a general n. In (a),
the region A0 = {(x, y) ∈ R2+ : [g + µ · v0 ◦ S] < 0} is readily available since v0 ≡ 0.
There is always a unique number ξe1 contained in the support [0, ξ1] of the boundary function
a0(·) of the region A0 such that the parametric curve C1 : (x(t, ξe1), y(t, a0(ξe1))), t ∈ R does
not intersect A0. The entrance boundary ∂Γ
e
1 of the stopping region Γ1 coincides with the
boundary {(x, (a0(x)) : x ∈ (ξe1, ξ1)} of the region A0 above the interval (ξe1, ξ1). If ξe1 > 0,
then the exit boundary ∂Γx1 = Γ1\ cl(Γe1) is not empty and can be found by backtracing
the parametric curves (x(−t, φ0), y(−t, φ1)), t ∈ R+ from every entrance boundary point
(φ0, φ1) ∈ ∂Γe1 until the first time r̂n(φ0, φ1) that Jv0(−t, φ0, φ1) becomes zero, see Step D.1
for the details. After the value function v1(·, ·) is calculated in Step D.1, the region A1 and
its boundary function a1(·) is found by a transformation under S−1 of the locus in (b) of the
points satisfying [(1/µ) · g ◦ S−1](·, ·) = v1(·, ·). By reiterating Steps D.1 and D.2 as in (c)
and (d) for every n ∈ N, we obtain all of the value functions vn, n ∈ N.
64 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Step D.1: There is unique number φ0 = ξ
e
n+1 in the bounded support φ0 ∈ [0, ξn+1] of
the function an(·) such that, for every small δ > 0
an(x(t, φ0)) ≤ y(t, an(φ0)), t ∈ [0, δ) if φ0 = 0, and t ∈ (−δ, δ) if φ0 > 0.
Equivalently, the parametric curve Cn+1 , R2+ ∩ {(x(t, ξen+1), y(t, an(ξen+1))) : t ∈ R}
in (3) of Proposition 12.17 majorizes the boundary {(x, an(x)) : x ∈ supp(an)} of
the region An = {(x, y) ∈ R2+ : [g + µ · vn ◦ S](x, y) < 0} everywhere. The entrance
boundary of the stopping region Γn+1 = {(φ0, φ1) ∈ R2+ : vn+1(φ0, φ1) = 0} is given
by ∂Γen+1 = {(φ0, an(φ0)) : φ0 ∈ (ξen+1, ξn+1)}.
(i) Find the entrance boundary ∂Γen+1.
(ii) For every (φ0, φ1) ∈ ∂Γen+1, take the following steps to calculate the value func-
tion vn+1(·, ·) on the continuation region Cn+1 and the exit boundary ∂Γxn+1:
(a) Calculate r̂(φ0, φ1) , inf{t ≥ 0 : (x(−t, φ0), y(−t, φ1)) 6∈ R2+}.
(b) If −Jvn(−r̂(φ0, φ1), φ0, φ1) < 0, then set r̂n(φ0, φ1) =∞. Otherwise, find
r̂n(φ0, φ1) , inf{t ∈ (0, r̂(φ0, φ1)] : −Jvn(−t, φ0, φ1) ≥ 0}
by a bisection search on (0, r̂(φ0, φ1)], and add the point
(x(−r̂n(φ0, φ1), φ0), y(−r̂n(φ0, φ1), φ1)) ∈ ∂Γxn+1
to the exit boundary.
(c) Calculate the value function
vn+1(x(−t, φ0), y(−t, φ1)) = −e−(λ+µ)tJvn(−t, φ0, φ1)
along the curve (x(−t, φ0), y(−t, φ1)), t ∈ (0, r̂(φ0, φ1)∧ r̂n(φ0, φ1)] until it
either leaves R2+ or hits the exit boundary ∂Γxn+1.
The union ∂Γxn+1 ∪ cl(∂Γen+1) = ∂Γxn+1 ∪ {(x, an(x)) : x ∈ [ξen+1, ξn+1]} gives the
boundary ∂Γn+1 = {(x, γn+1(x)) : x ∈ [0, ξn+1]} and the boundary curve γn+1(·),
which is strictly decreasing and convex on its support [0, ξn+1].
(iii) Set vn+1(·, ·) = 0 on the stopping region Γn+1 = {(x, y) : y ≥ γn+1(x)}.
Step D.2: Set n to n + 1. Determine the locus of the points (φ0, φ1) in R2+ satisfying
the equation [
1
µ
· g ◦ S−1
]
(φ0, φ1) = vn(φ0, φ1).
This locus is the same as {(x, S[an+1](x)) : x ∈ supp(S[an+1])}, see Notation 9.2.
Shift it by the linear transformation S−1 of (5.8) to obtain the boundary {(x, an(x)) :
ADAPTIVE POISSON DISORDER PROBLEM 65
x ∈ supp(an)} of the region An = {(x, y) : [g + µ · vn ◦ S](x, y) < 0}. Go to Step
D.1.
12.18. Conjecture. The algorithm relies on only two results from Section 12: (i) the entrance
boundary ∂Γen+1, n ∈ N0 is connected, and (ii) the boundary ∂Γxn+1, n ∈ N0 is the disjoint
union of the exit boundary ∂Γxn+1 and the closure of the entrance boundary ∂Γ
e
n+1. Part (ii)
was proved by using (i) and the first hypothesis A1(n + 1) on page 60, see Lemma 12.13.
We conjecture that the hypothesis A1(n+ 1) always holds for all n ∈ N0.
On the other hand, part (i) was proved by using the continuity of the mapping (φ0, φ1) 7→
rn(φ0, φ1) on the connected continuation region Cn+1, see Corollary 12.6. The continuity of
the mapping rn(·, ·) followed from its continuous differentiability on Cn+1 which we proved
by using the implicit function theorem (Theorem 12.2) under hypothesis A2(n + 1), see
Lemma 12.4. We conjecture that this mapping is always continuous on the continuation
region Cn+1. This may be proved directly by using a weaker version of the implicit function
theorem (see, e.g., Krantz and Parks (2002)) or by using nonsmooth analysis (see, e.g.,
Clarke et al. (1998)).
12.2. The regularity of the value functions and the optimal stopping boundaries
when the disorder arrival rate λ is “large”. One of the cases where both A1(n) and
A2(n) on page 60 are satisfied for every n ∈ N is when the disorder arrival rate λ is “large”,
see Section 4.3 and Figure 2(a).
Suppose that λ ≥ [1− (1 + µ)(c/2)]+. Then the parametric curve t 7→ (x(t, φ0), y(t, φ1)),
and therefore, the mapping t 7→ Gn(t, φ0, φ1), t ∈ R+ are strictly increasing for every
(φ0, φ1) ∈ R2+ and n ∈ N0, see Lemma 10.2. Hence, A1(n) always holds for every n ∈ N0.
For the same reason, all of the exit boundaries ∂Γxn, n ∈ N are empty, see Section 11. Since
∂Γx1 is empty, the value function v1(·, ·) is continuously differentiable everywhere. Therefore,
A2(1) holds. Then Proposition 12.17 implies that v2(·, ·) is continuously differentiable ev-
erywhere since ∂Γx2 is empty. Therefore, A2(2) holds, and so on.
12.19. Corollary (“Large” disorder arrival rate: smooth solutions of reference optimal stop-
ping problems). Suppose that λ ≥ [1− (1+µ)(c/2)]+. Then A1(n) and A2(n) hold for every
n ∈ N0, and Proposition 12.17 applies. Particularly, for every n ∈ N0
(1) the value function vn+1(·, ·) is continuously differentiable everywhere,
(2) the exit boundary ∂Γxn+1 is empty, and ∂Γn+1 = cl(∂Γ
e
n+1),
66 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
(3) the boundary function γn+1(·) is continuously differentiable on the interior of its sup-
port [0, ξn+1]. Thus, the function γn+1(·) coincides with the boundary function
a0(x) = −x+ λ
c
√
2 of the region A0 on the interval
[
0,
λ
√
2
2c
]
,
and fits smoothly to this function at the right end-point of the same interval.
The last part of (3) in the corollary follows from (11.11) in Section 11 and Proposi-
tion 12.17. Recall also from Remark 9.6 that, if the disorder arrival rate λ is “large”,
then there is an increasing sequence of sets R+ × [Bn,∞) whose limit is R+ × (0,∞), and
v(·, ·) = vn(·, ·) on R+ × [Bn,∞) for every n ∈ N. Therefore, Corollary 12.19 implies im-
mediately that the value function v(·, ·) and the boundary function γ(·) are continuously
differentiable on R+ × (0,∞) and on the interior of the support [0, ξ] of the function γ(·),
respectively.
To prove that v(·, ·) is continuously differentiable on (0,∞)× {0}, we shall again use the
implicit function theorem. By Proposition 5.6 and Remark 5.10, we have
v(φ0, 0) = Jv(r(φ0, 0), φ0, 0) =
∫ r(φ0,0)
0
e−(λ+µ)tG(t, φ0, 0)dt, φ0 ∈ R+.
The function (t, φ0) 7→ G(t, φ0, 0) , [g + µ · v ◦ S](x(t, φ0), y(t, 0)) is continuously dif-
ferentiable on (0,∞) × (0,∞) since v(·, ·) is continuously differentiable on R+ × (0,∞)
and (x(t, φ0), y(t, 0)) ∈ (0,∞) × (0,∞) for every t > 0. Moreover, the partial derivative
(t, φ0) 7→ Dφ0G(t, φ0, 0) is locally bounded on (0,∞) × (0,∞) by Corollary 5.4. Therefore,
the function (t, φ0) 7→ Jv(t, φ0, 0) is continuously differentiable on (0,∞)× (0,∞) and
Dφ0Jv(t, φ0, 0) =
∫ t
0
e−(λ+µ)uDφ0G(t, φ0, 0)du
=
∫ t
0
e−(µ+1)u [1 + (µ− 1)Dφ0v ◦ S] (x(u, φ0), y(u, 0))du, (t, φ0) ∈ R+ × (0,∞).
Since v(φ0, 0) ≡ 0 for every φ0 ∈ [ξ,∞), it is continuously differentiable on (ξ,∞). To
show that it is differentiable on (0, ξ), it is enough to prove that the mapping φ0 7→ r(φ0, 0)
from (0, ξ) to R+ is continuously differentiable. Observe that, if we define
F (t, φ0) , γ(x(t, φ0))− y(t, 0), (t, φ0) ∈ R2+,
then F (r(φ0, 0), φ0) = 0 for every φ0 ∈ [0, ξ]. For every φ0 ∈ (0, ξ), the function F (·, ·)
is continuously differentiable in some neighborhood of (r0(φ0, 0), φ0) since x(r(φ0, 0), φ0) ∈
ADAPTIVE POISSON DISORDER PROBLEM 67
(0, ξ) and γ(·) is continuously differentiable on [0, ξ). Moreover, at every (t, φ0) ∈ R2+, where
DtF (t, φ0) exists, we have
DtF (t, φ0) = γ
′(x(t, φ0))Dtx(t, φ0)−Dty(t, 0) < 0,
since γ(·) is decreasing, t 7→ x(t, 0) and t 7→ x(t, φ0) are strictly increasing. Then the
implicit function theorem implies that φ0 7→ r(φ0, 0), and therefore, φ0 7→ v(φ0, 0) =
Jv(r(φ0, 0), φ0, 0) is continuously differentiable on φ0 ∈ (0, ξ). A similar argument as in
12.12 shows that φ0 7→ r(φ0, 0) is continuous at φ0 = ξ and limφ0↑ξ r(φ0, 0) = 0. By Leibniz
rule (see, e.g., Protter and Morrey(1991, Theorem 11.1, p. 286)), the limit of the derivative
Dφ0v(φ0, 0) = DtJv(r(φ0, 0), φ0, 0)︸ ︷︷ ︸
=0 for every φ0∈[0,ξ)
+Dφ0Jv(r(φ0, 0), φ0, 0)
=
∫ r(φ0,0)
0
e−(µ+1)u [1 + (µ− 1)Dφ0v ◦ S] (x(u, φ0), y(u, 0))du, φ0 ∈ (0, ξ)
of the value function v(·, ·) at (φ0, 0) as φ0 increases to ξ equals zero. Recall that, since
r(φ0, 0) > 0 for every φ0 ∈ [0, ξ), the derivative of t 7→ Jv(t, φ0, 0) on the righthand side
vanishes at its minimizer t = r(φ0, 0). Thus, the left and right derivatives of the concave
function φ0 7→ v(φ0, 0) at φ0 = ξ,
D−φ0v(ξ, 0) = limφ0↑ξ
D−φ0v(φ0, 0) = limφ0↑ξ
Dφ0v(φ0, 0) = 0 = D
+
φ0
v(ξ, 0),
are equal. This completely shows that the function φ0 7→ v(φ0, ·) from (0,∞) to R is
continuously differentiable. Hence the value function v(·, ·) is continuously differentiable on
R+ × {0}.
12.20. Corollary (“Large” disorder arrival rate: smooth solution of the main optimal stop-
ping problem). Suppose that λ ≥ [1− (1 + µ)(c/2)]+. Then
(1) the value function v(·, ·) is continuously differentiable everywhere,
(2) the boundary function γ(·) is continuously differentiable on the interior of its support
[0, ξ]. It coincides with the boundary function
a0(x) = −x+ λ
c
√
2 of the region A0 on the interval
[
0,
λ
√
2
2c
]
,
and fits smoothly to this function at the right end-point of the same interval,
(3) the value function v(·, ·) is the solution of the variational inequalities in (12.1)-(12.4).
68 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
12.21. Corollary. If λ ≥ [1 − (1 + µ)(c/2)]+, then both the sequence {vn}n∈N of the value
functions and the sequences of their partial derivatives {Dφ0vn}n∈N and {Dφ1vn}n∈N converge
uniformly to the value function v and its partial derivatives Dφ0v and Dφ1v, respectively.
Proof. Immediately follows from Theorem 25.7 in Rockafellar (1997, p. 248). 
The results obtained in Section 10 for the functions vn, n ∈ N can be extended easily for
the value function v(·, ·), n ∈ N. As in Lemma 10.2, we have
A , {(x, y) ∈ R2+ : [g + µ · v ◦ S](x, y) < 0} = {(x, y) ∈ R2+ : y < a(x)},
{(x, y) ∈ R2+ : [g + µ · v ◦ S](x, y) = 0} = {(x, a(x)) : x ∈ [0, α]}
for some decreasing function a : R+ 7→ R+ which is strictly decreasing on its finite support
[0, α]. We have A ⊆ C all the time, and the equality holds if λ ≥ [1 − (1 + µ)(c/2)]+ since
the parametric curves t 7→ (x(t, φ0), y(t, φ1)) increase and do not come back the region A
after they leave; see also Section 11. Therefore, γ(·) ≡ a(·) and
[g + µ · v ◦ S](x, y) > 0, (x, y) ∈ Γ\∂Γ.(12.19)
Proof of Corollary 12.20. Only (3) remains to be proven. The function v : R2+ 7→ (−∞, 0]
is bounded and continuously differentiable. By the definition of the continuation region
C = {(x, y) ∈ R2+ : v(x, y) < 0} and the stopping region Γ = R2+\C, the (in)equalities (12.2)
and (12.4) are satisfied. On the other hand, (12.19) implies
[(A˜ − λ)v + g](φ0, φ1) = [g + µ · v ◦ S](φ0, φ1), (φ0, φ1) ∈ Γ
is strictly positive for every (φ0, φ1) ∈ Γ\∂Γ, i.e., (12.3) is also satisfied. On the other hand,
[(A˜ − λ)v + g](φ0, φ1) =
= Dφ0v(φ0, φ1)
[
(λ+ 1)φ0 +
λ(1−m)√
2
]
+Dφ1v(φ0, φ1)
[
(λ− 1)φ1 + λ(1 +m)√
2
]
+ µ
[
v
((
1− 1
µ
)
φ0,
(
1 +
1
µ
)
φ1
)
− v(φ0, φ1)
]
− λv(φ0, φ1) + g(φ0, φ1)
= Dφ0v(φ0, φ1) ·Dtx(0, φ0) +Dφ1v(φ0, φ1) ·Dty(0, φ1)− (λ+ µ)v(φ0, φ1)
+ [g + µ · v ◦ S](φ0, φ1)
=
∂
∂t
[
e−(λ+µ)tv(x(t, φ0), y(t, φ1)) +
∫ t
0
e−(λ+µ)t[g + µ · v ◦ S](x(u, φ0), y(u, φ1))du
]∣∣∣∣
t=0
=
∂
∂t
[
e−(λ+µ)tv(x(t, φ0), y(t, φ1)) + Jv(t, φ0, φ1)
]∣∣
t=0
, (φ0, φ1) ∈ C.
ADAPTIVE POISSON DISORDER PROBLEM 69
Observe that the expression enclosed in square brackets in the last equation above equals
v(φ0, φ1) for every sufficiently small t > 0 by (5.19) in Remark 5.10. Therefore, the derivative
above equals zero, and (12.1) holds. This completes the proof of that the function v(·, ·)
satisfies the variational inequalities (12.1)-(12.4).
The boundary function γ(·) is strictly decreasing on its support. The process Φ˜ can have
at most countably many jumps, and its sample paths are strictly increasing between the
jumps. Therefore, the time that the process Φ˜ spends on the boundary ∂Γ = {(x, γ(x)) :
x ∈ [0, ξ]} equals zero almost surely. Finally, since the derivative of the convex boundary
curve 0 ≥ γ′(x) ≥ γ′(0+) = a0(0+) = −1 is bounded on x ∈ (0, ξ), it is also Lipschitz
continuous on its support. 
Finally, Corollary 12.20 also shows that, for every λ ≥ [1 − (1 +m)(c/2)]+, the smooth
restrictions of value function vn+1(·, ·) to the continuation region Cn+1 and to the stopping
region Γn+1 fit to each other smoothly across the smooth boundary ∂Γn+1 = {(x, γn+1(x) :
x ∈ [0, ξn+1]}.
However, if 0 < λ < 1− (1+m)(c/2) is small, then the corresponding value function does
not have to have the same smooth-fit property.
12.3. Failure of the smooth-fit principle: a concrete example. Here we shall give
a concrete example for a case where the value function fits smoothly across the entrance
boundary and fails to fit smoothly across the exit boundary of the optimal stopping region,
see Figure 9(d).
Suppose that the disorder arrival rate λ, the pre-disorder arrival rate µ of the observations,
the detection delay cost c per unit time, and the expectation m = E0[Λ−µ] of the difference
Λ− µ between the arrival rates of the observations after and before the disorder are chosen
such that 
0 < λ < 1− (1 +m)(c/2)
µ+ 1
µ
φd > φ1
y < S[a0](x) =
µ+ 1
µ
a0
(
µ
µ− 1 x
)
, (x, y) ∈ {(φ∗0, φ∗1), (0, φ1)}

,(12.20)
where φd > 0 is the mean-reversion level in (4.14) of y 7→ y(t, φ1) for every initial condition
φ1 ∈ R+, see Section 4.4. The point
(φ∗0, φ
∗
1) =
(
λ√
2
(
1− λ
c
− 1
)
,
λ√
2
(
1 + λ
c
+ 1
))
(12.21)
70 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
is the intersection point of the straight lines ` in (6.7) and y = a0(x). Recall from (12.15) that
a0(·) is the boundary function of the region A0 = {(x, y) ∈ R2+ : g(x, y) < 0} ≡ {(x, y) ∈
R2+ : y < a0(x)}. For every initial point (φ0, φ1) in R2+, the sum t 7→ x(t, φ0) + y(t, φ1),
t ∈ R+ of the coordinates of the parametric curve t 7→ (x(t, φ0), y(t, φ1)), t ∈ R+ strictly
decreases before the parametric curve meets the line `, and strictly increases thereafter, see
Lemma 12.3 and (6.8). Finally, the point (0, φ1) with
φ1 = −
λ(1 +m)√
2(λ− 1) +
[
φ∗1 +
λ(1 +m)√
2(λ− 1)
][
1 + φ∗0
√
2(λ+ 1)
λ(1−m)
]−(λ−1)/(λ+1)
(12.22)
is the initial point on the y-axis of the parametric curve t 7→ (x(t, 0), (y, φ1)), t ∈ R+
which passes trough the point (φ∗0, φ
∗
1) in (12.21). The coordinate φ1 in (12.22) is found by
substituting the solution of x(t∗, 0) = φ∗0 for t
∗ into the equation y(t∗, φ1) = φ
∗
1 and solving
the latter for φ1; see also Figure 9(a).
Let us show that, under the conditions in (12.20), the “closedness” property in (9.4) holds.
By Lemma 12.3 and (6.8), the curve C1 in Corollary 12.9 becomes
C1 = {(x(t, 0), y(t, φ1)) : t ∈ R+} ≡ R2+ ∩ {(x(t, φ∗0), y(t, φ∗1)) : t ∈ R};
it is tangent to the broken line {(x, a0(x)) : x ∈ R+} at the point (φ∗0, φ∗1). Therefore,
ξe1 = φ
∗
0 by the same corollary, and the entrance boundary of the stopping region Γ1 =
{(x, y) : v1(x, y) = 0} is ∂Γe1 = {(x, a0(x)) : x ∈ (φ∗0, (λ/c)
√
2)} by Corollary 12.6. Moreover,
the boundary function γ1(·) of the region Γ1 = {(x, γ1(x)) : γ1(x) ≤ y} is supported on
[0, (λ/c)
√
2] and satisfies
γ1(x) = a0(x), x ∈
[
φ∗0,
λ
c
√
2
]
and γ1(x) < y(0, φ1) = φ1, x ∈ [0, φ∗0).(12.23)
The equality follows from Corollary 12.7, and the inequality follows from Remark 12.10 and
that the parametric curve C1 is decreasing. One can easily see from (12.23) and the second
inequality in (12.20) that
(0, φd) ∈ R+ × [φd,∞) ⊂ S−1(Γ1) =
{
(x, S−1[γ1](x)) : x ∈ R+
}
,
φd ≥ S−1[γ1](0) = µ
µ+ 1
γ1(0).(12.24)
ADAPTIVE POISSON DISORDER PROBLEM 71
φ1
λ
c
√
2
φd
A0 ≡ C0
(φ∗0, φ
∗
1)
C1
0 λc
√
2
a0(·)
S[a0](·):
[
1
µ · g ◦ S−1
]
(·, ·) = v0(·, ·)
`(a)
λ
c
√
2
φd
0 λc
√
2
B1
S[a1(·)]:
[
1
µ · g ◦ S−1
]
(·, ·) = v1(·, ·)
a1(·)
apply S−1
S−1[γ1](·)
γ(0)
ξe = φ∗0
γ1(·)
A1
x1
(c)
The value function v(·, ·)
is differentiable everywhere except
on the exit boundary ∂Γx
λ
c
√
2
φd
0
γ(0)
ξe = φ∗0
γ1(·)
γ2(·)
of the stopping region Γ
the exit boundary ∂Γx
γ(·)
x1
λ
c
√
2 ξ
Γ(d)
A0 ≡ C0
λ
c
√
2
φ1
(φ∗0, φ
∗
1)
C1
γ1(0)
φd
a0(·) γ1(·)
S−1[γ1](·)
v(·, ·) = v1(·, ·) on R+ × [B1,∞)
∂Γx = ∂Γx1 = {(x, γ1(x)) : x ∈ [0, ξe1)}
0
µ
µ+1γ1(0) ≡ B1
x1
λ
c
√
2ξe1 = φ
∗
0
γ(·) = γ1(·) on [0, x1)
(b)
Figure 9. (a) shows the location of points (φ∗0, φ
∗
1) and (0, φ1) and the line described by the
function S[a0](·). If the sufficient-statistic process Φ˜ starts in the region R+ × [φd,∞), then
it stays there forever and jumps above the line y = φ1 every time an observation arrives.
In (b), one can calculate the value function v1(·, ·) by backtracing the parametric curve
t 7→ (x(t, φ0), y(t, φ1)) from every (φ0, φ1) on the entrance boundary ∂Γe1 = {(x, a0(x)) :
x ∈ (ξe1, (λ/c)
√
2)}, see Figure 8(a,b). The thick curve above the region A0 is the boundary
function γ1(·) of the stopping region Γ1. Since S−1(Γ) is “closed” in the sense of (9.4), the
functions v(·, ·) and v1(·, ·) (and therefore, every vn(·, ·), n ∈ N) coincide on S−1(Γ). In
(c), we recall how to find the region A1; the calculation of the value function v2(·, ·) and
the boundary function γ2(·) is similar to (b), see also Figure 6. Since the exit boundary
{(x, γ(x)) : x ∈ [0, ξe)} is the same for all of the functions and is contained in R+ × [φd,∞),
the functions a1(·) and γ2(·) coincide on [ξe,∞). This and the boundary function γ(·) of the
stopping region Γ = {(x, y) ∈ R2+ : v(x, y) = 0} are sketched in (d).
72 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
The restrictions of the value functions v(·, ·) and v1(·, ·), and therefore those of the bound-
aries ∂Γ and ∂Γ1, coincide on R+ × [φd,∞). First, observe thatx(t, φ0) + y(t, φ1) ≥ x(t, 0) + y(t, φ1) ≥
λ
c
√
2
i.e., (x(t, φ0), y(t, φ1)) /∈ C0, t ∈ R+
 , (φ0, φ1) ∈ R+ × [φ1,∞),
where C0 = {(x, y) ∈ R2+ : g(x, y) < 0} is as in (4.13) and coincides with A0. By the second
inequality in (12.20) and the properties of the parametric curves t 7→ (x(t, φ0), y(t, φ1)),
t ∈ R+ (see Section 4.2), we have
(φ0, φ1) ∈ R+ × [φd,∞) =⇒
{
S(φ0, φ1) ∈ R+ × [φ1,∞) ⊂ R+ × [φd,∞)
(x(t, φ0), y(t, φ1)) ∈ R+ × [φd,∞), t ∈ R+
}
.
Using the last two displayed equations gives that, if the initial state Φ˜0 on a sample-path of
the sufficient statistic Φ˜ = (Φ˜(0), Φ˜(1)) is in R+ × [φd,∞), then the sample-path stays in the
region R2+ × [φd,∞) and never returns to the advantageous region C0 after the first jump,
see Section 4.1. In fact,
v ◦ S(φ0, φ1) = inf
τ∈S
ES(φ0,φ1)0
[∫ τ
0
e−λug(Φ˜u)du
]
= 0, (φ0, φ1) ∈ R+ × [φd,∞),
and therefore,
(12.25) v(φ0, φ1) = J0v(φ0, φ1) = inf
t∈[0,∞]
∫ t
0
e−(λ+µ)u
≡g(·,·)︷ ︸︸ ︷
[g + µ · v ◦ S](x(u, φ0), y(u, φ1))du
= J0v0(φ0, φ1) = v1(φ0, φ1), (φ0, φ1) ∈ R+ × [φd,∞).
The stopping region Γ = {(x, y) ∈ R2+ : v(x, y) = 0} and its boundary ∂Γ are determined
by the value function v(·, ·). Then (12.25) implies that the restrictions of the boundaries ∂Γ
and ∂Γ1 to the region R+ × [φd,∞) also coincide. Therefore, the first inequality in (12.20)
implies
φd <
λ
c
√
2 ≤ γ1(0) = γ(0), and S−1[γ](0) ≡ µ
µ+ 1
γ(0) < φd
follows from (12.24). Since the boundary function S−1[γ](·) of the region S−1(Γ) is decreasing
(see (9.7)), the second inequality gives
R+ × [φd,∞) ⊆ S−1(Γ) = {(x, y) ∈ R2+ : S−1[γ](x) ≤ y}.
But starting at any (φ0, φ1) ∈ R × [0, φd], the parametric curves t 7→ (x(t, φ0), y(t, φ1)),
t ∈ R+ are increasing. Since the boundary functions S−n[γ](·) of the regions S−n(Γ) =
ADAPTIVE POISSON DISORDER PROBLEM 73
{(x, y) ∈ R2+ : S−n[γ](x) ≤ y}, n ∈ N are also decreasing, every region S−n(Γ), n ∈ N is
“closed” in the sense of (9.4). Therefore, Method A on page 43 can be used in order to
calculate the value function v(·, ·) on R2+.
12.22. Corollary. Suppose that (12.20) holds. Let Bn , [µ/(µ+ 1)]nγ1(0) for every n ∈ N.
Then the sequence R+×[Bn,∞), n ∈ N increases to R+×(0,∞), and we have v(·, ·) = vn(·, ·)
on R+ × [Bn,∞) for every n ∈ N.
Since (φ∗0, φ
∗
1) ∈ R+ × [φd,∞) ⊆ R+ × [B1,∞), the exit boundaries ∂Γx1 and ∂Γx of the
stopping regions Γ1 and Γ are the same, and
∂Γx = ∂Γx1 = {(x, γ1(x)) : x ∈ [0, ξe1)} ≡ {(x, γ1(x)) : x ∈ [0, φ∗0)}.
From the entrance boundary ∂Γe1 = {(x, a0(x)) : x ∈ (φ∗0, (λ/c)
√
2)} of the stopping region
Γ1, we can obtain its exit boundary ∂Γ
x
1 and the value function v1(·, ·) on the continuation
region C1 by using Method D on page 62, see Figures 8(a,b) and 9(b).
Note also that the value function v(·, ·) ≡ v1(·, ·) is continuously differentiable on R+ ×
[B1,∞)\∂Γx1 and is not differentiable on ∂Γx1 by Corollary 12.5 and Lemma 12.16. Let
x1 ≡ x1(γ1) = min{x ∈ R+ : S−1[γ1](x) = γ1(x)} is the (smallest) intersection point of the
functions S−1[γ1](·) and γ1(·) as in (9.9). Then Corollary 9.4 implies
{(x, γ(x)) : x ∈ R+} ∩ S−1(Γ1) = {(x, γ1(x)) : x ∈ [0, x1]},
and the restriction of the boundary function γ(·) ≡ γ1(·) to the interval [0, x1) is continuously
differentiable by Lemma 12.15.
Using Corollary 12.22, we can also show that the restrictions of the value function v(·, ·)
and the boundary ∂Γ of the stopping region Γ on the complement of the region R+× [B1,∞)
are continuously differentiable.
Since the sequence {vn(·, ·)}n∈N of the value functions increases to the function v(·, ·), all
of them coincide with v(·, ·) ≡ v1(·, ·) on the region R+× [B1,∞). On the region R+× [0, B1),
they differ, but are continuously differentiable.
In fact, since every parametric curve t 7→ (x(t, φ0), y(t, φ1)), t ∈ R+ starting at any point
(φ0, φ1) ∈ R+ × [0, φd] ⊃ R+ × [0, B1] is increasing, the hypothesis A1(n) on page 60 holds
on the region R+ × [0, φd] for every n ∈ N.
On the other hand, the third inequality in (12.20) guarantees that hypothesis A2(n) on
page 60 also holds on R+ × [0, φd] for every n ∈ N. Indeed, every entrance boundary
∂Γen+1 coincides with some part of the boundary ∂An = {(x, an(x)) : x ∈ R+} of the region
74 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
An = {(x, y) ∈ R2+ : [g+µ·vn◦S](x, y) < 0}, see Lemma 10.5. Since the sequence {an(·)}n∈N0
of the boundary functions is increasing, the third inequality in (12.20) implies
y < S[a0](x) ≤ S[an](x), n ∈ N0, (x, y) ∈
{
(φ∗0, φ
∗
1), (0, φ1)
}
.
Thus, by an induction on n ∈ N0, we can easily show that the transformation S(∂Γen+1) of
the entrance boundary ∂Γen+1 of every stopping region Γn+1 is away from the exit boundary
∂Γxn+1 ≡ ∂Γx1 . Therefore, the function (x, y) 7→ [g + µ · vn ◦ S](x, y) is differentiable on
the entrance boundary ∂Γen+1. The same induction, as in Section 12.2, will also prove the
continuous differentiability of the value functions vn(·, ·), n ∈ N and v(·, ·) on the region
R+ × [0, φd] ⊃ R+ × [0, B1], as well as, the continuous differentiability of the restrictions of
the boundaries ∂Γn, n ∈ N and ∂Γ to the set R+ × [0, φd].
12.23. Corollary. Suppose that (12.20) holds. Then the boundary function γ(·) of the stop-
ping region Γ = {(x, y) ∈ R2+ : γ(x) ≤ y} is continuously differentiable on its support [0, ξ].
The exit boundary Γx is not empty. The value function v(·, ·) is continuously differentiable
on R2+\∂Γx, but not differentiable on ∂Γx.
The interesting feature of the solutions of the problems covered under condition (12.20) is
that the smooth-fit principle is satisfied on one connected subset and violated on another of
the same connected and continuously differentiable boundary curve of the optimal stopping
region by the value function, which is also continuously differentiable everywhere away from
the boundary.
The conditions in (12.20) are satisfied, for example, if λ = 0.15, µ = 1.5 and c = 0.7 and
m = 0.9. In general, the functions S−1[a0](·) and a0(·) always intersect on the line y = x.
Since γ1(·) ≥ a0(·) and γ1(·) is decreasing, we have x1 ≤ (λ/c) · (
√
2/2). The equality holds
if and only if
S−1(φ∗0, φ
∗
1) ∈ {(x, y) ∈ R2+ : x < y} ⇐⇒ 1 < µ(λ+ c).
This condition is satisfied for the numbers above. As a result, we have x1 = (λ/c) · (
√
2/2)
and γ(x) = a0(x) = x−(λ/c)
√
2 for every x ∈ [φ∗0, x1]. The boundary function γ(·) is strictly
above the function a0(·) everywhere else.
ADAPTIVE POISSON DISORDER PROBLEM 75
13. Appendix: proofs of selected results in Part 2
Proof of Proposition 9.3. Let us prove (9.5) for n = 1. Take (φ0, φ1) ∈ S−1(Γ). By (9.4),
the curve u 7→ (x(u, φ0), y(u, φ1)), u ∈ R+ does not leave S−1(Γ). Therefore,
S
(
x(u, φ0), y(u, φ1)
) ∈ Γ and (V ◦ S)(x(u, φ0), y(u, φ1)) = 0, u ∈ R+.
Then Lemma 5.6, (5.4), (5.6) and Proposition 5.5 imply that
V (φ0, φ1) = J0V (φ0, φ1) = inf
t∈[0,∞]
∫ t
0
e−(λ+µ)u[g + µ · V ◦ S](x(u, φ0), y(u, φ1))du
= inf
t∈[0,∞]
∫ t
0
e−(λ+µ)ug
(
x(u, φ0), y(u, φ1)
)
du = J0V0(φ0, φ1) = V1(φ0, φ1).
Since V is the limit of the decreasing sequence {Vn}n∈N, the equalities V = V1 = V2 = · · ·
on S−1(Γ) follow.
On S−1(Γ) ∩C, we have 0 > V = V1 = V2 = · · · . Therefore, S−1(Γ) ∩C ⊆ Ck for every
k ≥ 1. Taking intersection of both sides with S−1(Γ) gives S−1(Γ) ∩C ⊆ S−1(Γ) ∩Ck for
every k ≥ 1. To prove the opposite inclusion, note that V = Vk < 0 on S−1(Γ)∩Ck for every
k ≥ 1. Therefore, S−1(Γ) ∩ Ck ⊆ C, k ≥ 1. Intersecting both sides with the set S−1(Γ)
gives S−1(Γ) ∩Ck ⊆ S−1(Γ) ∩C, k ≥ 1.
The proof of S−n(Γ) ∩ Γ = S−n(Γ) ∩ Γn = S−n(Γ) ∩ Γn+1 = · · · reads the same as in the
previous paragraph after every “C” above is replaced with “Γ”, and every strict inequality
is replaced with an equality. This completes the proof of (9.5) for n = 1.
Suppose that (9.5) holds for some n ∈ N, and let us prove it for n + 1. Take (φ0, φ1) ∈
S−(n+1)(Γ). Since the curve u 7→ (x(u, φ0), y(u, φ1)), u ∈ R+ does not leave the region
S−(n+1)(Γ) by (9.4), we have S
(
x(u, φ0), y(u, φ1)
) ∈ S−n(Γ), u ∈ R+, and
(V ◦ S)(x(u, φ0), y(u, φ1)) = (Vn ◦ S)(x(u, φ0), y(u, φ1)), u ∈ R+
by induction hypothesis. Then Lemma 5.6, (5.4), (5.6) and Proposition 5.5 imply that
V (φ0, φ1) = J0V (φ0, φ1) = inf
t∈[0,∞]
∫ t
0
e−(λ+µ)u[g + µ · V ◦ S](x(u, φ0), y(u, φ1))du
= inf
t∈[0,∞]
∫ t
0
e−(λ+µ)u[g + µ · Vn ◦ S]
(
x(u, φ0), y(u, φ1)
)
du = J0Vn(φ0, φ1) = Vn+1(φ0, φ1).
Since V is the limit of the decreasing sequence {Vn}n∈N, we have V = Vn+1 = Vn+2 = · · · on
S−(n+1)(Γ). From these equalities follows the proof of the equalities of the regions in (9.5)
for n+ 1, by the similar arguments presented for n = 1 above. 
76 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Proof of Lemma 10.2. The obvious choices are the function an : R+ 7→ R+ and the num-
ber αn in (10.3) and (10.4), respectively. By the discussion above,
{
(x, y) ∈ R2+; [g + µ · vn ◦ S](x, y) < 0
}
= An = R2+\epigraph(an)
=
{
(x, y) ∈ R2+; y < an(x)
}
= {(x, y) ∈ [0, αn)× R+; y < an(x)} ,
and (10.6) follows. The proof will be complete if we show the equality in (10.5).
Since [g + µ · vn ◦ S](x, y), x ∈ R+ is continuous, we have [g + µ · vn ◦ S](x, an(x)) ≥ 0 for
every x ∈ R+, and the equality holds for every x ∈ [0, αn) because an(x) > 0, x ∈ [0, αn).
Because an(·) is also continuous, the equality also holds for (x, y) = (αn, a(αn)), and
[g + µ · vn ◦ S](x, an(x)) = 0, x ∈ [0, αn].(13.1)
The identity (10.5) will follow immediately if we show for the same An in (10.1) that
[g + µ · vn ◦ S](x, y) > 0, (x, y) ∈
(
R2+\An
) \{(x, an(x)) : x ∈ [0, αn]}.(13.2)
The nonpositive function vn(·, ·) is concave and equal to zero outside the bounded region
Cn. Therefore, the functions y 7→ vn(x, y), x ∈ R+ and x 7→ vn(x, y), y ∈ R+ are nonpositive,
concave and equal zero for every large real y and x, respectively. This implies that the
functions y 7→ vn(x, y), x ∈ R+ and x 7→ vn(x, y), y ∈ R+ are nondecreasing. Therefore, the
functions y 7→ [g+ µ · vn ◦ S](x, y), x ∈ R+ and x 7→ [g+ µ · vn ◦ S](x, y), y ∈ R+ are strictly
increasing since both S(x, y) and g(x, y) are strictly increasing in both x and y. Now, (13.2)
follows from (13.1). 
Proof of Lemma 10.7. Fix any (φ0, φ1) ∈ ∂Γen+1. Then vn+1(φ0, φ1) = 0, and substituting
(φ−t0 , φ
−t
1 ) , (x(−t, φ0), y(−t, φ1)) into (10.15) for any t ∈ [0, r̂(φ0, φ1)] gives
Jtvn(x(−t, φ0), y(−t, φ1)) = −e−(λ+µ)tJvn(−t, φ0, φ1), t ∈ [0, r̂(φ0, φ1)],(13.3)
thanks to the semigroup property of x(·, ·) and y(·, ·).
By the definition of the entrance boundary ∂Γen+1 in (10.10), the point (φ0, φ1) is reachable
from the inside of the continuation region Cn+1. Namely, there exists some δ > 0 such that
(x(−t, φ0), y(−t, φ1)) ∈ Cn+1 and rn(x(−t, φ0), y(−t, φ1)) = t for every t ∈ (0, δ]. Then
(10.16) implies
0 > vn+1(x(−t, φ0), y(−t, φ1)) = Jtvn(x(−t, φ0), y(−t, φ1)) = −e−(λ+µ)tJvn(−t, φ0, φ1)
ADAPTIVE POISSON DISORDER PROBLEM 77
for every t ∈ (0, δ]. Since r̂n(φ0, φ1) is the first time when the last function on the right may
change its sign, we obtain
−Jvn(−t, φ0, φ1) < 0, t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)).
Using (10.16) once again, we conclude
vn+1(x(−t, φ0), y(−t, φ1)) ≤ Jtvn(x(−t, φ0), y(−t, φ1))
= −e−(λ+µ)tJvn(−t, φ0, φ1) < 0, t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)).
Thus
{(x(−t, φ0), y(−t, φ1)); t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1))} ⊆ Cn+1,
rn(x(−t, φ0), y(−t, φ1)) = t, t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)),
vn+1(x(−t, φ0), y(−t, φ1)) = −e−(λ+µ)tJvn(−t, φ0, φ1), t ∈ (0, r̂n(φ0, φ1) ∧ r̂(φ0, φ1)).
The third equation follows from the second and (13.3), and the second equation follows from
the first and the fact (x(t, x(−t, φ0), y(t, y(−t, φ1))) = (φ0, φ1) ∈ Γ. Taking the limit in the
third equation as t increases to r̂n(φ0, φ1) gives vn+1(x(−t, φ0), y(−t, φ1))
∣∣∣
t=brn(φ0,φ1)
= 0, and
(x(−r̂n(φ0, φ1), φ0), y(−r̂n(φ0, φ1), φ1)) ∈ ∂Γxn+1
 if r̂n(φ0, φ1) ≤ r̂(φ0, φ1).
Finally, every (φ˜0, φ˜1) ∈ Cn+1∪∂Γxn+1 is reachable from (φ0, φ1) ≡ rn(φ˜0, φ˜1) ∈ ∂Γen+1 on the
entrance boundary by the curve {(x(t, φ˜0), y(t, φ˜1)); t ∈ [0, rn(φ˜0, φ˜1)]} which is contained
(possibly, except the end-points) in the continuation region Cn+1. 
Proof of Corollary 12.6. By Lemma 12.4, the function (φ0, φ1) 7→ r0(φ0, φ1) is continuous
on the continuation region (φ0, φ1) ∈ C1. Therefore, the entrance boundary ∂Γe1 is the image
of the continuous mapping, see the definition in (10.10),
(φ0, φ1) 7→ (x(r0(φ0, φ1), φ1), γ1(y(r0(φ0, φ1), φ1))), (φ0, φ1) ∈ C1
from the connected region C1 into R2+. Thus the set ∂Γe1 is a connected subset of R2+.
Since the parametric curves t 7→ (x(t, φ0), y(t, 0)), φ0 ∈ R+ starting on the x-axis are
increasing, the points on the boundary ∂Γ1 where these curves meet the boundary belong
to the entrance boundary ∂Γe1; see also Figure 2. Hence {(x, γ1(x)) : x ∈ [δ, ξ1)} ⊆ ∂Γe1 for
some 0 ≤ δ < ξ1. Then the connectedness of ∂Γe1 gives (12.14) with ξe1 , inf{x ∈ R+ :
(x, γ1(x)) ∈ ∂Γe1}.
78 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Indeed the point (ξe1, γ1(ξ
e
1)) does not belong to the entrance boundary ∂Γ
e
1. Suppose it
does. Then {(x(−t, ξe1), y, (−t, γ1(ξe1)); t ∈ (0, δ]} ⊂ C1 for some δ > 0. Let (φ0, φ1) ∈ C1 be
the point in the middle of the vertical line-segment connecting the points (x(−δ, ξe1), y(−δ, γ1(ξe1)))
and (x(−δ, ξe1), γ1(x(−δ, ξe1))). Then we have x(δ, φ0) = x(δ, x(−δ, ξe1)) = ξe1 and y(δ, φ1) >
y(δ, y(−δ, γ1(ξe1))) = γ1(ξe1) = γ1(x(δ, φ0)) since the mapping φ 7→ y(t, φ) is increasing for
every t ∈ R. Therefore, (x(δ, φ0), y(δ, φ1)) ∈ Γ1 and 0 < r0(φ0, φ1) < δ. Thus we have
(x(r0(φ0, φ1), φ0), y(r0(φ0, φ1), φ1)) ∈ ∂Γe1, but x(r0(φ0, φ1), φ0) < x(δ, φ0) = ξe1 (the mapping
φ 7→ x(t, φ) is increasing for every t ∈ R). This contradicts with the minimality of ξe1. 
Proof of Corollary 12.12. Suppose ξe1 > 0 and fix any φ0 ∈ [0, ξe1). Let φ0 , (1/2)(φ0+ξe1).
Then (φ0, γ1(φ0)) ∈ ∂Γx1 , and γ1(φ0) > γ1(φ0) > γ1(ξe1) since γ1(·) is strictly decreasing on
its support. Then the set B , [0, φ0) × (γ1(φ0),∞) is an open neighborhood of the point
(φ0, γ1(φ0)) such that for every (φ˜0, φ˜1) ∈ B ∩C1, we have
0 < r ≤ r0(φ˜0, φ˜1) ≤ r <∞,
where r , inf{t ≥ 0 : y(t, γ1(φ0)) ≤ γ1(ξe1)} and r , inf{t ≥ 0 : x(t, 0) ≥ ξ1}. This completes
the proof of the first part.
Now let (φ0, φ1) ∈ ∂Γe1 be a point on the entrance boundary. Take any convergent sequence
{(φ(n)0 , φ(n)1 )}n∈N in the continuation region C1 whose limit is the boundary point (φ0, φ1).
Since r0(·, ·) ≤ r (see above) on C1, the sequence {r0(φ(n)0 , φ(n)1 )}n∈N is bounded and has a
convergent subsequence. We shall conclude the proof of the second part by showing that
every convergent subsequence of the sequence {r0(φ(n)0 , φ(n)1 )}n∈N has the same limit 0.
Without changing the notation, suppose that {r0(φ(n)0 , φ(n)1 )}n∈N converges to some finite
number r0 ≥ 0. Since the value function (φ0, φ1) 7→ v1(φ0, φ1) from R2+ to R and the function
(t, φ0, φ1) 7→ Jv0(t, φ0, φ1) from R3+ to R are continuous, we have
0 = v1(φ0, φ1) = lim
n→∞
v1(φ
(n)
0 , φ
(n)
1 ) = lim
n→∞
Jv0
(
r0(φ
(n)
0 , φ
(n)
1 ), φ
(n)
0 , φ
(n)
1
)
= Jv0(r0, φ0, φ1) =
∫ r0
0
e−(λ+µ)tG0(t, φ0, φ1)dt.
If we show that G0(t, φ0, φ1) > 0 for every t > 0, then r0 = 0 follows.
However, t = 0 is a point of increase for the function t 7→ G0(t, φ0, φ1). Since (φ0, φ1) ∈
∂Γe1 = {(x, a0(x)) : x ∈ (ξe1, ξ1)} by Corollary 12.7, and the boundary function a0(·) of the
region A0 = {(x, y) ∈ R2+ : [g + µ · v0 ◦ S](x, y) < 0} is strictly decreasing, there exists some
ADAPTIVE POISSON DISORDER PROBLEM 79
δ > 0 such that (x(t, φ0), y(t, φ1)) ∈ A0 ⊆ C1 for every t ∈ [−δ, 0). Therefore,
G(t, φ0, φ1) = [g + µ · v0 ◦ S](x(t, φ0), y(t, φ1)) < 0 = G0(0, φ0, φ1), t ∈ [−δ, 0).
Then Lemma 12.3 implies that G0(t, φ0, φ1) > 0 for every t > 0 and completes the proof of
r0 = 0. 
Proof of Lemma 12.13. If ξe1 = 0, then cl(∂Γ
e
1) = {(x, γ1(x)) : x ∈ [0, ξ1]} = ∂Γ1 by
Corollary 12.6. In the remainder, suppose that ξe1 > 0 and fix any φ0 ∈ [0, ξe1). The
boundary point (φ0, γ1(φ0)) is not included in the entrance boundary ∂Γ
e
1. We shall prove
that it is an exit boundary point; namely, there exists some δ > 0 such that (see (10.10))
(x(t, φ0), y(t, γ1(φ0))) ∈ C1, ∀ t ∈ (0, δ].(13.4)
Since the boundary γ1(·) is strictly decreasing on its support [0, ξ1], we have
0 ≤ φ0 < ξe1 =⇒ γ1(φ0) > γ1(ξe1).
Then there is always a sequence of points {(φ(n)0 , φ(n)1 )}n∈N ⊆ C1 such that
φ
(n)
0 = φ0 and φ
(n)
1 > γ1(ξ
e
1) for every n ∈ N, and lim
n→∞
φ
(n)
1 =↑ γ1(φ0).
Namely, the sequence {(φ(n)0 , φ(n)1 )}n∈N “increases” to the point (φ0, γ1(φ0)) along the vertical
line passing through the point (φ0, γ1(φ0)). For every n ∈ N, we have
v1(φ
(n)
0 , φ
(n)
1 ) = Jv0
(
r0(φ
(n)
0 , φ
(n)
1 ), φ
(n)
0 , φ
(n)
1
)
, and(
x
(
r0(φ
(n)
0 , φ
(n)
1 ), φ
(n)
0
)
, y
(
r0(φ
(n)
0 , φ
(n)
1 ), φ
(n)
1
))
∈ ∂Γe1.
By Corollary 12.12, the sequence {r0(φ(n)0 , φ(n)1 )}n∈N is bounded. Therefore, it has a conver-
gent subsequence; we shall denote it by the same notation and its limit by r0. The functions
Jv0(·, ·, ·), x(·, ·), y(·, ·) and v1(·, ·) are continuous, and v1(φ0, γ1(φ0)) = 0. Therefore, taking
limits of the displayed equations above gives
0 = Jv0(r0, φ0, γ1(φ0)) and (x(r0, φ0), y(r0, γ1(φ0))) ∈ cl(∂Γe1).(13.5)
The second expression implies that x(r0, φ0) ≥ ξe1. We shall prove that the inequality is
strict, and therefore,
(x(r0, φ0), y(r0, γ1(φ0))) ∈ ∂Γe1.(13.6)
Let us assume that x(r0, φ0) = ξ
e
1. Then the second expression in (13.5) implies that
(x(r0, φ0), y(r0, γ1(φ0))) = (ξ
e
1, γ1(ξ
e
1)). Thus (φ0, γ1(φ0)) is on the curve C1 given by (12.16).
80 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Then Corollary 12.9 implies that G(t, φ0, γ1(φ0)) > 0 for every t 6= r0. Since r0 > 0, this
implies that
Jv0(r0, φ0, γ1(φ0)) =
∫ r0
0
e−(λ+µ)sG0(s, φ0, γ1(φ0))ds
is strictly positive. But this contradicts the first equality in (13.5). Therefore, we must have
x(r0, φ0) > ξ
e
1, and (13.6) is correct.
Now we are ready to prove (13.4). Since φ0 < ξ
e
1, we have [g + µ · v0 ◦ S](φ0, γ1(φ0)) > 0
by Corollary 12.11. Because the mapping [g+µ · v0 ◦S](·, ·) is continuous, there exists some
r0 > δ > 0 such that
G0(t, φ0, γ1(φ0)) = [g + µ · v0 ◦ S](x(t, φ0), y(t, γ1(φ0))) > 0, t ∈ [0, δ].
Then for every t ∈ (0, δ]
v0(x(t, φ0), y(t, γ1(φ0))) ≤ Jv0(r0 − t, x(t, φ0), y(t, γ1(φ0)))
=
∫ r0−t
0
e−(λ+µ)u[g + µ · v0 ◦ S]
(
x(u, x(t, φ0)), y(u, y(t, γ1(φ0)))
)
du
= e(λ+µ)t
∫ r0
t
e−(λ+µ)u[g + µ · v0 ◦ S]
(
x(t, φ0), y(t, γ1(φ0))
)
du
= e(λ+µ)t
Jv0(r0, φ0, γ1(φ0))︸ ︷︷ ︸
=0
−
∫ t
0
e−(λ+µ)uG0(u, φ0, γ1(φ0))du
 < 0.
Therefore, (13.4) holds, and (φ0, γ1(φ0)) ∈ ∂Γx1 . 
Proof of Lemma 12.14. There is nothing to prove if ξe1 = 0. Therefore, suppose ξ
e
1 > 0.
Let B1 be the union of the continuation region C1 and the open subset of [0, ξ
e
1)×R+ strictly
below the curve C1 in Corollary 12.9, see Figure 7(b). Then B1 is open and C1 ∪ ∂Γx1 ⊂ B1,
see Remark 12.10. Define{
r˜0(φ0, φ1) , inf{t > 0 : (x(t, φ0), y(t, φ1)) ∈ Γe1}
v˜1(φ0, φ1) , Jv0(r˜0(φ0, φ1), φ0, φ1)
}
, for every (φ0, φ1) ∈ B1.
Then
r0(φ0, φ1) = r˜0(φ0, φ1) and v1(φ0, φ1) = v˜1(φ0, φ1), (φ0, φ1) ∈ C1 ∪ ∂Γx1 ,(13.7)
Let us show that r˜0(·, ·), and therefore, v˜1(·, ·) are continuously differentiable on B1. The
infimum r˜0(φ0, φ1) is finite and strictly positive for every (φ0, φ1) ∈ B1. By (10.12),
G0(r˜(φ0, φ1), φ0, φ1) = [g + µ · v0 ◦ S](x(t, φ0), y(t, φ1)) = 0, (φ0, φ1) ∈ B1.(13.8)
ADAPTIVE POISSON DISORDER PROBLEM 81
The mapping (t, φ0, φ1) 7→ G0(t, φ0, φ1) from R3+ to R is continuously differentiable. If
DtG0(t, φ0, φ1)
∣∣∣
t=er0(φ0,φ1)
6= 0, (φ0, φ1) ∈ B1,(13.9)
then Theorem 12.2 implies that, in an open neighborhood in B1 of every (φ0, φ1), the equation
G0(t, φ0, φ1) = 0 determines t = t(φ0, φ1) implicitly as a function of (φ0, φ1), and this function
is continuously differentiable. In every neighborhood, these solutions must then coincide
with r˜0(φ0, φ1). Therefore, r˜0(φ0, φ1) is continuously differentiable on B1. Then the function
v˜1(φ0, φ1) is continuously differentiable on B1 since Jv0(·, ·, ·) is continuously differentiable
on R3+.
Now fix any (φ0, φ1) ∈ B1 and assume DtG0(r˜0(φ0, φ1), φ0, φ1) = 0. Then the function
t 7→ G0(t, φ0, φ1) has a local minimum at t = r˜0(φ0, φ1). Lemma 12.3 and (13.8) imply that
G0(t, φ0, φ1) > 0 for every t 6= r˜0(φ0, φ1). Therefore, the parametric curve
{(x(t, φ0), y(t, φ1)) : t ∈ R} ∩ R2+ ⊆ R2+\A0
does not intersect A0, but touches the boundary ∂A0. Then this curve has to be the same
as C1 in Corollary (12.16), and (φ0, φ1) ∈ C1. But this contradicts with (φ0, φ1) ∈ B1, since
Remark 12.10 and the description of B1 show that C1 ∩B1 = ∅. Therefore, (13.9) holds.
Now let us show that γ1(·) is continuously differentiable on [0, ξe1). Fix any φ0 ∈ [0, ξe1).
Then (φ0, γ1(φ0)) ∈ ∂Γx1 ⊂ B1, and v˜1(φ0, γ1(φ0)) = 0 by (13.7). The function v˜1(·, ·) is
continuously differentiable on B1. Therefore, the result will again follow from the implicit
function theorem (Theorem 12.2) if we show that Dφ1 v˜1(φ0, γ1(φ0)) 6= 0. However,
Dφ1 v˜1(φ0, γ1(φ0)) = D
−
φ1
v˜1(φ0, γ1(φ0)) = D
−
φ1
v1(φ0, γ1(φ0))
= lim
φ1↑γ1(φ0)
D−φ1v1(φ0, φ1) = limφ1↑γ1(φ0)
Dφ1v1(φ0, φ1) = lim
φ1↑γ1(φ0)
1− e−(µ+1)r0(φ0,φ1)
µ+ 1
> 0.
The second equality follows from (13.7), and the third from the concavity of v1(·, ·). The
fourth and the fifth follow from Corollary 12.5. Finally, the limit at the end is strictly positive
since r0(·, ·) is bounded away from zero in the intersection of C1 with some neighborhood of
(φ0, γ1(φ0)) by Corollary 12.12. 
Proof of Lemma 12.15. The result follows from Corollary 12.7 if ξe1 = 0. Therefore, sup-
pose ξe1 > 0. Then the boundary function γ1(·) is continuously differentiable on [0, ξe1)∪(ξe1, ξ1)
by Corollary 12.7 and Lemma 12.14. We need to show that x 7→ γ1(x) is continuously dif-
ferentiable at x = ξe1.
82 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Recall that the function γ1(·) is convex. Therefore, the left derivativeD−γ1(·) and the right
derivative D+γ1(·) of the function γ1(·) exist and are left- and right- continuous, respectively,
at x = ξe1. Thus
lim
x↑ξe1
Dγ1(x) = lim
x↑ξe1
D−γ1(x) = D−γ1(ξe1) ≤ D+γ1(x) = lim
x↓ξe1
D+γ1(x) = lim
x↓ξe1
Dγ1(x).(13.10)
The continuity of the derivative Dγ1(·) of the function γ1(·) at x = ξe1 will follow immediately
from the existence of the derivative of γ1(·) at x = ξe1.
Now recall from Corollary 12.9 and Remark 12.10 that the point (ξe1, γ1(ξ
e
1)) is on the
parametric curve C1, which lays above {(x, γ1(x)) : x ∈ R+} and touches it at the point
(ξe1, γ1(ξ
e
1)). Therefore, for every t > 0 and s > 0
y(0, γ1(ξ
e
1))− y(−t, γ1(ξe1))
x(0, ξe1)− x(−t, ξe1)
≤ γ1(x(0, ξ
e
1))− γ1(x(−t, ξe1))
x(0, ξe1)− x(−t, ξe1)
≤ γ1(x(s, ξ
e
1))− γ1(x(0, ξe1))
x(s, ξe1)− x(0, ξe1)
≤ y(s, γ1(ξ
e
1))− y(0, γ1(ξe1))
x(s, ξe1)− x(0, ξe1)
.
When we take the limit as t ↓ 0 and s ↓ 0, we obtain
Dty(0, γ1(ξ
e
1))
Dtx(0, ξe1)
≤ D−γ1(ξe1) ≤ D+γ1(ξe1) ≤
Dty(0, γ1(ξ
e
1))
Dtx(0, ξe1)
.
Note that the terms on far left and far right are the same. Therefore, D−γ1(ξe1) = D
+γ1(ξ
e
1)
and the derivative of the boundary function γ1(·) at x = ξe1 exists. 
Proof of Lemma 12.16. Since v1(·, ·) is concave, the left partial derivatives D−φ0v1(·, ·),
D−φ1v1(·, ·) and the right partial derivatives D+φ0v1(·, ·), D+φ1v1(·, ·) exist and are left- and
right-continuous on the boundary ∂Γ, respectively. Because v1(·, ·) vanishes on Γ1, and the
function γ1(·) is strictly decreasing, we have
D−φ0v1(φ0, φ1) ≥ D+φ0v1(φ0, φ1) = 0, (φ0, φ1) ∈ ∂Γ1\{(0, γ1(0))}.(13.11)
D−φ1v1(φ0, φ1) ≥ D+φ1v1(φ0, φ1) = 0, (φ0, φ1) ∈ ∂Γ1\{(ξ1, 0)}.(13.12)
For every boundary point (φ0, φ1) ∈ ∂Γ1\{(0, γ1(0))} and any sequence {(φ(n)0 , φ(n)1 )}n∈N ⊂
C1 such that limn→∞ φ
(n)
0 =↑ φ0 and φ(n)1 = φ1 for every n ∈ N, we have
(13.13) D−φ0v1(φ0, φ1) = limn→∞
D−φ0v1(φ
(n)
0 , φ
(n)
1 ) = lim
n→∞
Dφ0v1(φ
(n)
0 , φ
(n)
1 )
= lim
n→∞
1− exp
{
−(µ− 1)r0(φ(n)0 , φ(n)1 )
}
µ− 1 .
ADAPTIVE POISSON DISORDER PROBLEM 83
The second and the third equalities follow from Corollary 12.5. The function r0(·, ·) is con-
tinuous on the entrance boundary ∂Γe1 and is bounded away from zero in some neighborhood
of every point on the exit boundary ∂Γx1 , see Corollary 12.12. Therefore, the limit on the
right in (13.13) equals zero for every point (φ0, φ1) on the entrance boundary ∂Γ
e
1 and is
strictly positive for every point (φ0, φ1) on the exit boundary ∂Γ
x
1 .
Thus, for every (φ0, φ1) ∈ ∂Γe1, the equality in (13.11), and as a result of a similar argu-
ment, the equality in (13.12) are attained. Therefore, the partial derivatives Dφ0v1(·, ·) and
Dφ1v1(·, ·) exist at every (φ0, φ1) ∈ ∂Γe1 and are continuous since Dφ0v1(·, ·) = D±φ0v1(·, ·) is
both left- and right-continuous near the entrance boundary ∂Γe1.
However, if (φ0, φ1) is a point on the exit boundary ∂Γ
x
1 , then the inequalities in (13.12)
and (13.13) are strict. Namely, the v1(·, ·) is not differentiable on the exit boundary ∂Γx1 . 
References
Bayraktar, E. and Dayanik, S. (2003). Poisson disorder problem with exponential penalty
for delay. Preprint. Princeton University.
(http://www.princeton.edu/˜sdayanik/papers/expo.pdf).
Bayraktar, E., Dayanik, S. and Karatzas, I. (2004a). Adaptive Poisson disorder problem.
Preprint. Princeton University.
Bayraktar, E., Dayanik, S. and Karatzas, I. (2004b). The standard Poisson disorder problem
revisited. Preprint. Princeton University.
(http://www.princeton.edu/˜sdayanik/papers/standard.pdf).
Bre´maud, P. (1981). Point Processes and Queues, Springer-Verlag, New York. Martingale
dynamics, Springer Series in Statistics.
Clarke, F. H., Ledyaev, Y. S., Stern, R. J. and Wolenski, P. R. (1998). Nonsmooth Analysis
and Control Theory, Vol. 178 of Graduate Texts in Mathematics, Springer-Verlag, New
York.
Davis, M. H. A. (1976). A note on the Poisson disorder problem, Banach Center Publ.
1: 65–72.
Davis, M. H. A. (1993). Markov Models and Optimization, Vol. 49 ofMonographs on Statistics
and Applied Probability, Chapman & Hall, London.
Galchuk, L. I. and Rozovskii, B. L. (1971). The disorder problem for a Poisson process,
Theory of Prob. and Appl. 16: 729–734.
Gugerli, U. S. (1986). Optimal stopping of a piecewise-deterministic Markov process, Stochas-
tics 19(4): 221–236.
84 ERHAN BAYRAKTAR, SAVAS DAYANIK, AND IOANNIS KARATZAS
Krantz, S. G. and Parks, H. R. (2002). The Implicit Function Theorem, Birkha¨user Boston
Inc., Boston, MA. History, Theory, and Applications.
Liptser, R. S. and Shiryaev, A. N. (2001). Statistics of Random Processes. I, Vol. 5 of
Applications of Mathematics (New York), expanded edn, Springer-Verlag, Berlin. General
theory, Translated from the 1974 Russian original by A. B. Aries, Stochastic Modelling
and Applied Probability.
Øksendal, B. (1998). Stochastic Differential Equations, Springer-Verlag, Berlin.
Peskir, G. and Shiryaev, A. N. (2002). Solving the Poisson disorder problem, Advances in
finance and stochastics, Springer, Berlin, pp. 295–312.
Protter, M. H. and Morrey, Jr., C. B. (1991). A First Course in Real Analysis, Undergraduate
Texts in Mathematics, second edn, Springer-Verlag, New York.
Rockafellar, R. T. (1997). Convex Analysis, Princeton Landmarks in Mathematics, Princeton
University Press, Princeton, NJ. Reprint of the 1970 original, Princeton Paperbacks.
(Erhan Bayraktar) Department of Mathematics, University of Michigan, Ann Arbor, MI
48109
E-mail address: erhan@umich.edu
(Savas Dayanik) Department of Operations Research and Financial Engineering, and the
Bendheim Center for Finance, Princeton University, Princeton, NJ 08544
E-mail address: sdayanik@princeton.edu
(Ioannis Karatzas) Departments of Mathematics and Statistics, Columbia University, New
York, NY 10027
E-mail address: ik@math.columbia.edu
