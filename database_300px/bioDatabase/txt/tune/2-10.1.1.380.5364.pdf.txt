Large deviations with diminishing rates∗
Adam Shwartz
Electrical Engineering
Technion—Israel Institute of Technology
Haifa 32000, Israel
Alan Weiss
Bell Laboratories, Lucent Technologies
Murray Hill, NJ
March 2002; revised March 2003, December 2003, March 2004
Abstract
The theory of large deviations for jump Markov processes has been
generally proved only when jump rates are bounded below, away from
zero [4, 9, 13]. Yet various applications of interest do not satisfy this con-
dition. We describe several classes of models where jump rates diminish
to zero in a Lipschitz continuous way. Under appropriate conditions, we
prove that the sample path large deviations principle continues to hold.
Under our conditions, the rate function remains an integral over a local
rate function, which retains its standard representation.
1 Introduction
One of the common technical assumptions in existing large deviations theory
for jump Markov processes is that jump rates are bounded below, away from
zero [4, 9, 13]. This is not merely a technical assumption: if the rates may
go down to zero, the process may get stuck at a point, or it may or may not
be possible to reach certain regions. We illustrate these issues below through
examples. From a technical point of view, this condition is required in order to
obtain smoothness of the local rate function.
However, in many applications this condition is violated. In this paper we
prove the large deviations principle for the sample paths of a wide class of
such models, under the basic scaling of (2.2). Under our conditions, the usual
Markovian integral representation of the rate function continues to hold, with
∗Research supported in part by the United States—Israel Binational Science Foundation
grant no. BSF 1999179. Research of the first author supported also in part by the fund for
promotion of research at the Technion.
1
the same variational formula for the integrand, as in the case with rates bounded
away from zero. We allow the boundary defined by the region where some rates
go to zero to be fairly general; in contrast with existing large deviations theory
which deals only with “flat” boundaries, our boundaries are quite general.
Applications for which rates are not bounded away from zero fall roughly into
two categories. In the first, the rates are proportional to the occupancy of the
system, and thus go to zero when the system empties. In the second, a control
is designed in order to avoid, say, overflows. “Soft controls” are characterized
by continuous jump rates, and these controls may well have rates become zero.
Another example of a soft control is when additional processing power may be
gradually added to a system when it grows beyond a certain threshold. Here
are some specific examples of these applications.
Example A. In an M/M/∞ queue, the service rate is linear in the queue
size, so that as the queue empties, the rate goes to zero. In this case it is
not possible to continue reducing the queue size: the rate diminishes as we
approach a boundary of the state space. This type of behavior is one of our
main motivations, as there are many multi-dimensional models exhibiting this
behavior [11]. To obtain the scaling (2.2), scale the space variable by n and
make the arrival rate nλ.
Example B. Consider an M/M/1 queue scaled by n: arrivals occur at rate
nλ, services at rate nµ, and the queue size x(t) is scaled to zn(t) = x(t)/n.
Suppose that there is an auxiliary M/M/∞ server, also with parameter µ, that
kicks in when the scaled queue size zn(t) > 1, and that each service of this
auxiliary server kicks out a pair of customers. The rate of departure of pairs is
thus µ · (zn(t)−1) whenever zn(t) > 1, and 0 otherwise. Since the rate of jumps
of size (−2) diminishes to zero at zn = 1, this case falls outside the current
theory. One key feature of this model is that the set of possible jump directions
(the positive cone spanned by the jump directions) does not change as the jump
rate diminishes to zero.
Example C. Consider a two queue system with “soft priority” to queue
1. We suppose the two queues behave like independent M/M/1 queues, with
arrival rates 0.5 and service rate 1, as long as the (scaled) size of queue 1 is
below 1. When the first queue is larger than one, the server of queue 2 enters
a processor sharing mode, so that the service rate to queue 1 increases linearly
with queue size until its queue size is 2, and the rate remains at 2 when the first
queue is larger, while service rate at queue 2 decreases linearly with the size of
queue 1 until no service is offered at all. In this case the service rate at queue
2 goes down and is zero on {~x : x1 ≥ 2}. In contrast with the Example A, this
region can be entered with positive probability, so that it is not delineated by
a boundary. In contrast with our other examples, we do not develop a theory
that covers this case, although the methods we develop suffice for the analysis
of simple models of this type.
Example D. We describe a simplified model of connection admission control
(CAC), based on an idea of David Tse [15]. Suppose that customers arrive at
an infinite-server queue with Poisson rate nλ. Arrivals may be turned away
(blocked), according to rules given in the next paragraph. Accepted customers
2
depart the queue at rate µ. We let q(t) represent the number in the system at
time t. Customers who are accepted (not blocked) have two states, represented
by 0 and 1. Accepted customer i moves from xi = 0 to xi = 1 with rate α, and
from xi = 1 to xi = 0 with rate β. The bandwidth customer i uses at time t is
xi(t); the total bandwidth in use at time t is thus b(t)
4
=
∑q(t)
i=1 xi(t).
Now for the problem. Suppose that the capacity of the system is nC, so
that there is trouble if
∑
i xi(t) > nC. We try to ensure that this happens with
small probability by accepting connections only when the scaled system state
~zn(t)
4
= 1/n(b(t), q(t)) is in a fixed region G. For some fixed δ > 0 we define the
probability that a connection is accepted by
P(accept) =

1 if ~zn(t) ∈ G and d[~zn(t), ∂G] > δ
1
δ
dist[~zn(t), ∂G] if ~zn(t) ∈ G and d[~zn(t), ∂G] ≤ δ
0 otherwise
We may try to design the set G so that the cheapest rate function for a path
from ~zn(0) to the set b(t)/n = C is the same for all starting points ~zn(0) ∈ ∂G.
In conclusion, we have described a Markov model of CAC that fits into the
theory we develop in this paper, although we do not carry out the analysis and
design of the appropriate region G for this model.
Example E. This example is to show some potential pitfalls in the theory
of diminishing rates. Consider a pure birth process with λ(x) = |x|. This
process moves to the right and, since λ(0) = 0, if x(0) < 0 is an integer, then
x(t) ≤ 0 for all t. Using the formulas for the local rate functions that hold
true when the rates are bounded below, a formal calculation (detailed below in
Section 7) of the rate function I(r) for the path r(t) = t − 0.5, t ∈ [0, 1] yields
I(r) < ∞, implying that P (process ≈ r(t)) ≈ e−nI(r) > 0. It is clear that the
probability of the process following near this path is exactly 0, so the formal
calculation of the rate is incorrect. However, if the process x(t) starts with a
non-integer value, then the probability is strictly positive, and it turns out that
in this case the formal calculation is correct, although we will not detail that
straightforward calculation in this paper. In more generality, if the rates λ(x)
are bounded below, then the sequence of processes with rates λn(x) = nλ(x)+1
is exponentially equivalent to the case with λn(x) = nλ(x) so that, on the
large deviations scale, their behavior is identical. However, for the decreasing
rate case described above it leads to completely different behavior, since now it
becomes possible to cross the x = 0 barrier. The upshot is that the form of the
rate function we derive is in some sense less robust than it is under the usual
condition that the rates are bounded away from 0.
We note that our analysis holds for processes confined to a convex set by
having jump rates, in directions heading out of the set, diminish to zero at the
boundary, as in Example D. Thus we establish, for the first time, a sample-path
large deviations principle in the case of curved boundaries. Previous published
work [13, Chapter 8], [6, 4, 9] dealt only with flat boundaries; there is some
unpublished work dealing with curved boundaries with some restrictions. Our
3
general approach is based on [13]; indeed, we use this as the source of many of
our lemmas, and sometimes prove theorems by giving only the changes necessary
to use arguments in [13].
In general, there is no exponential equivalence between processes with log-
bounded rates and those whose rates may vanish, as Example E clearly shows.
In fact, it is natural to approximate a process with rates that go to zero by
imposing a small lower bound, say ε, on the rates, and there is an obvious
coupling between these two processes. However, it is not hard to see that this
coupling does not provide an exponential approximation. Therefore, we resort
to the more technical approach of following the steps of [13].
Our approach turns the problem of estimating the frequency or manner of
occurrence of a rare event into a variational problem. Such problems are not
necessarily easy to solve, although many one dimensional problems have been,
and some authors (e.g., [10]) have solved some specific multidimensional models.
The relationship between our finite-time approach and steady-state statistics is
given by the Freidlin-Wentzell theory, given for diffusion processes in [8], and
for queues in [13, Chapter 6]. There are many other approaches to sample-path
large deviations; see, for example, [5] or [12]. The literature on non-sample-
path large deviations is vast. The most relevant early references for the types
of models we address are [2, 3, 14], which include tail estimates in steady state.
The paper is organized as follows. In the next Section 2 we set up the no-
tation and describe the problem as well as our main results: Theorem 3 for
processes with boundaries, where some jump rates go to zero at the boundary,
and Theorem 4 for processes with diminishing rates but where, at each point,
the jump directions associated with positive rates span Rd. Corollary 5, § 2
shows that these two results can be combined, to cover a large class of mod-
els exhibiting small rates both near boundaries and in the interior. Finally,
in Corollary 9 we remove the technical assumption that the set of interest is
compact, to obtain our results under the weakest assumptions. This last ex-
tension is a consequence of Corollary 8, a result of independent interest, on the
exponential tightness of models with rates that grow at most linearly.
In Section 3 we develop some preliminary results. Next in Section 4 we prove
the large deviations upper bound for the case that rates diminish at a boundary.
In Section 5 we establish the corresponding lower bound. In Section 6 we state
and prove the large deviations principle when some rates become zero in the
interior of the region, for models such as Example B of the introduction.
In Section 7 we give a simple sufficient condition for when the rate function
is finite for paths that approach a boundary; this shows that many boundaries
may be reached with probability that decreases to zero at an exponential rate, as
opposed to a superexponential rate. This condition is also virtually necessary in
the one dimensional case. Section 8 summarizes our results, and sketches open
problems. Appendix 1 contains a technical lemma that allows us to extend
Lipschitz continuous jump rates from regions that are unions of convex sets to
all of Rd. Appendix 2 contains an example illustrating the type of processes we
consider, and shows how to verify a technical assumption.
4
2 Assumptions and main results
We study jump Markov processes with boundaries where some jump rates be-
come zero. The structure of the processes is simple, and is detailed first. The
structure of the boundaries of the regions of interest is more complicated, and
is deferred for a few paragraphs. Using the notation of [13], our model of jump
processes is specified through k jump directions {~ej}kj=1 and their respective
Poisson jump rates {λj(~x)}, which are defined for all ~x in some set G (see As-
sumption 1). We assume that the jump rates are Lipschitz continuous functions
and, without loss of generality, that the positive cone (defined in (2.3)) spanned
by the {~ej} is Rd. We call such a process x(t). Previous studies have assumed
that the the jump rates are uniformly bounded away from 0; the sole novelty of
this paper is the relaxation of that condition.
We scale the process x(t) in space and time to zn(t) as follows. All jump
rates are multiplied by a scaling parameter n, and all jump sizes are divided by
n; in other words, for zn(t), jump ~ej becomes ~ej/n, and occurs at rate nλj . The
generators for the original and scaled process are thus given respectively by
Lf(~x) 4=
∑
j
λj(~x)(f(~x + ~ej) − f(~x)) (2.1)
Lnf(~x) 4=
∑
j
nλj(~x)
(
f
(
~x+
~ej
n
)
− f(~x)
)
. (2.2)
We now begin our description of the boundaries with some notation and
definitions. We denote by xj the jth coordinate of a vector ~x, by ∂S the
boundary of a set S, by So the interior of S, by B(x, r) the open ball of radius
r > 0 centered at x, and by d(~x, ~y) the Euclidean distance between ~x and ~y.
Given a set of vectors {~uj}, the positive cone spanned by the vectors is
C{~uj} 4=
~v : there exist αj ≥ 0 with ~v = ∑
j
αj~uj
 . (2.3)
The cone generated by the positive jump rates at ~x is denoted by
Cx 4= C {~ej : λj(~x) > 0} . (2.4)
To motivate our assumptions, suppose G is bounded and convex; then it
as well as its boundary are compact. Therefore, they can be covered with a
finite number of open balls, and in particular we can achieve this covering with
some balls centered on the boundary, and the rest having no intersection with
the boundary. Moreover, at each point of ∂G we can fit a cone which is, at
least locally, contained in G: more precisely, we say that G has an interior cone
property if there are numbers ε > 0, β > 0 (independent of ~x), and for every
~x ∈ ∂G there is a vector ~v such that for each t ∈ (0, ε) we have B(~x+t~v, βt) ⊂ G.
5
Assumption 1 The set G is compact, the closure of its interior. There exist
positive η, ε, γ, δ0, β, vectors ~vi and open balls Bi so that
(i) {Bi = B(~xi, ri), i = 1, . . .I1} covers ∂G and ~xi ∈ ∂G.
(ii) {Bi = B(~xi, ri), i = 1, . . . I} covers G with ~xi ∈ ∂G, i ≤ I1 and ~xi ∈ Go,
i > I1.
(iii) G satisfies an interior cone condition with parameters ε and β. The vectors
~v for the interior cone may be taken as constant, ~vi, in each region Bi. Moreover,
if d(~x, ∂G) < γ then d(~x+ t~vi, ∂G) > ηt and is monotone increasing for 0 ≤ t ≤
ε.
(iv) For any ~x ∈ G we have B(~x, δ0) ∩G ⊂ Bi for some i.
To illustrate that these assumptions are quite weak, we consider the following
class. A set S is called star-like with respect to a point x ∈ S if for any point
y ∈ S the closed line segment between x and y lies in S.
Lemma 1 Suppose G is compact and the closure of its interior, and that there
exists a ball B ⊂ G so that G is star like with respect to each x ∈ B. Then
Assumption 1 holds.
In particular, any compact convex set with non-empty interior satisfies the as-
sumption of the lemma.
Proof. The first statement holds since G is compact and contains a ball. Since
the boundary is compact, we can cover ∂G with balls B centered on the bound-
ary. Cover the interior with balls contained in Go. Now extract a finite cover,
and (i)–(ii) are established.
Take a ball B(~x0, δ), δ ≤ 2, contained in B. Without loss of generality we may
assume that maxi ri <
δ
2 . Suppose ~x ∈ Bj . Then the convex hull ofBj∪B(~x0, δ)
is contained in G, and so the first part of (iii) follows. For the second part take
~vj =
~x0−~xj
2‖~x0−~xj‖ . The last part of (iii) follows from the first since there is a mini-
mal angle to the (finite number of) cones. Finally, (iv) follows by compactness:
assume the contrary and take a sequence of points for which the largest ball is
of size 2−n. Then the limit point cannot belong to any Bi, a contradiction.
A second property that holds easily when G is convex is that we can extend
the jump rates from being defined on G to being defined on Rd while maintaining
their Lipschitz continuity, as follows. Since G is convex, then for each point ~x 6∈
G there is a unique point p(~x) ∈ ∂G that is closest to G: it is the projection of ~x
on G. The definition of λj(~x) can therefore be extended to Rd by setting λj(~x)
4
=
λj(p(~x)). In Appendix 1 we show that if the λi(~x) are Lipschitz continuous for
~x ∈ G, then they are Lipschitz when extended in this way.
The choice of vectors ~vi in Assumption 1.(iii) is obviously not unique. Below
we make the assumption that this choice can be made so that the vectors are
consistent with the directions of increase of the diminishing rates.
The following assumption concerns the case where rates diminish near a
boundary.
6
Assumption 2 The rates and jump directions satisfy the following.
A. There is a constant Kλ such that |λj(~x)− λj(~y)| ≤ Kλ|~x− ~y|. Moreover,
the rates can be extended to a δ neighborhood of G, so that the Lipschitz
property continues to hold.
B. For each ~x ∈ ∂G there is an ε1 > 0 so that ~y ∈ Cx together with |~y| < ε1
implies ~x+ ~y ∈ G.
C. λi(~x) > 0 for all i and ~x ∈ Go. Moreover, γ, ε and ~vi of Assumption 1
can be chosen so that
~vi ∈ C
{
~ej : inf
~x∈Bi
λj(~x) > γ
}
and if ~x ∈ Bi, d(~x, ∂G) < γ and λj(~x) < γ then λj(~x + α~v) is monotone
increasing in α for 0 < α < ε.
D. C{~ej} = Rd.
Note that assumptions C and D together show ~x ∈ Go implies Cx = Rd. Ap-
pendix 2 contains a nontrivial example of a process satisfying the assumptions,
and sketches how to verify Assumption 2.C.
When rates diminish in the interior, Assumptions 2.B–C are not relevant,
and Assumption 2.D is replaced with
Assumption 3 Cx = Rd for all ~x.
This assumption is used for Theorem 4, which does not require all the parts of
Assumption 2.
Note that our assumptions allow the process to jump out of G; see Appendix
2 for a worked example. However, by Assumption 2, at any given point in G—
including the boundary—if the jump size is small enough, then one jump will
not cause the process to exit G. In particular, it is not possible to jump from a
point in ∂G in a direction parallel to ∂G in directions where G is strictly convex.
Finally note that Assumptions 2.A–B imply that rates for jumps out of G must
decrease to 0 as the boundary is approached.
Lemma 2 Let G = ∪Gi, 1 ≤ i ≤ k, where each Gi is a convex compact set.
Suppose that the Gi satisfy the property that any intersection of collections of
Gi is either empty or is the closure of its interior. Then Assumption 1 holds
for G. Furthermore, if the first part of Assumption 2.A holds, then the second
part does as well (Lipschitz extension).
Proof. The first claim follows from reasoning like that of Lemma 1, detailed
below. The second part is proved in the Appendix 1.
Since any nonempty intersection of Gi is convex and is the closure of its inte-
rior, this intersection satisfies the interior cone condition by Lemma 1. Consider
any point y ∈ ∂G. The intersection of all Gi containing y is nonempty. Thus y
7
satisfies the interior cone condition. For any such y there exists a δ(y) such that
the distance between y and any Gi with y /∈ Gi is greater than δ(y) (because
all the Gi are closed). Now we can use a single ball contained in ∩y∈GjGj, as
in Lemma 1, to construct a vector v(y) that serves as a uniform interior cone
direction for starting points near y (nearer than δ(y)/2, say). We are now in
a position to use the compactness of the set G to extract a finite subcover of
neighborhoods that have uniform interior cones. The increasing distance prop-
erty and single Bi property (part iv) now follow as in the proof of Lemma 1.
Note that in our example of the non-robustness of the rate function (Intro-
duction, Example E), the set G is the union of the convex sets (−∞, 0] and
[0,∞) which do not satisfy the intersection property above. The result fails
for this case, since for the ball containing 0 there can be no ~v satisfying 1.(iii).
Consequently, our theory does not apply (which it should not, since Theorem 3
does not hold for this case).
One more note on where our assumptions apply. Recall the second part of
Assumption 2.A follows from the first for convex sets by projection. It is not
hard to show that if there is a δ0 so that for each ~x ∈ ∂G there is a closed ball
B(~y, δ0) with B(~y, δ0) ∩G = ~x then the second part of Assumption 2.A follows
from the first, as we can define projection locally.
For ~x, ~y ∈ Rd and measurable ~r : [0, T ]→ G define
`(~x, ~y)
4
= sup
~θ∈Rd
〈~θ, ~y〉 −∑
j
λj(~x)
(
e〈~θ,~ej〉 − 1
) (2.5)
I[0,T ](~r )
4
=

∫ T
0
`(~r (t), ~r ′(t)) dt if ~r (t) is absolutely continuous,
∞ otherwise.
(2.6)
We can now state our main results. Let D denote the space of bounded, right
continuous functions from [0, T ] to Rd, possessing left-hand limits, and let Ds
denote the space D endowed with the sup norm topology. (Note that Corollary 6
shows that, under the present scaling, we may use either the sup norm topology
or the Skorohod J1 topology.) For any measurable set S in Ds denote
I[0,T ](S)
4
= inf{I[0,T ](~r ) : ~r ∈ S, ~r (0) = ~x}
where the dependence on ~x is suppressed. Our main result concerning rates
that diminish towards a boundary is
Theorem 3 Let Assumptions 1–2 hold and consider the sequence {~zn} of pro-
cesses taking values in Ds. This sequence satisfies the large deviations principle
with rate function I[0,T ]. That is, for each T > 0, closed set C ∈ Ds, and open
8
set O ∈ Ds, and each point ~x ∈ G,
lim sup
n→∞
1
n
logPx (~zn ∈ C) ≤ −I[0,T ](C) (2.7)
lim inf
n→∞
1
n
logPx (~zn ∈ O) ≥ −I[0,T ](O). (2.8)
Moreover, the non negative function I[0,T ] is a good rate function, meaning its
level sets {~r : I[0,T ](~r ) ≤ α,~x ∈ G} are compact for each α.
Suppose now that G = Rd; the process has no boundaries. When the rates
diminish in the interior without changing the positive cone of jump direction,
we have
Theorem 4 Let Assumptions 2.A and 3 hold and consider the sequence {~zn} of
processes taking values in Ds. This sequence satisfies a large deviations principle
with good rate function I[0,T ].
The two results can be combined as follows. The assumptions of Theorem 3
are imposed near the boundary, while the assumptions of Theorem 4 are imposed
away from the boundary. Thus rates may go to zero in the interior, but the cone
can only change on the boundary.
Corollary 5 Let B be an open set so that B ⊂ Go. Let Assumptions 1, 2.A
and 2.B hold in G, let Assumption 2.C hold in G \ B and Assumption 3 holds
for ~x ∈ B. Consider the sequence {~zn} of processes taking values in Ds. This
sequence satisfies a large deviations principle with good rate function I[0,T ].
Proof. Use Theorems 3 and 4 in each region separately. Then connect the
results exactly as in the proof of Lemma 19: see also the comments at the end
of the proof of the lower bound.
Comment: as in [13], we move freely between a jump process and its piece-
wise linear interpolation—which is obtained by interpolating linearly every sam-
ple path between jump points—without changing notation. These two processes
are exponentially equivalent under the sup norm as their distance at any time
is at most one jump. We therefore work with interpolated processes, which
are piecewise linear (and in particular Lipschitz continuous) and for which the
notion of compactness is easier to handle.
Corollary 6 Theorems 3, 4 and Corollary 5 hold as stated if we endow the
space D with the Skorohod J1 topology.
Proof. Consider the linearly interpolated version of the process. The identity
map from Ds to D with the Skorohod topology is continuous. Therefore by
the contraction principle, the theorem holds with the same rate function. Since
the distance between the jump process and the interpolated process is at most
1/n, the two are exponentially equivalent under the Skorohod J1 metric, and
the result is established.
9
Corollary 8, a result of independent interest, shows that linear growth rates
guarantee exponential tightness. This implies that is suffices to prove the upper
bound for compact sets, namely to establish the weak large deviations principle.
Thus, the compactness assumption in Assumption 1 is not necessary: it suffices
that the assumptions hold for bounded subsets of G.
Lemma 7 Assume that the rates λi(~x) have linear growth: λi(~x) ≤ K(1 + |~x|).
Then, uniformly for ~x in compact sets,
lim
r→∞ lim supn→∞
1
n
logPx
(
sup
0≤t≤T
|~zn(t)| > r
)
= −∞ . (2.9)
Proof. Fix b0 and a point ~x with |~x| ≤ b0. Define a process yn by setting
yn(0) = b0 and having y
n increase by Ke = maxj |~ej | with every jump of ~zn.
Then, since yn(t) is increasing,
sup
0≤t≤T
|~zn(t)| ≤ y
n(T )
n
. (2.10)
Take now a collection of independent Yule processes xi, which are pure birth
processes, with xi(0) = 1 and jump rates K1xi where K1
4
= KλkKe. We couple
yn to this collection in the sense that y jumps whenever one of the xi jumps, so
that
yn(t) ≤ b0 +Ke
n∑
1
xi(t) n ≥ b0 . (2.11)
Using [13, Corollary 14.14], it follows that there exists a good rate function `T
so that
`T
(
eK1T
)
= 0, lim
m→∞ `T (am) =∞ (2.12)
for all a > 0, and such that `T (a) is the rate function for xi(t). That is, for each
a > eK1T we have
P
(
n∑
i=1
xi(t) ≥ ap
)
≤ e−n`T (a). (2.13)
Therefore,
lim sup
n
1
n
logPx
(
sup
0≤t≤T
|~zn(t)| > r
)
≤ lim sup
n
1
n
logPx
(
n∑
1
xi(t) >
(r − b0)n
K2
)
(2.14)
= −`T
(
(r − b0)
K2
)
(2.15)
which tends to −∞ as r →∞.
Note that Assumption 2.A implies the condition of Lemma 7. From this we
obtain exponential tightness as follows.
10
Corollary 8 If λi(~x) ≤ K(1+|~x|) for all i then the {~zn} are exponentially tight.
Consequently, it suffices to prove the large deviations estimates for bounded sets,
uniformly in the initial conditions over compact sets.
Proof. Given α > 0, by Lemma 7 there is a rα so that
lim sup
n
1
n
logPx
(
sup
0≤t≤T
|~zn(t)| > rα
)
≤ −α (2.16)
for all initial conditions ~x in a bounded set. But then, since the rates are
Lipschitz, they are bounded over the region |~zn(t)| ≤ rα. Therefore, by [13,
Lemma 5.58] the {~zn} are exponentially tight. Thus by [7, Lemma 1.2.18(a)] it
suffices to prove the upper bound for compact (and in particular bounded) sets.
By [13, (5.31–5.32)] it suffices to prove the lower bound on balls: however, all
paths in a ball are by definition bounded.
Corollary 9 Let G be a closed set and λi(~x) ≤ K(1 + |~x|) for all i. Assume
that there are Rk →∞ and open Bk ⊂ Go so that the conditions of Corollary 5
hold with Bk replacing B, G ∩ B(0, Rk) replacing G and with ∂G ∩ B(0, Rk)
replacing ∂G. Then Theorems 3, 4 and Corollary 5 hold.
Note that the constants in Assumptions 1–2 need not be uniform: they may
depend on Rk. Since it thus suffices to derive our results in the case that G is
compact, we shall do so since this simplifies the technicalities. We shall therefore
assume henceforth that G is bounded and so λ
4
= maxj sup~x λj(~x) <∞.
Lemma 10 There exists a continuous monotone “scale function” s(δ) such that
s(0) = 0, s(δ) > 0 for any δ > 0, and for any ~x ∈ G we have
λi(~x) ≥ s(d(~x, ∂G)) for all i.
Proof. Take s(δ)
4
= inf {λi(~x) : i; ~x : d(~x, ∂G) ≥ δ} The properties follow by
continuity of λi(~x) and compactness of G.
3 Preliminaries
This section collects some preliminary definitions and estimates. We use an
alternative form of ` in many of the proofs. Let
f(µ, λ)
4
=
∑
j
λj − µj + µj log µj
λj
(3.1)
K~y
4
=
µ : µj ≥ 0, ∑
j
µj~ej = ~y
 . (3.2)
Note that, by definition, K~y is non empty if and only if ~y ∈ Cx.
11
Lemma 11 [13, Theorem 5.26] `(x, y) = inf {f(µ, λ(x)) : µ ∈ K~y}.
Note that if λj(x) = 0, then it plays no role in the definition of `. Further, since
f is to be minimized, if λj(x) = 0, the corresponding µj is necessarily 0, too, so
that the j component again plays no role. The lemma also means that either
both expressions are finite, or both are infinite.
Lemma 12 [13, Lemma 5.20] There exists a constant κ such that for all ~x and
all ~y ∈ C~x there exists ~a such that
~y =
k∑
i=1
ai~ei; ai ≥ 0; |~a| ≤ κ |~y| .
Lemma 13 Fix λ. The function f is nonnegative, strictly convex in µ for
µj ≥ 0 with compact level sets. On Cx the following hold. The function f has
a minimum f∗(~y) over K~y, attained at a unique point µ∗(~y), so that f∗(~y) =
f(µ∗(~y), λ). Both the minimum f∗ and µ∗ are continuous in ~y. In addition,
given B, there are constants C0, C1, C2, D so that for all |λi| < B,
|µ∗| ≤ C0 for |~y| ≤ D (3.3)
|µ∗| ≤ C1|~y| for |~y| > D (3.4)
|µ∗| ≥ C2|~y|. (3.5)
Proof. The function 1 − x + x logx is nonnegative and strictly convex for
x ≥ 0, as seen by differentiation. Level sets are compact since the function
grows super-linearly. Existence of a minimum now follows since K~y is closed.
Uniqueness follows from strict convexity since K~y is a convex set.
The bound (3.3) is a consequence of Lemma 12 and the compactness of the
level sets. The second bound (3.4) follows from the same argument since f
grows faster than linearly in µ. Finally, (3.5) is immediate from the definition
of K~y.
Finally, we prove continuity of µ∗, which in turn implies continuity of f∗.
Fix a point ~y. First we claim that, given ε > 0 there exists a δ(ε) = O(ε) so
that ~yi ∈ Cx together with |~yi − ~y| < δ(ε) implies
~yi = (1− ε1)~y +
∑
j
aij~ej for some ε1 < ε and 0 ≤ aij ≤ ε2 < ε. (3.6)
For a proof assume the contrary. Then there are ~y and ε > 0 and a sequence
~yi → ~y for which (3.6) does not hold. Let µ∗(~yi) denote the minimizing points
for f over Kyi , so that
~yi =
∑
j
µ∗j (yi)~ej .
By (3.3), the collection {µ∗(~yi)} is bounded. So take a converging subsequence,
and denote the limit µ∗. By taking further subsequences we may assume that
12
for each j, the sequence µ∗j (~yi) is monotone in i. So fix 0 < ε1 < minj µ
∗
j and
note that
~yi =
∑
j
µ∗j (~yi)~ej (3.7)
=
∑
j:µ∗j>0
µ∗j~ej +
∑
j
(
µ∗j (~yi)− µ∗j
)
~ej (3.8)
= (1− ε1)
∑
j:µ∗j>0
µ∗j~ej +
∑
j
(
µ∗j (~yi) − µ∗j + ε1µ∗j
)
~ej . (3.9)
By definition of ε1, the coefficients in the first summand are positive. Consider
the second summand. If µ∗j > 0 then, for all large i, we have 0 < (µ
∗
j (~yi)−µ∗j +
ε1µ
∗
j ) < ε1 maxj µ
∗
j . If µ
∗
j = 0 then necessarily µ
∗(~yi) is monotone decreasing
to zero and we have
0 < µ∗j (~yi) − µ∗j + ε1µ∗j = µ∗j (~yi)→ 0.
Thus, an approximation as in (3.6) holds, and the claim is established.
To complete the proof of continuity, we retain the preceding notation. Let
µ∗(~y) be the (unique) minimizing point at ~y: we need to establish that µ∗ =
µ∗(~y). But if not, then using (3.6), set ~ai =
∑
j aij~ej , and we have
f(µ∗(~yi), λ) ≤ f(µ∗(~y)(1− εi) + ~ai, λ) (3.10)
≤ f(µ∗(~y), λ) + ηi (3.11)
by continuity of f , and where ηi → 0 as i → ∞. But since µ∗ is not the
minimum at ~y,
f(µ∗(~yi), λ) ≤ f(µ∗(~y), λ) + ηi ≤ f(µ∗, λ) + ηi − γ
so that for all large i, f(µ∗(~yi), λ) ≤ f(µ∗, λ)− γ/2. But this is a contradiction
since by the continuity of f , f(µ∗(~yi), λ)→ f(µ∗, λ).
We now establish that I[0,T ], as defined in (2.6), is a good rate function.
Theorem 14 [13, Prop. 5.49 and Cor. 5.50] Assume the λi(~x) are bounded
and continuous. Then, for each ~x, I[0,T ](·) is a good rate function under either
the sup norm or the Skorohod J1 metric.
Proof. Identical to the proof in [13]; the additional assumption (that the
logλi are bounded) is not used in the proof (the original theorem was stated
with unnecessary conditions).
Lemma 15 [Uniform absolute continuity: [13, Lemma 5.18]] Assume the λi(~x)
are bounded, let I[0,T ](~r ) ≤ K, and fix some ε > 0. Then there is a δ, indepen-
dent of ~r , such that for any collection of nonoverlapping intervals
{[tj, sj], j = 1, . . . , J} with
J∑
j=1
sj − tj = δ,
13
in [0, T ] with total length δ, we have
J∑
j=1
|~r (tj)− ~r (sj)| < ε.
Lemma 16 Assume λi(~x) ≤ λ for all i and define Bi as in Assumption 1.
Then for any T > 0, K > 0 there is a J such that if I[0,T ](~r ) ≤ K then there
are 0 = t0 < t1 < . . . < tJ = T and ji so that ~r (t) ∈ Bji , ti−1 ≤ t ≤ ti.
Proof. We first claim that there is an α > 0 such that for any ~x ∈ G there
is an i such that B(~x, α) ⊂ Bi. This is easily proved by contradiction; if false,
then there is a sequence of points ~xj → ~x with diminishing open balls; but the
point ~x is contained in the interior of some Bj .
Now for any path ~r (t) with I[0,T ](~r ) ≤ K, we break up the path according
to the following rules. We initially assign the ball Bi to the path at time 0
if B(~r (0), α) ⊂ Bi (we may take any Bi that satisfies this constraint). We
maintain the choice i until such time that B(~r (t), α
2
) 6⊂ Bi. At this time,
change to any Bj with B(~r (0), α) ⊂ Bj . By the uniform absolute continuity of
~r , Lemma 15, there is a minimum time τ between any change in balls Bi. So
J = Tτ suffices as a bound on the number of pieces. Note that at any switchover
time t between balls Bi and Bj we have both ~r (t) +
α
4
~vi and ~r (t) +
α
4
~vj are
contained in Bi ∩Bj .
4 Upper Bound—Boundary case
We prove the upper bound (2.7) along the same general lines as [13, Theorem
5.54]. Here is a rough sketch of the main idea of the proof, ignoring technicalities
(some of which are commented on below). As in [13] we show in Lemma 25 that
it suffices to estimate the probability that an approximation ~yn of ~zn (defined
below) lies in a compact set of paths K. Paths in the set K have a cost I larger
than I[0,T ](C); here C is the closed set of paths appearing in the statement of
the large deviations principle, Theorem 3. We stitch together local estimates,
over short segments of time, of the probability that ~yn follows a segment of a
path in K. These estimates are obtained by approximating the process with a
constant-coefficient one, essentially reducing the problem to estimates for ran-
dom variables; see [13, Lemma 5.61].
There are two additional difficulties to overcome in our setting, both tech-
nical. The first is that the previous proof used the fact that shifting a path
~r (t) to ~r (t) + δ~vi resulted in a continuous change in the cost I[0,T ](~r ). This
is not true in the present case. A shift may result in a path being outside the
set G, thus having infinite cost. Another difficulty is that the approximating
functional `δ , used in the proof of the upper bound, is difficult to estimate in
the present case. Our solution is to make approximating functionals `∆ and
`m that are finite for all absolutely continuous paths in G. We then need to
14
calculate how well these functionals approximate `, and to show that the requi-
site bounds obtain. The main technical estimate we need is that every path ~r ,
having approximating cost I∆(~r ), has a path ~r 1 at a distance no more than ε,
with true cost I(~r 1) ≈ I∆(~r ).
Although we shall use Assumptions 1–2, in fact for the upper bound we do
not need Lipschitz continuity of the rates. For our proof it suffices that the
rates are absolutely continuous. Recall that a function f(x) (possibly from one
Euclidean space to another) is absolutely continuous if and only if there exists
ε(δ) → 0 as δ → 0 so that
I∑
i=1
|xi − yi| < δ implies
I∑
i=1
|f(xi) − f(yi)| < ε(δ).
It follows immediately that Lipschitz functions are absolutely continuous, and
that the composition of absolutely continuous functions yields an absolutely
continuous function. To emphasize this fact that a Lipschitz property is not
required, we denote by Kλ the modulus of continuity of the rates. Recall that
for Kλ(δ) to be the modulus of continuity of all λi(~x) means |λi(~x) − λi(~y)| ≤
Kλ(|~x− ~y|), where Kλ(δ) is continuous and Kλ(0) = 0.
Lemma 17 Under assumptions 1 and 2.C there is a constant C such that for
every x ∈ ∂G and x ∈ Bi, i ≤ I1, and every η small enough, the cost of the
linear path ~r (t)
4
= x + t~vi, 0 ≤ t ≤ η is less than Cη. In other words, with
T = η,
I[0,T ](~r ) ≤ Cη. (4.1)
Proof. By Assumption 2.C, we can find a set of ~ei and ai with
∑
i ai~ei = ~v
and with λi(~r (t)) > γ/2 > 0 for all t ≤ η. The ai are bounded since ~v is, by
Lemma 12. So, by the representation of ` in Lemma 11, taking the µi = ai and
λi as given yields a bound on `.
The proof of the upper bound of [13] uses an “approximate” rate function
δ
I :
here we require several different approximations. The functional I∆ is defined
via the function g∆ as follows:
g∆(~x, ~θ)
4
=
k∑
i=1
sup
~zi∈Bδ(~x)
[
λi(~zi)
(
e〈~θ,~ei〉 − 1
)]
(4.2)
`∆(~x, ~y)
4
= sup
~θ∈Rd
(
〈~θ, ~y〉 − g∆(~x, ~θ)
)
(4.3)
and I∆[0,T ](~r ) is now defined like I[0,T ](~r ), with `
∆ replacing `. The functional
δ
I was defined in a similar manner [13, Defs. 5.36–38], but starting with
gδ(~x, ~θ)
4
= sup
~z∈Bδ(~x)
k∑
i=1
λi(~z)
(
e〈~θ,~ei〉 − 1
)
. (4.4)
15
The difference between gδ and g∆ is that the supremum and sum are inter-
changed. In g∆ there are many different ~zi where each maximum is attained;
in gδ there is only one ~z. We are abusing notation a bit here; the terms labeled
with a ∆ are defined in terms of a parameter δ. Of course, we are suppos-
ing that the value of ∆ is δ, but the difference in notation should make clear
what we mean. Clearly we have g∆(~x, ~θ) ≥ gδ(~x, ~θ) for every δ, ~x, and ~θ, so
`∆(~x, ~y) ≤ `δ(~x, ~y) and I∆[0,T ](~r ) ≤
δ
I [0,T ] (~r ). We could have avoided the use
of
δ
I entirely in this paper, except that there are many bounds available for it
already, so instead of having to prove all these bounds for I∆ separately, we will
use them in conjunction with the simple relationship between
δ
I and I∆.
The advantage of using I∆ instead of
δ
I is that I∆ has a representation as
a change of measure as follows. Note that since λi is continuous and since the
supremum is taken for each i separately, g∆(~x, ~θ) is the supremum over a convex
set of {λi} of a function which is linear in the λi. Therefore
`∆(~x, ~y)
4
= sup
~θ∈Rd
(
〈~θ, ~y〉 −
k∑
i=1
sup
~zi∈Bδ(~x)
λi(~zi)
(
e〈~θ,~ei〉 − 1
))
(4.5)
= sup
~θ∈Rd
inf
~zi∈Bδ(~x)
(
〈~θ, ~y〉 −
k∑
i=1
λi(~zi)
(
e〈~θ,~ei〉 − 1
))
(4.6)
= inf
~zi∈Bδ(~x)
sup
~θ∈Rd
(
〈~θ, ~y〉 −
k∑
i=1
λi(~zi)
(
e〈~θ,~ei〉 − 1
))
(4.7)
= inf
~zi∈Bδ(~x)
inf
µ∈Ky
k∑
i=1
(
λi(~zi) − µi + µi log µi
λi(~zi)
)
(4.8)
=
k∑
i=1
(
λi(~z
∗
i )− µ∗i + µ∗i log
µ∗i
λi(~z∗i )
)
, (4.9)
where the ~z∗i ∈ Bδ(~x). Equation (4.5) is by definition. Equation (4.7) follows
from the min-max saddle point theorem, since the function in the definition of
`∆(~x, ~y) is concave in ~θ and linear, hence convex in the λi; furthermore, the
infimum is taken over a bounded convex set of λi. Equation (4.8) follows by
taking a minimizing ~zi, and Equation (4.9) follows from the representation result
Lemma 11 for `(~x, ~y).
For technical complexity, but to make easy proofs, we also define a function
`m as follows. Define ~λm by
λmj (~x)
4
= max{λj(~x), 1/m}. (4.10)
Define `m and Im through (2.5)–(2.6) but using the modified rates λmj (~x). To
complete this sequence of definitions, we define the set of cheap paths in any of
16
the metrics:
Φ~x(K)
4
= {~r (t) : ~r (0) = ~x, I[0,T ](~r ) ≤ K} (4.11)
Φ∆~x (K)
4
= {~r (t) : ~r (0) = ~x, I∆[0,T ](~r ) ≤ K} (4.12)
Φm~x (K)
4
= {~r (t) : ~r (0) = ~x, Im[0,T ](~r ) ≤ K}. (4.13)
Lemma 18 Let Assumptions 1–2 hold. Fix i. For each T > 0, K > 0 and
0 < ε < K there is an 0 < η0 < ε such that for any 0 < η < η0 there is a m0
with the following property. If m > m0 and if the path ~r (t) takes values in Bi
with Im[0,T ](~r ) < K− ε, then the path ~r 2(t)
4
= ~r (t) +η~vi satisfies I[0,T ](~r 2) < K.
Proof. Let ~v = ~vi,
`1(t) = `
m(~r (t), ~r ′(t)) (4.14)
`2(t) = `(~r (t) + η~v, ~r
′(t)) (4.15)
λ1i (t) = λ
m
i (~r (t)) (4.16)
λ2i (t) = λi(~r (t) + η~v) (4.17)
µ∗i (t) = µ
m,∗
i (~r (t), ~r
′(t)) (4.18)
where µm,∗ is optimal (see Lemma 13) for jump rates λmi . We denote by λ
1(t)
the vector with coordinates λ1i (t) and similarly for λ
2 and µ∗. By Lemma 11,
`2(t) ≤ f
(
µ∗(t), λ2i (t)
)
.
Choose m0 > 1/η. Then by continuity of λi
|λ2i (t) − λ1i (t)| ≤
1
m
+ |λ2i (t)− λi(~r (t))| ≤ Kλ(η) + η 4= K ′λ(η).
Therefore
`2(t) − `1(t) ≤
k∑
i=1
λ2i (t)− λ1i (t) + µ∗i (t) log
λ1i (t)
λ2i (t)
(4.19)
≤ k ·K ′λ(η) +
k∑
i=1
µ∗i (t) log
λ1i (t)
λ2i (t)
. (4.20)
By Assumption 2.C there exists a γ > 0 such that if λi(~x) < γ then λi(~x+η~v) ≥
λi(~x). Increase m0 so that (recall s(η) was defined in Lemma 10)
m0 > max{1/γ, 1/s(η)}.
For the rest of the proof let m > m0. Then if λi(~r (t)) < γ we have λ
1
i (t) < γ
and λ2i (t) ≥ λ1i (t). Therefore if λi(~r (t)) < γ then logλ1i (t)/λ2i (t) < 0. If however
17
λi(~r (t)) > γ then λi(~r (t)) = λ
1
i (t), and so, with ~x = ~r (t),
log
λ1i (t)
λ2i (t)
= log
λi(~x)
λi(~x+ η~v)
(4.21)
≤ log λi(~x)
λi(~x) −Kλ(η) (4.22)
≤ log γ
γ −Kλ(η) (4.23)
= log
1
1− Kλ(η)γ
(4.24)
≤ 2Kλ(η)
γ
(4.25)
for Kλ(η) <
γ
2
, since log 1
1−x < 2x for 0 < x ≤ 1/2. By [13, Lemma 5.17]
and Lemma 13 there exists constants C1, C3, B1 such that if |~y| > B1 then
`(~x, ~y) ≥ C3|~y| log |~y| and and |µ∗| ≤ C1|~y|. So if |~r ′(t)| ≥ B1 then using (4.19)–
(4.25)
`2(t) − `1(t) ≤ kK ′λ(η) + C1|~r ′(t)|
2Kλ(η)
γ
.
But by the estimate above, if |~r ′(t)| > B1 then |~r ′(t)| < `1(t)C3 log |~r ′(t)| . Thus if
|~r ′(t)| > B1 we get
`2(t) − `1(t) ≤ kK ′λ(η) + C
`1(t)
C3 log |~r ′(t)|
2Kλ(η)
γ
(4.26)
≤ kK ′λ(η) + C
`1(t)
C3 logB1
2Kλ(η)
γ
. (4.27)
Now consider the case |~r ′(t)| ≤ B1. By Lemma 13, |µ∗(t)| ≤ KµB1 for some
Kµ. Therefore
`2(t)− `1(t) ≤ kK ′λ(η) + KµB1
2Kλ(η)
γ
.
Putting together the estimates for large and small values of ~r ′(t) we obtain
`2(t) − `1(t) ≤
(
kK ′λ(η) +KµB1
2Kλ(η)
γ
+ C
`1(t)
C1 logB1
2Kλ(η)
γ
)
. (4.28)
Therefore if Im[0,T ](~r ) ≤ K − ε then
I[0,T ](~r + η~v) ≤ K − ε + T
(
kK ′λ(η) +KµB1
2Kλ(η)
γ
+
C(K − ε)2Kλ(η)
TC1 logB1γ
)
.
(4.29)
Thus for small enough η, I[0,T ](~r + η~v) ≤ K.
18
Lemma 19 Assume conditions 1–2. Let A be a compact set in G. For each
T > 0, K > 0 and 0 < ε < K there is an 0 < η0 < ε such that for any
0 < η < η0 there is a m0 with the following property. For any x ∈ A, m > m0,
and any path ~r (t) with ~r (0) = ~x and Im[0,T ](~r ) < K − ε, there is a path ~r 2(t)
with ~r (0) = ~x with I[0,T ](~r 2) < K and d(~r , ~r 2) ≤ ε.
This lemma corresponds to [13, Lemma 5.48]. It is based on a direct construc-
tion, having the path ~r 2 composed of a number of segments. The main segments
are made of shifted pieces of ~r , using Lemma 18 to estimate the costs of the
segments, with the initial part of the path estimated by Lemma 17, and the
segments stitched together with the aid of Lemma 17 as well.
Proof. Given ~r (t) with ~r (0) = ~x and Im[0,T ](~r ) < K − ε, take J as defined in
Lemma 16. Recall from Assumption 1.(iii), there is a number β > 0 such that
for any ~x ∈ Bi where Bi is one of the boundary neighborhoods, then for small
ε we have d(~x+ ε~vi, ∂G) ≥ βε. Clearly β ≤ 1.
We now construct the path ~r 2 from ~r using a parameter η that will be
chosen later. Let 0 = t0, t1, . . . , tJ represent the switchover times of ~r (t), as
in Lemma 16. Recall the definition of α from the proof of Lemma 16. If
B(~r (0), α) ⊂ Bi where Bi is one of the boundary neighborhoods (i ≤ I1, cf.
Assumption 1.(i)), then take ~r 2(t) = ~r (0)+~vit for 0 ≤ t ≤ η. For η ≤ t ≤ t1 +η
let ~r 2(t) = ~r (t − η) + η~vi. Then for t1 + η ≤ t ≤ t1 + η + 3η/β let
~r 2(t) = ~r (t1) + η~vi + (t − t1 − η)~vj; (4.30)
here j is the index of the neighborhood Bj that ~r (t) switches to at time t1, as
defined in Lemma 16. Then from time t1 + η + 3η/β until time t2 + η + 3η/β
we let ~r 2(t) = ~r (t − η − 3ηβ) + η~vi + (3η/β)~vj . We continue in this fashion,
with switchover k having ~r 2(t) following a linear path of length η(3/β)
k−1 in
direction ~vk, followed by a segment parallel to ~r (t). Note that we need not
include the paths of length η(3/β)k−1 in direction ~vk if Bk is not a boundary
neighborhood.
There are three things to check about this path. First, if η is small enough,
does ~r 2(t) stay within ε of ~r (t)? Second, does ~r 2 remain in G? Third, does it
satisfy I[0,T ](~r 2) ≤ K? If all three items hold, then the lemma will be proved.
These estimates are hardest if all the paths of type η(3/β)k−1 in direction ~vk
are included, so we assume without loss of generality that they are.
The first thing is easy to verify. Since Im[0,T ](~r ) < K − ε by assumption,
~r (t) is uniformly absolutely continuous, so by choosing η small enough, we can
ensure that the time shifts introduced in the definition of ~r 2 don’t cause the
paths to differ by more than ε/2. Furthermore, the number of segments J is
bounded, so the difference introduced in the segments η(3/β)k adds up to less
than ε/2 if η is chosen small enough.
For the second point, this is the reason we chose 3/β as a multiplier. By defi-
nition of β, the point ~r 2 +η(3/β)
k~vk is at least βη(3/β)
k from ∂G. Since 3/β >
3, the sum of all the previous shifts has total length less than (ηβ/2)(3/β)k.
19
Therefore the point is at least (ηβ/2)(3/β)k from ∂G when the kth shift is fin-
ished. The beginning of the shift also occurs in the interior of G by construction,
whenever the total shifts are of length less than α/4. This demonstrates that
~r 2 ∈ G when η is small enough.
For the third point, we assume that η has been chosen small enough to
satisfy the first two points, and note that, after the initial time η, the path
~r 2(t) remains at least βη/2 away from ∂G. By Lemma 17, there is a uniform
constant C ′ such that for 0 ≤ t ≤ η
I[0,t](~r 2) ≤ C ′t. (4.31)
We use this estimate on every η(3/β)k path; the total cost is linear in η, so
can be made less than ε/2. Lemma 18 enables us to bound the cost of each
other segment of ~r 2 uniformly as no more than ε1(η) plus the I
m-cost of the
corresponding segment ~r , where ε1(η) goes to 0 with η. Choose η small enough
so that ε1(η) ≤ ε/2J . Then choose m large enough that Lemma 18 applies.
Then the total additional cost is bounded by ε/2 for the segments of ~r 2. This
concludes the estimate, and hence the proof.
Lemma 20 Under the assumption of bounded continuous λi, given ε > 0 there
exists an m0 > 0 such that for all positive m > m0 there exists a δ0 > 0 such
that for all δ < δ0,
`m(~x, ~y) ≤ ε+ (1 + ε)`∆(~x, ~y). (4.32)
Proof. We consider separately the cases |~y| > B and |~y| ≤ B. Here we take
B so that `(~x, ~y) > CB logB for all ~x; this is possible by [13, Lemma 5.17].
If |~y| > B then, using (4.9) and the reasoning leading to Equation (4.25),
`m(~x, ~y) − `∆(~x, ~y) ≤ Kλ(δ)
(
k +
2C`∆(~x, ~y)
γC1 logB
)
(4.33)
Therefore, by choosing δ small enough, we can make the right-hand side of (4.33)
smaller than ε/3.
Now if |~y| ≤ B, take µ as the optimal jump rate for λ∆(~x). We have µi ≤ CB
for all i. Then
`m(~x, ~y)− `∆(~x, ~y) ≤
∑
i
λmi (~x)− λ∆i (~x) + µi log
λ∆i (~x)
λmi (~x)
(4.34)
≤ k(Kλ(δ) + 1/m) +
∑
i
µi log
λ∆i (~x)
λmi (~x)
. (4.35)
If λi(~x) ≤ 1/m−Kλ(δ) then λ∆i (~x) ≤ 1/m, so, since λmi (~x) ≥ 1/m for each ~x,
log
λ∆i (~x)
λmi (~x)
≤ 0. Furthermore, if λi(~x) > 1/m−Kλ(δ), then
log
λ∆i (~x)
λmi (~x)
≤ log 1/m+ Kλ(δ)
1/m
≤ mKλ(δ). (4.36)
20
Hence, by choosing δ small enough that kmKλ(δ) < ε/3, we obtain, for |~y| ≤ B,
`m(~x, ~y)− `∆(~x, ~y) < ε/3. Combined with the result of the previous paragraph,
this finishes the proof.
Corollary 21 Assume the λi are bounded and continuous. Given ε, K, and
T > 0 there exists an m0 > 0 such that for all positive m > m0 there exists a
δ0 > 0 so that δ < δ0 and I
∆
[0,T ](~r ) ≤ K − ε imply Im[0,T ](~r ) ≤ I∆[0,T ](r) + ε.
Corollary 21 and Lemma 19 combine to give the following:
Corollary 22 Assume 1–2. Given K > 0 and ε > 0, there exists a δ > 0 such
that
Φ∆~x (K − ε) ⊂ {~r : d(~r ,Φ~x(K)) ≤ ε}. (4.37)
The proof of this corollary is immediate, by using ε/2 to replace ε, and choosing
a large enough m. (Specifically, if ~r ∈ Φ∆~x (K − ε) then, by Corollary 21,
~r ∈ Φm~x (K − ε/2). By Lemma 19, there is an ~r 2 within ε/2 of ~r satisfying
I[0,T ](~r 2) ≤ K.)
Corollary 22 shows that, by choosing δ and η small enough,
d(~r ,Φ~y(K − 4ε)) > η/2 implies d(~r ,Φ∆~y (K − 4ε− η/4)) > η/4. (4.38)
We now state two technical lemmas in measure theory that are used in the
proof of Lemma 25. Lemma 23 is used only for the proof of Lemma 24. We let
Leb(A) denote the Lebesgue measure of a set A.
Lemma 23 Let u(t) be a nonnegative, absolutely continuous function on [0, T ].
Then given δ > 0 there exists an η > 0, a set A ⊂ [0, T ], and a finite collection
{Ci} of intervals so that Leb(A) < δ and, for each i, either inf{u(t) : t ∈ Ci > η}
or u(t) = 0 for all t ∈ Ci \A.
Proof. If u(0) > 0 set t1
4
= inf{t > 0 : u(t) = 0}. Then by continuity
inf{u(t) : 0 ≤ t ≤ t1 − δ/2} > 0, and so it suffices to establish the result when
u(0) = 0. By a similar argument we may assume u(T ) = 0.
Given t, if u(t) > 0 then by continuity there exists an open intervalOt containing
t so that u(s) ∈ Ot for all s ∈ Ot and u(s)→ 0 as s→ ∂Ot. Let mt 4= sup{u(s) :
s ∈ Ot}. Since u is absolutely continuous, there is a finite number of disjoint
open intervals with mt > 1/m. Therefore there exists a countable collection
{Oi} of disjoint open intervals so that u(t) > 0 if and only if t ∈ Oi for some i.
Fix N large so that
Leb
{∪∞N+1Oi} < δ2 . (4.39)
For i ≤ N let C˜i ⊂ Oi be a closed interval such that
Leb{Oi \ C˜i} ≤ δ
2N + 1
.
21
Let Ci be the finite collection of closed intervals that cover [0, T ] \ ∪iC˜i and let
A
4
=
{∪∞N+1Oi} ∪ {∪Ni=1Oi \ C˜i} .
Then by construction, Leb(A) < δ and u(t) = 0 on t ∈ Ci \ A. Since there are
only a finite number of C˜i, we obtain by continuity inf{u(t) : t ∈ C˜i for some i} >
η > 0.
For vectors ~θ,~λ and ~y in Rd define
`(~θ,~λ, ~y)
4
=
〈~θ, ~y〉 −∑
j
λj
(
e〈~θ,~ej〉 − 1
) . (4.40)
Our next result extends [13, Lemma 5.43] to the case of rates which are not
bounded below, but under the assumption that they are absolutely continuous.
Lemma 24 Assume that the λj(~x) are bounded and absolutely continuous. Then
for any ~r with I[0,T ](~r ) < ∞ and any ε > 0 there exists a step function ~θ so
that ∫ T
0
`(~θ(t), ~λ(~r (t)), ~r ′(t)) dt ≥ I[0,T ](~r )− ε.
Proof. Since by definition
`(~θ(t), ~λ(~r (t)), ~r ′(t)) ≤ `(~r (t), ~r ′(t)),
it suffices to establish the result outside a set of small measure: this approxi-
mation and the extension of the step function over this set are derived in the
proof of [13, Lemma 5.43].
We claim that given δ, there is a partition of [0, T ] into a finite collection of
intervals Ci and a set A with Leb(A) < δ so that the lemma holds on each Ci\A.
Then the result follows by patching together the step function and dealing with
A as above. In the rest of the proof we establish this claim.
Fix j and apply Lemma 23 to the absolutely continuous function λj(~r (t))
with δ/k. We then obtain a collection {Cji , Aj , 1 ≤ j ≤ k, 1 ≤ i ≤ N} so
that either λj(~r (t)) = 0 for all t ∈ Cji \Aj or λj(~r (t)) > ηj > 0 for all t ∈ Cji .
Set A
4
= ∪jAj and η = minj ηj . Then Leb(A) < δ and η > 0. Fix a subset
α ⊂ {1, . . . , k}. Using intersections of the sets Cji we obtain a finite collection
of intervals Cαi with the following properties:
λj(~r (t)) = 0, t ∈ Cαi \A for all j ∈ α (4.41)
λj(~r (t)) > η, t ∈ Cαi for all j 6∈ α. (4.42)
In particular, λj(~r (t)) > η for t ∈ C∅i . Now fix i and α and consider the process
on Cαi with rates and jump directions {λj(~x), ~ej , j 6∈ α}. For this process, the
22
assumptions of [13, Lemma 5.43] hold. Moreover, if we denote the local rate
function for this process by `α then, by definition, `α(~r (t), ~r ′(t)) = `(~r (t), ~r ′(t))
for t ∈ Cαi \A. Thus our claim is established and the proof is concluded.
Now we state and prove that the analogue of [13, Proposition 5.62] holds for
I∆. By [13, Lemma 5.61], under our continuity assumptions, if C is a compact
set in Rd, and ~θ(t) is a fixed step function then for any δ > 0 and compact set
K ⊂ K(M ) of functions ~r (t) with ~r (0) ∈ C,
lim sup
n→∞
1
n
logP~x(~yn ∈ K~x) ≤ − inf
~r ∈K~x
δ
I [0,T ] (~r , ~θ)
where K~x 4= {~r ∈ K : ~r (0) = ~x}. Note that I∆ >
δ
I ; therefore, we have, under
the same assumptions, that
lim sup
n→∞
1
n
logP~x(~yn ∈ K~x) ≤ − inf
~r ∈K~x
I∆[0,T ](~r ,
~θ). (4.43)
Lemma 25 Assume the λi(~x) are bounded and absolutely continuous, and C is
a compact set in Rd. Then for each K > 0, δ > 0, and ε > 0,
lim sup
n→∞
1
n
logP~x
(
d
(
~yn,Φ
∆
~x (K)
)
> ε
) ≤ −(K − ε)
uniformly in ~x ∈ C.
The proof of this lemma follows exactly the proof of Lemma 5.62 as presented
in the book, with the following two exceptions. First, the step function ~θ(t) is
the one defined in Lemma 24. Second, every superscript δ is replaced by the
corresponding ∆. This is obvious throughout the proof, with the help of (4.43).
We now state and prove the large deviations upper bound for our process.
Recall that for a set F of paths, I~x(F ) = inf(I[0,T ](~r ) : ~r (0) = ~x, ~r ∈ F ).
Theorem 26 Assume conditions 1–2. Let F be a closed set in (Dd[0, T ], dd),
and let ~x be a point in G. Then
lim
~y→~x
lim sup
n→∞
1
n
logP~y(~zn ∈ F ) ≤ −I~x(F ) (4.44)
where the points ~y are in F ∩G.
Proof. Fix a closed set F , and let K = I~x(F ). Suppose for now that K <
∞; the same proof will work for K = ∞, but some arguments need minor
modifications. By [13, Lemma 5.63], for given ε there is a δ so that if |x−y| < δ,
I~y(F ) ≥ K − ε.
23
Let
C = ∪y∈Bδ(~x)∩G {Φ~y(K − 4ε)} (4.45)
Fδ = {~r ∈ F : |~r (0)− ~x| ≤ δ}. (4.46)
Then C is a compact set. Note that C ∩ Fδ = ∅. By [13, Theorem A.19] there
is a number η > 0 such that d(C,Fδ) = η.
Define the random path ~yn(t) as the linear interpolation of ~zn(t) with time
spacing T/n; that is, at times jT/n for j = 0, . . . , n, ~yn(t) = ~zn(t), and ~yn(t)
is linear in between these points. [13, Lemma 5.57] states that, for each ε > 0,
uniformly in ~x in bounded sets,
lim sup
n→∞
1
n
logP~x(d(~zn, ~yn) > ε) = −∞. (4.47)
Now
P~y(~zn ∈ F ) = P~y(~zn ∈ Fδ) (4.48)
≤ P~y(d(~yn, Fδ) < η/2) + P~y(d(~yn, ~zn) ≥ η/2). (4.49)
Equation (4.47) shows that the second term on the right-hand side of this in-
equality is negligible.
If ~r (0) = ~y, then by definition of η,
d(~r , Fδ) < η/2 implies d(~r ,Φ~y(K − 4ε)) > η/2 (4.50)
Therefore, by equation (4.38) and Lemma 25,
lim sup
n→∞
1
n
logP~y(d(~yn, Fδ) < η/2) (4.51)
≤ lim sup
n→∞
1
n
logP~y
(
d
(
~yn,Φ
∆(K − 4ε− η/4)) ≥ η/4) (4.52)
≤ −(K − 4ε− η/4) (4.53)
uniformly in ~y in a compact set. Therefore
lim sup
n→∞
1
n
logP~y(~zn ∈ F ) ≤ −(K − 4ε− η/4)
whenever |~y− ~x| ≤ δ. Since ε and η can be made arbitrarily small, the theorem
is proved.
5 Lower Bound—Boundary Case
Our approach to the proof of the lower bound, Equation (2.8), is mainly stan-
dard. Every path ~r in an open set O can be surrounded by a “sausage,” a
neighborhood of ~r , that is entirely contained in O. If we can show that the
24
probability that ~zn lies in this sausage is about exp(−nI[0,T ](~r )), then the lower
bound will be proved, for if we take a sequence of ~r whose I functions are
approximately minimal in O, then we find that the probability that ~zn ∈ O
is at least the probability that ~zn ∈ the sausage around ~r , which is about
exp(−nI[0,T ](~r )).
The novelty in the proof is a twofold estimate. The first step is to show
that, for any path ~r that lies entirely in a single neighborhood Bi (see Assump-
tion 1.(i)), the probability that ~zn is near ~r is approximately exp(−nI[0,T ](~r )).
This is done by showing that the path ~r + δ~vi, which lies strictly away from
the boundary, has rate function I(~r + δ~vi) ≈ I(~r ). Then, since the λj(~x) are
bounded away from zero on this path, existing lower bound theory shows that
the probability of ~zn being near this new path is at least exp(−nI(~r + δ~vi)).
Using I(~r +δ~vi) ≈ I(~r ) proves the result for such paths ~r . The second step is to
show that every path ~r with finite cost I[0,T ](~r ) can be decomposed into a finite
number of pieces ~r j, each of which lies entirely within a ball Bj , and that the
endpoints of the shifted pieces ~r j + δ~vj can be connected with asymptotically
negligible cost. This is mostly the same as Lemma 19.
Lemma 27 Let Assumptions 1–2 hold. Suppose ~r (t) is a path contained in a
single Bi with I[0,T ](~r ) < ∞. Let ~r δ(t) = ~r (t) + δ~vi, where ~vi is the direction
in Assumption 1.(iii) for the region Bi. Then
lim sup
δ→0
I[0,T ](~r δ) ≤ I[0,T ](~r ). (5.1)
Proof. This is proved in exactly the same way as Lemma 18. Let `1(t) =
`(~r (t), ~r ′(t)) and `2(t) = `(~r δ(t), ~r ′δ(t)). Then, letting ~µ(t) be the optimizing
set of jump rates for the path ~r , we have the equivalent of (4.19):
`2(t)− `1(t) ≤ k ·K ′λ(η) +
k∑
i=1
µ∗i (t) log
λ1i (t)
λ2i (t)
.
The same reasoning as in Lemma 18 then leads to a bound like (4.28), which
immediately leads to the result.
Lemma 28 Assume conditions 1–2. Using the notation therein, fix i, T > 0
and a path ~r (t) which takes values in Bi such that I[0,T ](~r ) = K < ∞. Then
(with ~r (0) = ~x)
lim
δ↓0
lim inf
n→∞
1
n
logPx (~zn(t) ∈ B(~r , δ)) ≥ −I[0,T ](~r ). (5.2)
Proof. Let ~v = ~vi be the direction of the interior cone (see Assumptions 1–2)
of Bi and let Ki be the constant such that for ~x ∈ Bi we have d(~x+ t~v, ∂G) ≥
Kit for t small. Denote by η˜ the modulus of continuity of ~r and set η(a) =
25
max{η˜(a), a} so that η−1(a) ≤ a.
Now fix δ and set tδ = η
−1 (δ/3). Then tδ ≤ δ and for t ≤ tδ ,
sup
0≤t≤tδ
|t · ~v − ~r (t)| ≤ tδ · |~v|+ η(tδ) ≤ 2δ
3
. (5.3)
Therefore, for 0 < α < Ki/6,
Px (~zn ∈ B(~r , δ)) ≥ Px (|~zn(t)− t · ~v| ≤ αδ, 0 ≤ t ≤ tδ, ~zn ∈ B(~r , δ)) (5.4)
where the last ball is around the restriction of ~r to [tδ, T ]. Now let rδ(t)
4
=
~r (t) + tδ~v be a function on [tδ, T ]. Then on this time interval
sup
tδ≤t≤T
|~r (t)− ~r δ(t)| ≤ δ
3
and moreover, d(~r δ(t), ∂G) ≥ Kiδ. Therefore, for any function ~u on that time
interval, ‖~u−~r δ‖ ≤ Kiδ/2 implies that ‖~u−~r ‖ ≤ 2δ/3 and d(~u(t), ∂G) ≥ Kiδ/2.
Now let Bδ
4
= B(~x + tδ~v, αδ) and let ~r
y
δ be the shift of ~r δ so that ~r
y
δ (tδ) = ~y.
Then
Px (~zn ∈ B(~r , δ)) ≥ Px (|~zn(t) − t · ~v| ≤ αδ, 0 ≤ t ≤ tδ) (5.5)
× inf
y∈Bδ
Py (~zn ∈ B(~r yδ ,Kiδ/2))
where the last ball contains paths on [tδ, T ]. Now the first term is bounded below
by the probability that, over 0 ≤ t ≤ tδ, rates for jumps in directions outside the
cone of Assumption 2.C are zero while the rates for jump in directions within the
cone are such that the process proceeds with speed one. However, the second
condition satisfies a standard large deviations lower bound, and so
lim inf
n→∞
1
n
logPx (|~zn(t) − t · ~v| ≤ αδ, 0 ≤ t ≤ tδ) ≥ −Ctδ (5.6)
for some constant C. Consider now the second probability in (5.5). Since the
paths inB(~r yδ ,Kiδ/2) are bounded away from the boundary uniformly in ~y ∈ Bδ
we have by [13, Thm. 5.51] that the large deviations lower bound holds uniformly
over ~y ∈ Bδ (where uniformity is the usual sense of analysis). Therefore
lim inf
n→∞
1
n
log inf
y∈Bδ
Py (~zn ∈ B(~r yδ ,Kiδ/2)) (5.7)
≥ − inf
y∈Bδ
inf{I[tδ,T ](~w) : ~w ∈ B(~r yδ ,Kiδ/2) (5.8)
≥ −I[tδ ,T ](~r δ) (5.9)
≥ −I[0,T ](~r δ) (5.10)
≥ −I[0,T ](~r ), (5.11)
where the final inequality comes from Lemma 27. Thus the result follows
from (5.6)–(5.7).
26
Proof of the lower bound of Theorem 3. Using Lemma 28 the proof of
the lower bound is almost identical to that of Lemma 19, and so we only give a
sketch of the proof. Given a path ~r with I[0,T ](~r ) = K <∞ we break the path
into J segments and use the shift-and-stitch argument of Lemma 19. Lemma 28
shows that the shifted path provides a good approximation for the rate and
a lower bound for the probabilities, and that the probabilities of the shifts are
bounded below (on the exponential scale) by an arbitrarily small constant. This
establishes the lower bound.
6 Interior zeros
In this section we state and prove a large deviations principle for processes
that have rates that may become zero in the interior of the region G, not at a
boundary. Our main assumption about these processes is that the cone C~x of
jump directions of the process does not change, but remains Rd for all ~x. This
is a strong assumption. But the result is strong, too: under Lipschitz continuity
of the jump rates λi(~x), the large deviations principle holds, with the usual
rate function. This is, therefore, a strict generalization of the large deviations
principle proved in [13, Chapter 5], where the assumption is the logarithms of
the jump rates is bounded, which itself implies C~x = Rd for all ~x. Combined
with the results for rates that diminish towards a boundary, we obtain a fairly
general theory of diminishing rates, Corollary 5.
We state our theorem for Lipschitz continuous jump rates and processes
that have no boundaries. But, using the exponential tightness argument of
Corollary 8 we give proofs only for bounded regions and bounded Lipschitz
jump rates. Our proofs are based very heavily on the arguments in [13]. Rather
than reproduce those arguments, we give only the lemmas and arguments needed
to extend the previous proof to the present case.
We begin with some notation. For δ > 0 we define
λδi (~x)
4
=
{
λi(~x) if λi(~x) > δ
0 otherwise.
(6.1)
We define Cδ~x as the positive cone spanned by the ~ei whose corresponding λδi (~x) >
0. Let Iν(~x) = {i : λi(~x) > ν}, so that I0(~x) = {i : λi(~x) > 0}. Also, for a set I
of indices, we let C~x(I) be the positive cone spanned by {~ei : i ∈ I, λi(~x) > 0}.
Lemma 29 Assume that Assumption 3 holds, and that the λi(~x) are contin-
uous. Then for every R < ∞, there exists δ > 0 and ν > 0 such that for
|~x| ≤ R we have a set I(~x) with the following property. Every ~z ∈ B(~x, δ) has
λi(~z) > ν, i ∈ I(~x) and C~z(I(~x)) = Rd.
Note: we use this ν in following lemmas and proofs.
27
Proof. This follows easily from compactness. The set S, consisting of ~x such
that at least one λi(~x) = 0, is closed. Therefore, the set of such ~x that satisfy
|~x| ≤ R is compact. Cover each point ~x in S with an open ball with radius δ
chosen so that the minimal rate of λi(~z), i ∈ I0(~x), ~z ∈ B(~x, δ) is at least half the
minimal rate of λi(~x), i ∈ I0(~x). By assumption, C~x = Rd, so C~z(I0(~x)) = Rd
for all ~z ∈ B(~x, δ). Choose a finite subcover of such balls {B(~xj , δ)}. Call the
resulting union of these sets U . Then
inf
{
λi(~z) : ~z ∈ B(~xj , δj), i ∈ I0(~xj)
} 4
= η > 0
by construction. Then for ~x /∈ U , λi(~x) > 0. In fact, there is a positive bound,
which without loss of generality we take to be η such that λi(~x) ≥ η for all
~x : |~x| ≤ R,~x /∈ U , since this set is closed and the rates are continuous and not
equal to 0. Now set ν = η/2. The proof is concluded by showing that there is
a δ so that each ~x ∈ U ,
B(~x, δ) ∩ U ⊂ B(~xj , δj) for some j; set I(~x) = I(~xj) (6.2)
or d(~x, ∂U ) ≤ δ; set I(~x) = {1, . . . , J}. (6.3)
The first case, equation (6.2), obviously satisfies the statement of the lemma.
In the second case, (6.3), since the rates are continuous over a compact set,
they are uniformly continuous, and we choose δ so that the rates change by at
most η/2 over the δ ball. We establish the claim by contradiction; assume the
contrary. Then there is a sequence ~xi and δi ↓ 0 so that ~xi ∈ U and the ball
B(~xi, δi) is not contained in any B(~xj , δj). Take a converging subsequence with
limit ~x: then if ~x ∈ U we obtain a contradiction, since U is a finite union of
balls, so that, for large i, the ball around ~xi must be in some B(~xj , δj). If ~x is
on the boundary of U then for large i, ~xi is within δ of ∂U .
We define ~µ∗(~x, ~y) as the unique optimizer for f(~µ,~λ); that is, ~µ causes
`(~x, ~y) = f(~µ,~λ(~x)), with ~y =
∑
i µi~ei. We define ~µ
∗
δ (~x, ~y) to be the optimizer
for rates λδ . We also define `δ(~x, ~y) (not to be confused with `
δ!) to be the rate
function with jump rates λδ.
Lemma 30 Assume condition 3 and that the λi are continuous. The maximiz-
ing ~θ in the definition of ` is bounded uniformly in |~x| ≤ R and |~y| ≤ C.
Proof. The proof is similar to [13, Lemma 5.21]. By Lemma 29,
max{〈~θ,~ei〉 : i ∈ Iν(~x)} ≥ α|~θ|
for some α > 0 that depends only on R. Let ~θn be a maximizing sequence.
Representing ~y as in Lemma 12, and using Lemma 29,
`(~x, ~y) = lim
n→∞〈~θn, ~y〉 −
k∑
i=1
λi(~x)
(
e〈~θn ,~ei〉 − 1
)
(6.4)
≤ lim
n→∞ |~θn|C − νe
α|~θn| +
k∑
i=1
λi(~x). (6.5)
28
However, the last sum is bounded and the function ax+c−ex diverges to (−∞)
as x →∞. Since ` is non negative we conclude that |~θn| must be bounded for
large n, where the bound depends only on R,C, ν and Iν.
Corollary 31 Assume condition 3 and that the λi are continuous. For each B
there exists a C such that for all |~x| ≤ B and all |~y| ≤ B,
`(~x, ~y) ≤ C. (6.6)
Proof. [13, Theorem 5.26, and exercise 5.30] show that the optimizing µ∗ can
be represented as µ∗i = λie
〈~θ∗ ,~ei〉, where ~θ∗ is the optimizing ~θ in the definition
of `. Therefore, since Lemma 30 shows that ~θ∗ is bounded for bounded ~y, we
have µ∗i /λi is bounded for bounded ~y. Therefore, for optimizing µ
∗, there is a
constant u such that
λi − µ∗i + µ∗i log
µ∗i
λi
≤ λiu. (6.7)
Lemma 32 Assume condition 3 and that the λi(~x) are continuous. Then for
each B and ε > 0 there exists a δ > 0 such that for all |~y| ≤ B, ~xδ ∈ B(~x, δ),
~yδ ∈ B(~y, δ) we have
|`(~x, ~y) − `δ(~xδ, ~yδ)| < ε.
Proof. By Lemma 11,
`(~x, ~y)
4
= inf
µ∈Ky
(
k∑
i=1
λi(~x)− µi + µi log µi
λi(~x)
)
(6.8)
≤ inf
µ∈Ky
 ∑
i:λδi>0
λδi (~x) − µi + µi log
µi
λδi (~x)
 + ∑
i:λδi=0
λi (6.9)
≤ `δ(~x, ~y) + kδ (6.10)
where the first inequality holds since the infimum is taken over a smaller set—
where λδi = 0 implies µi = 0. We claim that to conclude the proof it suffices to
establish that
`δ(~xδ, ~yδ) ≤ `(~x, ~y) + ε, (6.11)
since using (6.8) and interchanging the roles of ~x, ~y with ~xδ, ~yδ in (6.11) and
then using (6.8) again we get
`δ(~xδ, ~yδ) ≥ `(~xδ, ~yδ)− kδ (6.12)
≥ `δ(~x, ~y) − kδ − ε (6.13)
≥ `(~xδ, ~yδ)− 2kδ − ε . (6.14)
29
The difficulty in demonstrating (6.11) is that the ~µ∗δ might be very different
from the ~µ∗. But we can bound this difference, using the property that C~x = Rd
for all ~x. Let Iν(~x) be the set of i with λi(~xδ) ≥ ν for all ~xδ near ~x; see
Lemma 29. Note that for each δ there is a uniformly bounded vector ~α with
αi = 0 for i /∈ Iν(~x), such that∑
i
(µ∗i + δαi)~ei = ~yδ . (6.15)
There may be some nonzero components of the µi for i /∈ Iν(~x). However,
by Lemma 13, since ~yδ is bounded, for any η > 0 we may choose δ small enough
that if any of the λδi = 0 then by continuity, λi < η. Take
~vδ =
∑
i/∈I
µ∗i~ei. (6.16)
Then ~vδ ∈ Cδ~z for all ~z ∈ B(~x, δ), so by Lemma 12 we may write
~vδ =
∑
i∈I
ai~ei (6.17)
for some ai ≥ 0, |ai| ≤ κδ. Therefore we may write
~yδ =
∑
i∈I
(µ∗i + ai + δαi)~ei. (6.18)
We have
`(~xδ, ~yδ)
≤
∑
i∈I
λi(~xδ)− µ∗i − ai − δαi + (µ∗i + ai + δαi) log
µ∗i + ai + δαi
λi(~xδ)
. (6.19)
Note that
`(~x, ~y) =
∑
i
λi(~x)− µ∗i + µ∗i log
µ∗i
λi
. (6.20)
Both ~α and ~a are of size δ, so the difference between corresponding terms in
the sums (6.19) and (6.20) can be bounded by terms that go to zero with δ.
Furthermore, by definition of I, λi(~z) ≥ ν for all ~z near ~x. This finishes the
estimates.
Proof of Theorem 4. The large deviations upper bound follows from these
arguments based on the argument in [13]. The key theorem there is [13, Theorem
5.64]. It is based on Lemmas 5.57 and 5.58; Lemma 5.63; Lemma 5.62; and
Lemma 5.48 there. Lemmas 5.57 and 5.58 require only bounded rates λi, and
Lemma 5.63 requires bounded continuous rates, so these three lemmas continue
30
to hold. Lemma 5.62 is based on Lemma 5.43, which required log-bounded rates,
but is established without the lower bound on the rates in our Lemma 24 which
requires only absolutely continuous jump rates λi. Finally, [13, Lemma 5.48] is
based on Lemma 5.35 there, which also requires log-bounded rates. Our new
Lemma 32 replaces Lemma 5.35. So, under the assumption that the λi(~x) are
bounded, absolutely continuous, and that C~x = Rd for all ~x, the large deviations
upper bound is proved.
The large deviations lower bound follows even more directly. Assuming that
the rates λi(~x) are bounded and Lipschitz continuous, so that Kurtz’s theo-
rem [13, Theorem 5.3] applies, essentially the same proof of the lower bound [13,
Theorem 5.51], goes through. The only place that log-boundedness is used is [13,
Corollary 5.53], and using Lemma 32 it is easily seen to hold without that as-
sumption. Using Lemma 32, we see that the rate function `(~x, ~y) is jointly
continuous in ~x and ~y over bounded regions. Approximating jump rates with
λδi makes a small (δ) difference in the rate function; this can be made arbitrarily
small. So, under the assumption that the λi(~x) are bounded, Lipschitz contin-
uous, and that C~x = Rd for all ~x, the large deviations lower bound is proved.
7 Reachability of the boundary
We now provide a simple condition for showing that a point ~x ∈ ∂G may be
reached with a finite cost (hence exponentially non-zero probability) via a path
from the interior of G. We state a sufficient condition that is far from necessary;
nevertheless, it is general enough to cover many cases of interest. For the one-
dimensional case, we prove that a similar condition is also necessary. Recall
that s(δ) is the scale function defined in Lemma 10.
Lemma 33 Assume that the rates λi are bounded and let Assumption 2.C hold.
Fix ~x ∈ ∂G. If ∫ δ
0
log
1
s(t)
dt <∞ (7.1)
for some δ > 0 then I[0,T ](~r ) <∞ for some ~r with ~r (T ) = ~x.
Proof. For convenience we shift time. Let ~r (t) be a path with the following
properties: ~r (0) = ~x; for some c > 0 and t0 < 0 we have d(~r (t), ∂G) > c|t| for
t0 ≤ t ≤ 0; and, for some C > 0,
∣∣~r ′(t)∣∣ < C for almost all t ∈ [t0, 0). Since∣∣~r ′(t)∣∣ < C for almost all t ∈ [t0, 0), by Lemma 13 µi ≤ C0, where µi(t) is the
optimal change of measure for `(~r (t), ~r ′(t)). Since we assume d(~r (t), ∂G) > c|t|
for t0 ≤ t ≤ 0, we have from (3.1)
`(~r (t), ~r ′(t)) ≤ C + C log 1
s(d(~r (t), ∂G))
, (7.2)
31
since both λi and µi are bounded. But d(~r (t), ∂G) > c|t|. Therefore,
`(~r (t), ~r ′(t)) ≤ C + C log 1
s(ct))
(7.3)
This proves the result.
Note that the condition for reachability holds for any polynomially decreas-
ing scale function, since
∫ 1
0
1
log xj dx < ∞. In particular, in the case of an
infinite server queue, where the rates decrease linearly to zero at the boundary,
the boundary may be reached in finite time at finite cost. Note also that the
condition is tight under the assumption that
∣∣~r ′(t)∣∣ < C: the linear rate of
approaching the boundary is optimal under this bound, as can be seen by a
change of variable argument.
In the case of one-dimensional processes, the condition on reachability is
virtually necessary as well as sufficient. This result is interesting enough that
we detail it here. We suppose without loss of generality that the boundary is
x = 0, and that the interior of G is contained in x > 0. Let µ(x) denote the sum
of the jump rates in negative directions, and Λ(x) denote the sum in positive
directions. We consider a path r(t) = b − at for t ∈ [0, T = b/a], where we
suppose that [0, b] ∈ G.
Lemma 34 Consider the one dimensional case. Let Assumption 2.C hold. As-
sume that the rates λi are bounded and that at least one rate of jump away from
the boundary is bounded away from 0. Then the boundary can be reached by the
path r(t) = b− at with finite cost under the following condition:
I[0,T ](r) <∞ if and only if
∫ b
0
log
1
µ(x)
dx <∞. (7.4)
Proof.
`(r, r′) = sup
θ
(
−aθ −
∑
i
λi(r)(exp(θei) − 1)
)
. (7.5)
Let I+ be the i with ei > 0, and let I− be the i with ei < 0. Differentiating
(7.5) with respect to θ and setting the result equal to zero, we see
−a −
∑
i∈I+
λi(r)eie
θei −
∑
i∈I−
λi(r)eie
θei = 0, (7.6)
or, equivalently, ∑
i∈I−
λi(r)(−ei)eθei = a+
∑
i∈I+
λi(r)eie
θei (7.7)
Both sides of (7.7) are positive, consisting of all positive terms. As r → 0 we
have λi(r) → 0 for all i ∈ I−. Therefore we have θ → −∞ as r → 0; recalling
32
that for at least one j ∈ I+ we have λj(0) > 0, we see that the rate at which
θ →−∞ as r → 0 is bounded below, independent of a.
Let
h
4
= min
i∈I−
|ei| and H 4= max
i∈I−
|ei| (7.8)
ε(r)
4
=
∑
i∈I+
λj(r)eie
θei . (7.9)
Then ε(r)→ 0 as r → 0, since θ →−∞ as r→ 0. By (7.7) we have
µ(r)heθh ≤ a + ε(r) ≤ µ(r)HeθH (7.10)
(recall µ(r) =
∑
i∈I− λi(r)). Therefore, for each δ > 0, when r is close enough
to zero, by using (7.10) to bound θ, we obtain
`(r, r′) ≤ a
h
log
a+ ε(r)
µh
+ µ+ Λ + δ (7.11)
`(r, r′) ≥ a
H
log
a+ ε(r)
µH
+ µ+ Λ− δ − a+ ε(r)
h
; (7.12)
the last term on the right of (7.12) is derived from (7.7) by the estimate∑
i
λi(r)e
θei ≤ a+ ε(r)
h
.
Therefore
I[0,T ](r) =
∫ T
0
(
log
a
µ(r(t))
+ O(1)
)
dt (7.13)
=
∫ b
0
(
log
1
µ(r)
+O(1)
)
dt. (7.14)
This shows that I[0,T ](r) is finite if and only if (7.14) is.
8 Conclusion
We have shown that the usual sample-path jump Markov large deviations theo-
rem and rate function remain unchanged even when jump rates tend to zero in
some cases. The cases include the very important one of infinite server queues,
as well as the case where the positive cone of jump directions does not change.
Moreover, while existing theories that include boundaries assume flat bound-
aries, the boundaries here are quite general.
However, our understanding of diminishing rate is not complete, as illus-
trated in Example C of the Introduction. Moreover, a theory combining dimin-
ishing rates with discontinuous rates, even with flat boundaries, or finite level
boundaries, is still lacking.
33
9 Appendix 1: Extensions to non-convex sets
In Section 2 we claimed that it is possible to extend the rates λi(x) from x ∈ G
to x ∈ Rd when G is convex, in such a way that the extended rates are Lipschitz
continuous (below Lemma 1). We now establish this for a more general class of
sets. Let G be a closed set that is the union of a finite number of closed convex
sets Gj. We do not assume that the Gj are compact. We use the following
notation. For any point x denote by yj(x) the projection of x onto Gj.
Lemma 35 (Contraction) For any x, z and j,
|yj(x)− yj(z)| ≤ |x− z| . (9.1)
Proof. Obvious. But can be made precise as follows. The only non trivial
case is where both are outside Gj. But then
|x− yj(z)|2 > |x− yj(x)|2 + |yj(x)− yj(z)|2 (9.2)
since the angle xyj(x)yj(z) is necessarily larger than 90 degrees. Write the
symmetric expression and sum up to obtain the result.
Lemma 36 (Continuity) Let f be a Lipschitz function on G with Lipschitz
constant Lf . Define the function g through
g(x) =

f(x) for x ∈ G ,∑J
j=1(d[x, yj(x)])
−1f(yj (x))∑J
j=1(d[x, yj(x)])
−1 for x 6∈ G .
(9.3)
Then g is continuous.
Proof. Fix an arbitrary point x and note that it suffices to prove continuity
locally at x. This is obvious if x is in the interior of G or in the interior of
its complement. Since G is closed, the only case to consider is how g changes
between the boundary and an outside point. So let {xn} be a sequence of
points outside G converging to x ∈ G. Suppose first that x ∈ Gj but x 6∈
Gk, k 6= j. Then by lemma 35 yj(xn)→ yj(x) = x. Since Gk is closed, we have
d(yk(xn), x) > εk ≥ ε > 0 for some ε, all k 6= j and all n large. Continuity then
follows since {f(xn)} is bounded. In the general case, by reordering the indices
we may assume that, for some `, x ∈ Gj for all j ≤ ` and x 6∈ Gj for all j > `.
Since f is Lipschitz and yj(x) = x, lemma 35 gives
f(yj (xn)) = f(x) + εjn , |εjn| ≤ Lf |xn − x| for j ≤ `, (9.4)
while obviously
|f(yj(xn)) − f(x)| ≤ K for some K, for j > `. (9.5)
34
By definition, for some ε > 0 we have, for all large n,{
d(xn, yj(xn))→ 0 j ≤ `, and
d(xn, yj(xn)) > ε j > `.
(9.6)
Let dn
4
= minj{d(xn, yj(xn))}. Then 0 < dn(d[xn, yj(xn)])−1 ≤ 1, and
lim
n→∞ dn(d[xn, yj(xn)])
−1εjn = 0 for j ≤ `, (9.7)
lim
n→∞ dn(d[xn, yj(xn)])
−1K = 0 for j > `. (9.8)
(9.9)
Therefore
lim
n
g(xn) = lim
n
∑J
j=1 dn(d[xn, yj(xn)])
−1f(yj (xn))∑J
j=1 dn(d[xn, yj(xn)])
−1 (9.10)
= lim
n
∑`
j=1 dn(d[xn, yj(xn)])
−1f(x)∑`
j=1 dn(d[xn, yj(xn)])
−1 (9.11)
= f(x) . (9.12)
Thus g(x) is continuous at each point x.
We now turn to a proof of Lipschitz continuity. This, again, is a local
property.
Lemma 37 (Lipschitz) Under the conditions of Lemma 36, g as defined in
(9.3) is Lipschitz continuous with Lipschitz constant Lf .
Proof. It suffices to prove Lipschitz continuity with constant Lf locally at x
for each point x. This holds by definition for x in the interior of G, and we next
establish the result for x in the interior of its complement. So, fix such a point
x and define
ε
4
=
1
2
min{d(x, z) : z ∈ G} (9.13)
so that by our assumption ε > 0. Fix an arbitrary 0 < δ < 1/2 and a point
z such that d(z, x) < δε: then d(z,G) > ε. Since x is fixed it will be conve-
nient to denote qj
4
= (d[x, yj(x)])
−1. Without loss of generality we assume that
f(x), f(yj (x)), f(z) and f(yj (z)) are all positive (this amounts to a shift by a
constant, and does not influence continuity properties). Now by lemma 35,
d[x, yj(x)] ≤ d[x, z] + d[z, yj(z)] + d[yj(z), yj(x)] (9.14)
≤ d[z, yj(z)] + 2εδ (9.15)
35
so that, since d(x,G) > ε,
1
d[z, yj(z)]
≤ 1
d[x, yj(x)]− 2εδ (9.16)
≤ qj(1 + 2δ) . (9.17)
Exchanging the roles in the triangle inequality, we conclude that
qj(1− 2δ) ≤ 1
d[z, yj(z)]
≤ qj(1 + 2δ) . (9.18)
But then
g(x)− g(z) =
∑J
j=1 qjf(yj (x))∑J
j=1 qj
−
∑J
j=1(d[z, yj(z)])
−1f(yj (z))∑J
j=1(d[z, yj(z)])
−1 (9.19)
≤
∑J
j=1 qjf(yj (x))∑J
j=1 qj
−
∑J
j=1 qj(1− 2δ)f(yj(z))∑J
j=1 qj(1 + 2δ)
(9.20)
≤
∑J
j=1 qj[f(yj(x)) − f(yj (z))]∑J
j=1 qj
+
∑J
j=1 qjf(yj (z))∑J
j=1 qj
(
1− 1− 2δ
1 + 2δ
)
(9.21)
≤ Lfd(x, z) + max
j
{|f(yj(z))|} · 4δ
1 + 2δ
. (9.22)
Exchanging the roles of x and z we obtain in exactly the same way
g(z) − g(x) ≤ Lfd(x, z) + max
j
{|f(yj(x))|} · 4δ
1 + 2δ
. (9.23)
Since δ was arbitrary, the Lipschitz continuity is established.
It remains to consider the case where x is on the boundary of G. So fix a
direction z: if x+η · z is in G for all η small, then there is nothing to prove. We
need only consider the case where x+ η · z is in the (open) complement of G for
all 0 < η ≤ η0. So fix an arbitrary ε. By lemma 36, we can find a δ and a point
u in the complement of G (actually, on the line segment (x, x+ η0z)) so that
|g(x)− g(u)| ≤ ε , (9.24)
d(x, u) ≤ εd(x, z) . (9.25)
By the Lipschitz property for points in the complement of G we conclude
|g(x)− g(z)| ≤ |g(x)− g(u)|+ |g(u)− g(z)| (9.26)
≤ ε+ Lfd(u, z) (9.27)
≤ ε+ Lf (1 + ε)d(x, z) . (9.28)
Since ε is arbitrary the result is established.
36
10 Appendix 2: An Example
Consider the region
G = {(x, y) : 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, x+ y ≤ 1.5} .
As pictured in Figure 10, we take jump directions
~e1 = (1, 0); ~e2 = (−1, 0); ~e3 = (0, 1);~e4 = (0,−1);~e5 = (−1, 1).
We suppose that the jump rates are as follows. Let
t(x, y) = min(1− x, 1.5− y − x);
t(x, y) represents a distance from the point (x, y) to the right boundaries of
G (the vertical and slanted lines). Similarly, t(y, x) represents a distance from
(x, y) to the upper boundaries of G. Now we define jump rates as follows:
λ1(~x) = t(x, y) λ2(~x) = x
λ3(~x) = t(y, x) λ4(~x) = y
λ5(~x) = x(1− y).
0
0
1
1
0.5
0.5
A B
C
DE
12
4
35
Figure 1: Illustration of the example
Note that our assumptions allow the process to jump out of G. For example,
suppose ~zn(0) = (1/2, 1/4). Then for every odd n the process can exit G. Take
n = 3. Then two jumps in a row in direction 1 lead ~zn(t) first to (5/6, 1/4), then
to (7/6, 1/4); the associated jump rates are 1/2 and 1/6 respectively. Now the
process is outside G. As detailed in Appendix 1, we can extend the jump rates of
~zn to all of Rn by taking the rates to be those at the projection of ~zn on G; that
is, as the rates of the point closest to ~zn in G. Continuing with our example,
from here (7/6, 1/4) (which projects to (1, 1/4)) the process can take a jump in
direction 3, to (7/6, 7/12), with rate 3/4. Now the process can take two jumps in
direction 5, from (7/6, 7/12) to (5/6, 11/12) and then to (1/2, 5/4), with rates
37
1/2 and 15/98 (the points project to (1, 1/2) and (5/7, 11/14) respectively).
From (1/2, 5/4) the process can jump only to the left or down (directions 2 or
4); we leave the reader to explore how far it may travel before returning to the
interior of G.
We claim that the process thus described satisfies all our assumptions. The
only part of this claim that is not immediate is Assumption 2 part C. For this
we need to make direction vectors ~v at each corner point (labeled A through
E in Figure 10) that make the small jump rates that exist near each corner
increase monotonically as we move in directions parallel to the vectors. (We
also need to check the flat portions of the boundaries, but this is easy given the
corner point calculations.) The appropriate vectors are pictured in each corner.
Specifically, at point A we use vector (1, 1), at point B we use vector (−1, 1),
at point C we use vector (−1, 0), at point D we use vector (0,−1), and at point
E we use vector (1,−1). The reader may check easily that these vectors cause
monotone increases in the small jump rates: at point A the small rates are λ2,
λ4, and λ5; at point B the small rates are λ1 and λ4; at point C the small rates
are λ1 and λ3; at point D the small rates are λ1, λ3, and λ5; and at point E
the small rates are λ2, λ3, and λ5.
To work just one example (all work the same way), consider a point ~x = (x, y)
near point A = (0, 0). Then
λ2(~x + α~v) = x+ α
λ4(~x + α~v) = y + α
λ5(~x + α~v) = (x+ α)(1− y − α) = x(1− y) + α(1− y − x)− α2;
it is clear that these functions are all monotone increasing in α for small values
of x, y, and α.
38
References
[1] Atar, Rami and Paul Dupuis, “Large deviations and queueing networks:
methods for rate function identification,” Stoch. Proc. Appl. 84 pp. 255–
296, 1999.
[2] Botvich, D. and N. Duffield. “Large deviations, the shape of the loss
curve, and economies of scale in large multiplexers,” Queueing Systems
20, pp. 293–320, 1995.
[3] Courcoubetis, C. and R. Weber, “Buffer overflow asymptotics for a buffer
handling many traffic sources,” J. of Appl. Prob. 33, pp. 886–903, 1996.
[4] Dupuis, Paul and Richard Ellis, “The large deviations principle for a gen-
eral class of queueing systems I.” Trans. Amer. Math. Soc. 347 pp. 2689–
2751, 1995.
[5] Dupuis, Paul and Richard Ellis, A Weak Convergence Approach to the
Theory of Large Deviations, John Wiley, 1997.
[6] Dupuis, Paul, Richard Ellis and Alan Weiss, “Large deviations for Markov
processes with discontinuous statistics I: general upper bounds,” Ann.
Prob. 19 pp. 1280–1297, 1991.
[7] Dembo, Amir and Ofer Zeitouni, Large Deviations Techniques and Appli-
cations, Second Edition, Springer-Verlag, NY 1998.
[8] Freidlin, M.I. and A.D. Wentzell, Random perturbations of Dynamical
Systems, Second Edition, Springer-Verlag, NY 1998.
[9] Ignatiouk-Robert, Irina, “Sample path large deviations and convergence
parameters,” Ann. Applied Prob. 11 pp. 1292–2329, 2002.
[10] Mandjes, M. and A. Ridder “Optimal trajectory to overflow in a queue
fed by a large number of sources,” Queueing Systems 31, pp. 137–170,
1999.
[11] Mandjes, Michel and Alan Weiss, “Sample path large deviations of a mul-
tiple time-scale queueing model,” submitted, 2000.
[12] Puhalskii, Anatolii, Large Deviations and Idempotent Probability,
Chapman-Hall/CRC, 2001.
[13] Shwartz, Adam and Alan Weiss, Large Deviations for Performance Anal-
ysis, Chapman-Hall, 1995.
[14] Simonian, A. and J. Guibert. “Large deviations approximation for fluid
queues fed by a large number of on/off sources,” IEEE J. Selected Areas
Comm. 13, pp. 1017–1027, 1995.
[15] Tse, David, “Asymptotic Optimality of a Measurement-Based Call Ad-
mission Control Scheme”, Proc. 34th Allerton Conf, Oct., 1996.
39
