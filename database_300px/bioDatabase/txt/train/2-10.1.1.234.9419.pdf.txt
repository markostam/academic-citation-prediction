ar
X
iv
:m
at
h/
06
10
82
4v
1 
 [m
ath
.PR
]  
27
 O
ct 
20
06
L-DIVERGENCE CONSISTENCY FOR A DISCRETE PRIOR
MARIAN GRENDAR
Department of Mathematics, FPV UMB
Tajovskeho 40, 974 01 Banska Bystrica, Slovakia
Institute of Mathematics and CS, Banska Bystrica, Slovakia
Institute of Measurement Sciences, Bratislava, Slovakia
Email: marian.grendar@savba.sk
summary
Posterior distribution over a countable setM of continuous data-sampling distri-
butions piles up at L-projection of the true distribution r on M, provided that
the L-projection is unique. If there are several L-projections of r onM, then the
posterior probability splits among them equally.
Keywords and phrases: Bayesian consistency, L-divergence, multiple L-
projections
AMS Classification: 60F10, 60F15
1 Introduction
Walker [6] has recently considered consistency of posterior distribution in Hellinger distance,
for strictly positive prior over a countable set of continuous data-sampling distributions.
By means of his martingale approach [7], Walker developed a sufficient condition for the
Hellinger consistency of posterior density in the above mentioned setting. Via a simple
large-deviations approach we show that in this setting posterior density is always consistent
in L-divergence. The consistency holds also under misspecification. If there are multiple
’concentration points’ (L-projections) the posterior spreads among them equally.
2 Bayesian nonparametric consistency
Let there be countable set M = {q1, q2, . . . } of probability density functions with respect
to the Lebesgue measure; sources, for short. On the set a Bayesian puts his strictly positive
prior probability mass function π(·). Let r be the true source of a random sample Xn ,
X1, X2, . . . , Xn. Provided that r ∈ M, as the sample size grows to infinity, the posterior
distribution π(·|Xn = xn) overM is expected to concentrate in a neighborhood of the true
source r. Whether and under what conditions this indeed happens is a subject of Bayesian
nonparametric consistency investigations. Surveys of the subject can be found at [2], [8]
among others.
Ghosal, Ghosh and Ramamoorthi [2] define consistency of a sequence of posteriors with
respect to a metric or discrepancy measure d as follows: The sequence {π(·|Xn), n ≥ 1} is
said to be d-consistent at r, if there exists a Ω0 ⊂ R∞ with r(Ω0) = 1 such that for ω ∈ Ω0,
1
for every neighborhood U of r, π(U |Xn) → 1 as n goes to infinity. If a posterior is d-
consistent for any r ∈M then it is said to be d-consistent. There, two modes of convergence
are usually considered: convergence in probability and almost sure convergence.
Obviously, in the definition the set of sources is not restricted to be countable. The
present work is concerned with the countable M case.
3 Sanov’s Theorem for Sources, L-consistency
Let Me , {q : q ∈ M, π(q) > 0} be support of the prior pmf. In what follows, r is not
necessarily from Me. Thus we are interested also in Bayesian consistency under misspeci-
fication; i.e., when π(r) = 0. The problem is the same as in the case of standard Bayesian
consistency (cf. Sect. 2): to find the source(s) upon which the posterior concentrates.
For two densities p, q with respect to the Lebesgue measure1 λ, the I-divergence I(p||q) ,∫
p log(p/q). The L-divergence L(q||p) of q with respect to p is defined as L(q||p) ,
−
∫
p log q. The L-projection qˆ of p on Q is qˆ , arg infq∈Q L(q||p). There Q is a set of prob-
ability densities defined on the same support. The value of L-divergence at an L-projection
of p on Q is denoted by L(Q||p).
The following Sanov’s Theorem for Sources (LST) will be needed for establishing the
consistency in L-divergence. The Theorem provides rate of the exponential decay of the
posterior probability.
LST Let N ⊂Me. As n→∞,
1
n
log π(q ∈ N|xn)→ −{L(Me||r) − L(N||r)},
with probability one.
Proof Let ln(q) , exp(
∑n
l=1 log q(Xl)), ln(A) ,
∑
q∈A ln(q), and ρn(q) , π(q)ln(q),
ρn(A) ,
∑
q∈A ρn(q). In this notation π(q ∈ N|x
n) = ρn(N )
ρn(Me)
. The posterior probability is
bounded above and below as follows:
ρˆn(N )
lˆn(Me)
≤ π(q ∈ N|xn) ≤
lˆn(N )
ρˆn(Me)
,
where lˆn(A) , supq∈A ln(q), ρˆn(A) , supq∈A ρn(q).
1
n
(log lˆn(N )− log ρˆn(Me)) converges with probability one to L(N||r) − L(Me||r). The
same is the ’point’ of a.s. convergence of 1
n
log of the lower bound.
Let for ǫ > 0, NCǫ (M
e) , {q : L(Me||r) − L(q||r) > ǫ, q ∈ Me}. Let Nǫ(Me) ,
Me\NCǫ .
Corollary Let there be a finite number of L-projections of r on Me. As n → ∞,
π(q ∈ NCǫ (M
e)|xn)→ 0, with probability one.
Standard Bayesian consistency follows as a special π(r) > 0 case of the Corollary.
1Any σ-finite measure, in general.
2
4 Posterior Equi-concentration of Sources
If there is more than one L-projection of r on Me, how is the posterior probability asymp-
totically spread among them? This issue is ’in probability’ answered by the next Theorem.
Let N 1ǫ ⊂ Nǫ(M
e) contain (among other sources) just one L-projection of r on Me.
Theorem Let there be k L-projections of r on Me. Then for n going to infinity, π(q ∈
N 1ǫ |x
n)→ 1k , in probability.
Proof For any ǫ > 0, there exists such n0 that for n > n0, r{xn : S(qˆλ) = S(qˆL)} = 1,
where qˆλ , arg supq∈Me π(q|x
n), qˆL is L-projection of r on Me, and S(·) stands for ’set of
all’. Consequently, π(qˆL|xn) ≥ π(q|xn) for all q ∈ Me. Posterior π(q ∈ N 1ǫ |x
n) can be ex-
pressed as (1−A)/(k(1−B)), where A ,
∑
σ1
π(q|xn)/π(qˆL|xn), B ,
∑
σ2
π(q|xn)/kπ(q|xn);
σ1 , N
1
ǫ \qˆL, σ2 , M
e\
⋃k
j=1 qˆ
j
L. Markov’s inequality implies that π(q|x
n)/π(qˆL|x
n) con-
verges to zero, in probability. Slutsky’s Theorem then implies that A, B converges to zero,
in probability.
5 EndNotes
In order to place this note in context let us make a few comments.
1) An inverse of Sanov’s Theorem has been established by Ganesh and O’Connell [1]
for the case of sources with finite alphabet, by means of formal large-deviations approach.
Unaware of their work, the present author developed in [3] an inverse of Sanov’s Theorem
for n-sources, for both discrete and continuous alphabet and applied it to conditioning by
rare sources problem and criterion choice problem; cf. also [4].
2) At [3] the concepts of L-divergence and L-projection were introduced. See [3] for a
short discussion on why or why not the ’new’ divergence.
3) The present form of Sanov’s Theorem for Sources (LST) as well as its proof are new.
4) Bayesian consistency under misspecification has already been studied by Kleijn and
van der Vaart [5] for general setting of continuous prior on a set of continuous sources, using
a different technique. The authors developed sufficient conditions for somewhat related
consistency (cf. Corollary 2.1 and Lemma 6.4 of [5]) as well as rates of convergence. The
equi-concentration was not considered there.
5.1 Acknowledgements
Supported by VEGA grant 1/3016/06.
References
[1] Ganesh, A. and N. O’Connell (1999). An inverse of Sanov’s Theorem. Stat. & Prob.
Letters, 42, 201–206.
3
[2] Ghosal, A., Ghosh, J. K. and R. V. Ramamoorthi (1999). Consistency issues in
Bayesian Nonanparametrics. Asymptotics, Nonparametrics and Time Series: A Tribute
to Madan Lal Puri, 639–667, Dekker.
[3] Grendar. M. (2005). Conditioning by rare sources. Acta Univ. Belii, Math., 12 19-29.
Online at http://actamath.savbb.sk
[4] Grendar, M. and G. Judge (2006). Large deviations theory and Empirical Estimator
choice. preprint, UC Berkeley.
[5] Kleijn, B. J. K. and A. W. van der Vaart (2002). Misspecification in infinite-dimensional
Bayesian statistics. preprint.
[6] Walker, S. (2004). Hellinger consistency for a discrete prior. Pak. J. Stat., 20 321–327.
[7] Walker. S. (2004). New appraoches to Bayesian consistency. Ann. Statist., 32 2028–
2043.
[8] Walker, S., Lijoi, A. and I. Pru¨nster (2004). Contibutions to the understanding of
Bayesian consistency. working paper.
4
