MATHEMATICS OF OPERATIONS RESEARCH
Vol. 34, No. 1, February 2009, pp. 1â€“25
issn 0364-765X eissn 1526-5471 09 3401 0001
informs Â®
doi 10.1287/moor.1080.0352
Â©2009 INFORMS
On Safe Tractable Approximations of Chance-Constrained
Linear Matrix Inequalities
Aharon Ben-Tal
Faculty of Industrial Engineering and Management, MINERVA Optimization Center, Technionâ€“Israel Institute of Technology,
Technion City, Haifa 32000, Israel, abental@ie.technion.ac.il
Arkadi Nemirovski
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332,
nemirovs@isye.gatech.edu
In the paper we consider the chance-constrained version of an afï¬nely perturbed linear matrix inequality (LMI) constraint,
assuming the primitive perturbations to be independent with light-tail distributions (e.g., bounded or Gaussian). Constraints
of this type, playing a central role in chance-constrained linear/conic quadratic/semideï¬nite programming, are typically com-
putationally intractable. The goal of this paper is to develop a tractable approximation to these chance constraints. Our
approximation is based on measure concentration results and is given by an explicit system of LMIs. Thus, the approximation
is computationally tractable; moreover, it is also safe, meaning that a feasible solution of the approximation is feasible for
the chance constraint.
Key words : chance constraints; linear matrix inequalities; convex programming; measure concentration
MSC2000 subject classiï¬cation : Primary: 90C15, 90C22, 90C25; secondary: 60F10
OR/MS subject classiï¬cation : Primary: programming/stochastic; secondary: mathematics/matrices
History : Received November 20, 2006; revised October 19, 2007 and August 13, 2008.
1. Introduction. In this paper we study uncertain linear matrix inequalities (LMIs)
x  0 (1)
where x âˆˆRm is the decision vector,  âˆˆRd is data perturbation, the body x  of the inequality is bi-afï¬ne
mapping in x and  taking values in the space Sn of symmetric nÃ— n matrices, speciï¬cally:
x =0	x
+
dâˆ‘
l=1
ll	x
 (2)
where the matrices 0	x
    d	x
 âˆˆ Sn are afï¬ne in x. Here A B means that AB are symmetric matrices
such that the matrix Aâˆ’B is positive semideï¬nite. We are interested in the case when (1) is a constraint in an
optimization problem we wish to solve, and our goal is to process such an uncertain constraint. Given the basic
role played by LMI constraints in modern convex optimization and the fact that the data in real-life optimization
problems in many cases are uncertain (not known exactly the time the problem is to be solved), the question of
how to process an uncertain LMI constraint is of major interest.
For the time being, there are two main approaches to treating uncertain constraints. The more traditional one,
offered by stochastic programming, utilizes a stochastic uncertainty model:  is assumed to be a random vector
with known (perhaps only partially) distribution. Here a natural way is to pass from the uncertain constraint (1)
to its chance-constrained versionâ€”the usualâ€”â€œcertainâ€â€”constraint
px = inf
Pâˆˆ
Probâˆ¼P x  0â‰¥ 1âˆ’  (3)
where  is the family of all probability distributions of  compatible with our a priori information, and  âˆˆ 01
is a given tolerance. An alternative to this approach, offered by robust optimization, is based on an â€œuncertain-
but-boundedâ€ model of data perturbations where all our a priori knowledge of  is that it belongs to a given
uncertainty set . In this case, a natural way is to replace the uncertain constraint with its robust counterpart
x  0 âˆ€  âˆˆ (4)
Note that both outlined approaches â€œas they areâ€ usually lead to computationally intractable constraints. As far
as the chance-constrained LMI (3) is concerned, typically the only way to check whether a given point belongs
to its feasible set is to use Monte Carlo simulation with sample size of order âˆ’1, and this is computationally
too demanding when  is small. Another difï¬culty comes from the fact that the feasible set of (3) is usually
1
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
2 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
nonconvex. The latter complication does not arise with the robust optimization approachâ€”the feasible set of (4)
is always convex; unfortunately, the ï¬rst difï¬cultyâ€”impossible to check efï¬ciently whether this semi-inï¬nite
convex constraint is satisï¬ed at a given pointâ€”may become even more severe than in the case of chance-
constrained LMI. These tractability difï¬culties of processing the LMI (1) make it natural to replace such a
constraint with a safe tractable approximationâ€”a system  of efï¬ciently computable convex constraints in
variables x and, perhaps, additional variables u such that whenever xu is feasible for  , x is feasible for the
constraint (1). For the time being, â€œtightâ€ (in a certain precise sense) approximations of this type are known only
for the robust counterpart type constraints (4), and only under speciï¬c restrictions on the structure of x ;
see Ben-Tal et al. [3, 4, 5]. In this paper, we focus solely on chance-constrained LMIs (3). In this case, seemingly
the only safe tractable approximation known in the literature is the one given by the general scenario approach.
For a chance-constrained optimization program
min
x
f0x Probfix â‰¤ 0â‰¥ 1âˆ’  i= 1     I
its scenario approximation is the random optimization program
min
x
f0x fix 
jâ‰¤ 0 i= 1     I j = 1     J 
where 1     J is a sample of independent realizations of  . Theoretical justiï¬cation of this natural approxi-
mation scheme is presented in Calaï¬ore and Campi [8] and de Farias and Van Roy [9]. In particular, it is shown
in Calaï¬ore and Campi [8] that if f0x, fix , i= 1     I , are convex in x âˆˆRm and the sample size J is
large enough:
J â‰¥ J âˆ— =Ceil [2mâˆ’1 log 12/+ 2âˆ’1 log 2/+ 2m]  (5)
then an optimal solution to the approximation, up to probability â‰¤  of â€œbad sampling,â€ is feasible for the
chance-constrained problem. (For substantial extensions of this remarkable result to the case of ambiguously
chance-constrained convex problems, see Erdogan and Iyengar [10].) Although pretty general (in particular,
imposing no restrictions on how the random perturbations enter the constraints and how they are distributed)
and tractable, the scenario approximation has an intrinsic drawbackâ€”it requires samples of order 1/, and
thus becomes prohibitively computationally demanding when  becomes small, like 10âˆ’5 or less. For afï¬nely
perturbed LMIs (2) with independent of each other â€œlight-tailâ€ perturbations l, l= 1     d, this drawback can
be circumvented by a kind of importance sampling; see Nemirovski and Shapiro [15]. In this paper, we work
under the same assumptions as in Nemirovski and Shapiro [15], i.e., focus on afï¬nely perturbed LMIs with
independent-of-each-other light-tail random perturbations l, and develop a novel, safe, tractable approximation
of the chance-constrained versions (3) of these LMIs. In contrast to the purely simulation-based approximations
of Calaï¬ore and Campi [8], Erdogan and Iyengar [10], and Nemirovski and Shapiro [15], our new approximation
is nearly analytic. Speciï¬cally, by itself our approximation is an explicit semideï¬nite program depending on
a pair of real parameters and completely independent of any samples. In order for this approximation to be
safe, the pair of parameters in question should be â€œproperly guessed,â€ that is, should ensure the validity of a
speciï¬c large-deviation-type inequality. In principle, we can point out appropriate values of these parameters in
advance. However, to reduce the conservatism of the approximation, we allow for an â€œoptimisticâ€ choice of the
parameters and introduce a speciï¬c simulation-based postoptimization validation procedure that allows us either
to justify our â€œoptimistic guessâ€ (and thus guarantees â€œup to probability â‰¤  of bad samplingâ€ that the solution
we end up with is feasible for the chance constraint of interest), or else demonstrates that our guess was â€œtoo
optimistic,â€ in which case we can pass to an approximation with better-chosen parameters. It should be stressed
that in principle the size J of the sample used in this â€œvalidation procedure" is completely independent of how
small the tolerance  is; all we need is J â‰¥O1 ln1/.
The rest of the paper is organized as follows. In Â§2 we make our standing assumptions and outline and
motivate our approximation strategy. This strategy is fully developed in Â§Â§3.1 and 3.2. In Â§4 we consider
two important special cases of (3). In the ï¬rst of them, all matrices l	x
, l = 01     d, are diagonal. This
is the case of randomly perturbed scalar linear inequalities or, which is the same, about chance-constrained
linear programming. In the second special case, the matrices l	x
, l= 1     d, are of the form lxGx+
exf Tl x+ flxeT x, where ex and flx are vectors (and, as always in this paper, l	x
 is afï¬ne in x).
This situation covers the case when (1) is a randomly perturbed conic quadratic inequality A	x
 + b	x
2 â‰¤
cT 	x
 +d	x
 (A	x
 b	x
 c	x
d	x
 are afï¬ne in x); indeed,
A	x
 + b	x
2 â‰¤ cT 	x
 +d	x
 â‡”
[
d	x
 bT 	x

b	x
 d	x
I
]
ï¸¸ ï¸·ï¸· ï¸¸
0	x

+
dâˆ‘
l=1
l
[
cl	x
 A
T
l 	x

Al	x
 cl	x
I
]
ï¸¸ ï¸·ï¸· ï¸¸
l 	x

 0 (6)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 3
where Al	x
 are the columns of A	x
, and cl	x
 are the entries of c	x
. Note that â€œfully analyticâ€ safe, tractable
approximations of chance-constrained LPs were recently proposed in Nemirovski and Shapiro [14]; Â§4 contains a
comparison of approximations from Nemirovski and Shapiro [14] with the one developed in this paper. Section 5
presents techniques allowing us to reduce the task of building a safe approximation for the chance-constrained
LMI (3) (under partially known â€œlight-tailâ€ distributions of independent perturbations l) to a similar task for
an appropriately chosen reference distribution of  (most notably a Gaussian one). The concluding Â§6 presents
numerical illustrations.
2. Goals, assumptions, strategy. Recall that our ultimate goal is to process a given chance-constrained
optimization problem of the form
min
x
ï£±ï£´ï£²ï£´ï£³cT x
F xâ‰¤ 0
Prob
{
0	x
+
dâˆ‘
l=1
ll	x
 0
}
â‰¥ 1âˆ’ 
ï£¼ï£´ï£½ï£´ï£¾  (7)
where F x is an efï¬ciently computable vector function with convex components, 0	x
    d	x
 are sym-
metric matrices afï¬nely depending on the decision vector x,  âˆˆ 01 is a given tolerance, and 1     d are
random perturbations. What we intend to do is to replace in (7) the â€œtroublemakingâ€ chance constraint with a
safe tractable approximation, the latter notion being deï¬ned as follows:
Deï¬nition 2.1. We say that an explicit system  of efï¬ciently computable convex constraints on variables x
and additional variables u is a safe, tractable approximation of the chance-constrained LMI
px = Prob
{
0	x
+
dâˆ‘
l=1
iAi	x
 0
}
â‰¥ 1âˆ’  (8)
if whenever a vector x can be extended to a feasible solution xu of  , x is feasible for the chance con-
straint (8) (or, which is the same, if the projection X of the solution set of  on the space of x-variables is
contained in the feasible set of (8)).
Note that the requirement that X is contained in the feasible set of (8) means that  produces a sufï¬cient
condition for (8) to be satisï¬ed (â€œsafetyâ€ of the approximation). Similarly, the requirement that  is a system
of efï¬ciently computable convex constraints implies that we can minimize efï¬ciently convex functions over X
(â€œtractabilityâ€ of the approximation).
Replacing the chance constraint (8) in the optimization problem (7) with a safe tractable approximation we
get an optimization problem in variables xu with efï¬ciently computable convex constraints, that is, we get
an efï¬ciently solvable problem, and feasible solutions of this problem are feasible for the problem of actual
interest (7).
We shall address the problem of building a safe, tractable approximation of (8) under the following assumption
on the random perturbations:
Assumption A. The scalar random variables 1     d are mutually independent with zero means and
either (a) all l have bounded ranges, or (b) all l are Gaussian.
Note that applying deterministic scalings l â†’ l/sl, l	x
 â†’ sll	x
, in the case of (a) we can convert the
ranges of l into the segment 	âˆ’11
, and in the case of (b) we can enforce l âˆ¼  01 for all l. Therefore,
from now on, if not stated otherwise, we assume that either
(A.1.) l is supported on 	âˆ’11
, or
(A.2.) l âˆ¼ 01 for all l.
2.1. The strategy. The idea of the construction we are about to develop is simple. Essentially, what we are
looking for is a veriï¬able sufï¬cient condition for the relation
A0+
dâˆ‘
l=1
lAl  0 (9)
to be satisï¬ed with probability at least 1âˆ’ ; here A0    Ad are given nÃ— n symmetric matrices. Assuming,
for the sake of argument, that l are symmetrically distributed and  is small, this is basically the same as to
seek a sufï¬cient condition for the relation
Prob
{
âˆ’A0  S =
dâˆ‘
l=1
lAl A0
}
â‰¥ 1âˆ’  (10)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
4 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
An evident necessary condition here is A0  0. Assuming a bit more, namely, that A0  0, the condition of
interest becomes
Prob
{
âˆ’I  SË† =
dâˆ‘
l=1
lAË†l  I
}
â‰¥ 1âˆ’  AË†l =Aâˆ’1/20 AlAâˆ’1/20  (11)
Now, in case (A.2) it is intuitively clear (and can be easily proved) that (11) implies that
ESË†2=
dâˆ‘
l=1
AË†2l O1I âˆ—
with some positive absolute constant O1. Thus, the condition âˆ— is a necessary condition for (11), provided
that we want the latter condition to be satisï¬ed for all distributions of  compatible with Assumption (A.1).
Now assume for a moment that a condition of the type âˆ—, namely, the condition
dâˆ‘
l=1
AË†2l â‰¤ )2I  (12)
with properly chosen ), is sufï¬cient for (11) to be valid. Then we are basically done: It is immediately seen
that (12) can be equivalently reformulated as the LMI
Arrow)A0A1    Adâ‰¡
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
)A0 A1    Ad
A1 )A0

  
Ad )A0
ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£» 0 (13)
in variables A0A1    Ad. It follows that when Al = l	x
, l = 01     d, depend afï¬nely on a decision
vector x (the situation we are interested in), our sufï¬cient condition (13) for the validity of (10) (and thus for the
validity of (9) as well) becomes an LMI in variables x and thus provides us with safe tractable approximation
of (8). The level of conservatism of this approximation can be quantiï¬ed by )â€”the less ) is, the larger is the
â€œgapâ€ between the sufï¬cient condition (12) and the necessary condition âˆ—. It is shown in Nemirovski [13] that
in order for (12) to be always sufï¬cient for (9) (i.e., independently of the structure of AË†l âˆˆ Sn and the random
perturbations lâ€”as long as they satisfy Assumption A), then ) should be at most O1	
âˆš
lnn+âˆšln1/
âˆ’1.
In Nemirovski [13], it is also proved that with properly chosen O1 and with ) = O1	n1/6 +âˆšln1/
âˆ’1,
condition (12) is sufï¬cient for the validity of (10), and is conjectured that this conclusion remains true when
n1/6 is replaced with â€œunimprovableâ€
âˆš
lnn. This conjecture was justiï¬ed recently; see Man-Cho So [12] and
Proposition A.1 in the appendix. Note, however, that the outlined value of ) that provably makes (12) sufï¬cient
for (10) is worst-case oriented and thus might typically lead to an overly conservative approximation (13). The
main idea of this paper is that, given any guess of )* that makes (12) sufï¬cient for the validity of (10), we can
use a cheap simulation-based procedure to validate the result yielded by this guess, or else to reï¬ne our guess.
Numerical results presented in Â§3 demonstrate that this approach can result in signiï¬cantly less-conservative
approximations of (9) than those associated with the above â€œprovably safeâ€ values of )*.
3. Approximating chance-constrained LMIs.
3.1. Preliminaries on measure concentration. Our strategy heavily exploits the following fact:
Theorem 3.1 (â€œMeasure Concentrationâ€). Let 1     d satisfy Assumption A, + > 0, and * âˆˆ 01/2
be reals, and B0    Bd be deterministic symmetric matrices such that
a ArrowB0B1    Bd 0
b Prob
{
âˆ’+B0 
dâˆ‘
l=1
lBl +B0
}
â‰¥ 1âˆ’* (14)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 5
Then
) â‰¥ 1 â‡’ Prob
{
âˆ’)+B0 
dâˆ‘
l=1
lBl  )+B0
}
â‰¥ 1âˆ’ *)
*)=
ï£±ï£´ï£²ï£´ï£³
1
1âˆ’* expâˆ’+
2)âˆ’ 12/16  satisï¬es (A.1)
ErfErfInv*+ )âˆ’ 1max	+ ErfInv*
  satisï¬es (A.2)
(15)
Here and in what follows ErfÂ· and ErfInvÂ· are the error function and its inverse:
Erfx= 1âˆš
2-
âˆ« 
x
expâˆ’s2/2ds ErfErfInvrâ‰¡ r 0< r < 1 (16)
Proof. Under the premise of our theorem, we clearly have B0  0; by continuity reasons, it sufï¬ces to
prove the theorem in the case of B0  0. In this case, passing from the matrices B0B1    Bd to the matri-
ces IBâˆ’1/20 B1B
âˆ’1/2
0     B
âˆ’1/2
0 BdB
âˆ’1/2
0 , we immediately reduce the situation to the one with B0 = I , which we
assume from now on. In this case, (14.a) becomes simply
âˆ‘d
l=1B
2
l  I . With this normalization, the validity
of (15) in the case of (A.1) is readily given by Lemma 1 in Nemirovski [13], and in the case of (A.2) by the
following reï¬nement of Theorem 1 in Nemirovski and Shapiro [14]: 
Lemma 3.1. Let  âˆ¼ 0 Id, and let QâŠ‚Rd be a closed convex set such that Prob âˆˆQâ‰¤ * < 1/2. Then
(i) Q contains the centered at the origin  Â· 2-ball of radius ErfInv*;
(ii) If Q contains the centered at the origin  Â· 2-ball of a radius r â‰¥ ErfInv*, then for every ) > 1 one has
Prob âˆˆ )Qâ‰¤ Erf)âˆ’ 1r +ErfInv*â‰¤ Erf)ErfInv*â‰¤ expâˆ’)2ErfInv2*/2 (17)
Proof of Lemma 3.1. (i) is immediate. Indeed, assuming the opposite and invoking the separation theorem,
Q is contained in a closed half-space 1 = x eT x â‰¤ r with a unit vector e and certain r < ErfInv*, and
therefore Prob âˆˆQâ‰¥ Prob âˆˆ1= Erfr > * , which is a contradiction.
(ii) is an immediate corollary of the following fact due to Borell [6]:
(!) For every * âˆˆ 01,  â‰¥ 0 and every closed set X âŠ‚ Rd such that Prob âˆˆ X â‰¤ * one has
Prob distX > â‰¤ ErfErfInv*+ , where distaX=min
xâˆˆX
aâˆ’ x2.
In the situation of (ii), Q contains the centered at the origin  Â· 2-ball Br of the radius r , whence the set )Q,
) â‰¥ 1, contains Q + ) âˆ’ 1Q âŠƒ Q + ) âˆ’ 1Br and thus contains the set x distxQ â‰¤  = ) âˆ’ 1r.
Invoking (!) with X =Q and = )âˆ’ 1r , we get the ï¬rst inequality in (17); the second inequality there is due
to r â‰¥ ErfInv*, and the last inequality is well known. 
Lemma â‡’ case (A.2) of theorem: Let Q= u âˆˆRd âˆ’+I âˆ‘dl=1 ulBl +I, so that Q is a closed convex
set in Rd such that Prob âˆˆ Q â‰¤ * < 1/2 by (14.a). We claim that Q contains the centered at the origin
 Â· 2-ball of the radius + (and thus, by Lemma 3.1.(i)), contains the centered at the origin  Â· 2-ball of the
radius rÂ¯ =max	+ ErfInv*
). Indeed, when u2 â‰¤+ , we have for every e âˆˆRn:âˆ¥âˆ¥âˆ¥âˆ¥ dâˆ‘
l=1
ulBle
âˆ¥âˆ¥âˆ¥âˆ¥
2
â‰¤
dâˆ‘
l=1
ulBle2 â‰¤ u2
[ dâˆ‘
l=1
Ble22
]1/2
â‰¤+
[ dâˆ‘
l=1
eT B2l e
]1/2
= +
âˆšâˆšâˆš
eT
[ dâˆ‘
l=1
B2l
]
eâ‰¤+e2
where the concluding inequality is due to
âˆ‘d
l=1B
2
l  I . Thus, whenever u2 â‰¤+ , we have u âˆˆQ. We clearly
have Probâˆ’)+ âˆ‘dl=1 lBl  )+ = 1âˆ’ Prob âˆˆ )Q; the latter quantity, by Lemma 3.1.(ii) applied with
r = rÂ¯ , is â‰¥ 1âˆ’ *). 
Corollary 3.1. Given  âˆˆ 01, + > 0, * âˆˆ 01/2, let us set
3âˆ’1 =
{
+ + 4âˆšlnâˆ’11âˆ’*âˆ’1 we are in the case of (A.1)
+ +max	ErfInv/ErfInv*âˆ’ 10
min	+ ErfInv*
 we are in the case of (A.2)
(18)
Assume, further, that symmetric matrices A0    Ad satisfy
Arrow3A0A1    Ad 0 (19)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
6 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
and, in addition, that
Prob
{
âˆ’+	3A0

dâˆ‘
l=1
lAl +	3A0

}
â‰¥ 1âˆ’* (20)
Then
Prob
{
âˆ’A0 
dâˆ‘
l=1
lAl A0
}
â‰¥ 1âˆ’  (21)
Proof. Relations (19) and (20) imply that the matrices B0 = 3A0B1 = A1    Bd = Ad satisfy (14).
It remains to apply Theorem 3.1 to the just-deï¬ned B0B1    Bd and to ) = 3+âˆ’1 and to note that with
this ) one has *)â‰¤ . 
3.2. The approximation. Our proposed way to process (7) is as follows.
1. Building the approximation. We start with somehow choosing parameters + > 0, * âˆˆ 01/2 and act as
if we were sure that whenever symmetric nÃ— n matrices B0    Bd satisfy
ArrowB0B1    Bd 0 (22)
then they satisfy the relation
Prob
{
âˆ’+B0 
dâˆ‘
l=1
lBl +B0
}
â‰¥ 1âˆ’* (23)
Speciï¬cally, we replace the chance constraint (8) in (7) with the LMI
Arrow30	x
1	x
    d	x
 0 (24)
where 3 is given by (18), and process the resulting optimization problem, arriving at its feasible solution xâˆ—.
Let us set Bâˆ—0 =30	xâˆ—
Bâˆ—1 =1	xâˆ—
    Bâˆ—d =d	xâˆ—
4 by construction, these matrices satisfy (22). If these
matrices satisfy (23) as well, then by Corollary 3.1, xâˆ— is a feasible solution to the chance-constrained prob-
lem (7). The difï¬culty, however, is that unless we can prove that for +* in question, relation (22) always
implies relation (23), we cannot be sure in advance that the matrices Bâˆ—l satisfy (23) and, consequently, cannot
be sure that xâˆ— is feasible for the chance-constrained problem (7).
In order to overcome this difï¬culty, we use the following validation procedure.
2. Validation procedure. We generate a training sample of N independent realizations 1     N of  and
compute the number M of realizations for which the relation âˆ’+Bâˆ—0 
âˆ‘d
l=1 
i
lB
âˆ—
l +Bâˆ—0 is not satisï¬ed. We then
use these statistics to get a 1âˆ’ -reliable lower bound - on the probability pâˆ— = Probâˆ’+Bâˆ—0 
âˆ‘d
l=1 
i
lB
âˆ—
l 
+Bâˆ—0 speciï¬cally, set
- = min
0â‰¤pâ‰¤1
{
p
Mâˆ‘
i=0
(
N
i
)
1âˆ’pipNâˆ’i â‰¥ 
}

where  âˆˆ 01 is a chosen in advance â€œunreliability levelâ€ (say, = 10âˆ’12). We then check whether - â‰¥ 1âˆ’*4
if it is the case, we claim that the feasibility of xâˆ— for the problem of interest (7) is validated. Otherwise, we apply
our approximation scheme anew, increasing the value of + and/or the value of *.
Proposition 3.1. For the outlined randomized approximation procedure, the probability of xâˆ— being vali-
dated when in fact it is infeasible for (7) is at most .
Proof. It is easily seen that the random quantity - is, with probability at least 1âˆ’, a lower bound on pâˆ—.
Thus, the probability of validating the feasibility of xâˆ— in the case when pâˆ— < 1âˆ’* is at most ; because xâˆ— is
provably feasible for (7), in the case of pâˆ— â‰¥ 1âˆ’*, is indeed safe up to probability of bad sampling â‰¤ . 
The advantage of the outlined validation routine is that when working with * not too close to 0 (and we can
afford to work with any * âˆˆ 01/2, say, * = 025 or * = 01), in the case of
Prob
{
âˆ’+Bâˆ—0 
dâˆ‘
l=1
lB
âˆ—
l +Bâˆ—0
}
â‰¥ 1âˆ’ 08* (25)
(that is, validating an assumption
Prob
{
âˆ’+Bâˆ—0 
dâˆ‘
l=1
lB
âˆ—
l +Bâˆ—0
}
â‰¥ 1âˆ’*
slightly stronger than the one we wish to validate) the cardinality N of the sample which is sufï¬cient to validate,
the feasibility of xâˆ— for (7) with probability 1âˆ’ 7 close to 1 should not be too large. A rough estimate shows
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 7
that it sufï¬ces to take
N â‰¥ 100ln1/+ ln1/7*âˆ’2
With = 7 = 10âˆ’8, * = 025, this formula yields N = 58947; a more accurate computation shows that N = 8750
also will do. It should be stressed that the sample size in question is completely independent of , which therefore
can be arbitrarily small; this is in sharp contrast to what would happen if we were checking the fact that xâˆ— is
feasible for (8) by trying to estimate pxâˆ— (see (8)) by a straightforward Monte Carlo simulation in order to
understand whether indeed pxâˆ—â‰¥ 1âˆ’ . Such a simulation would require a sample of cardinality â‰¥O1/ and
would therefore be completely impractical when  is small, like 10âˆ’6 or less.
3.3. A modiï¬cation. In many applications, it makes sense to pose problem (7) in a slightly different form,
speciï¬cally, as the problem
8âˆ—cÂ¯=max
x8
ï£±ï£´ï£²ï£´ï£³8
F xâ‰¤ 0 cT xâ‰¤ cÂ¯
Prob
{
0	x
+8
dâˆ‘
l=1
ll	x
 0
}
â‰¥ 1âˆ’ 
ï£¼ï£´ï£½ï£´ï£¾  (26)
Thus, instead of minimizing the value of the objective under the deterministic constraints and the chance con-
straint with the â€œreferenceâ€ uncertainty level 8= 1, we are now maximizing the uncertainty level 8 for which
the chance-constrained problem admits a feasible solution with the value of the objective â‰¤ cÂ¯. In reality, we
could, e.g., start with solving the â€œnominalâ€ problem
Opt=min
x
cT x F xâ‰¤ 0 0	x
 0
and then build the â€œtrade-off curveâ€ 9s= 8âˆ—Opt+ s, s > 0, which shows which uncertainty level could be
tolerated given a â€œsacriï¬ceâ€ s > 0 in the optimal value.
The advantage of (26) in our context is that here the safe, tractable approximation given by our approach
does not require any a priori guess of + , *. Indeed, assume that we start with certain + , * which, we believe,
ensure the validity of the implication â€œ(22) â‡’ (23).â€ Acting in exactly the same fashion as above, but aiming
at the problem (26) rather than at the problem (7), we would arrive at the approximation
max
x8
{
8
F xâ‰¤ 0 cT xâ‰¤ cÂ¯
Arrow380	x
1	x
    d	x
 0
}
(27)
where 38 is given by (18) with + replaced with 8+ . Because 38 clearly decreases as 8 grows, we see
that as far as the x-component of an optimal solution to the resulting problem is concerned, this component is
independent of our guesses + , * and coincides with the x-component of the optimal solution to the quasi-convex
(and thus efï¬ciently solvable) optimization problem
min
x3
{
3
F xâ‰¤ 0 cT xâ‰¤ cÂ¯ 3 â‰¥ 0 0	x
 0
Arrow30	x
1	x
    d	x
 0
}
 (28)
The fact that the resulting approximation is independent of any guess on + and * does not resolve all of our
difï¬cultiesâ€”we still need to say what is the â€œfeasibility radiusâ€ 8âˆ—xâˆ— of an optimal (or nearly so) solution xâˆ—
to (28), which we get when solving the latter problem, that is, what is the largest 8= 8âˆ—xâˆ— such that
Prob
{
âˆ’0	xâˆ—
 8
dâˆ‘
l=1
ll	xâˆ—
0	xâˆ—

}
â‰¥ 1âˆ’  (29)
Assume that xâˆ— can be extended by certain 3 to a feasible solution to (28). If the guess we started with were
true, we could take as 8+xâˆ— the supremum of those 8 > 0 for which 38 â‰¥ 3âˆ—xâˆ—, where 3âˆ—xâˆ— is the
smallest 3 â‰¥ 0 such that Arrow30	xâˆ—
1	xâˆ—
    d	xâˆ—
  0 (when xâˆ— is an optimal solution to (28),
3âˆ—xâˆ— is exactly the optimal value in (28)). In the case when we are not sure that our guess is true, we can
build a lower bound 8âˆ—xâˆ— on 8âˆ—xâˆ— via an appropriate modiï¬cation of the validation procedure, speciï¬cally,
as follows.
Assume that 3âˆ—xâˆ— > 0 (this is the only nontrivial case, because 3âˆ—xâˆ— = 0 means that l	xâˆ—
 = 0, l =
1     d; because 0	xâˆ—
 0 due to the constraints in (28), in this case we clearly have 8âˆ—xâˆ—=+). Let us
use the following.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
8 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
Calibration procedure. Given xâˆ—3âˆ—xâˆ— > 0, let B0 = 3âˆ—xâˆ—0	xâˆ—
, Bl = l	xâˆ—
, l = 1     d satisfy
ArrowB0B1    Bd 0. Let, further,  âˆˆ 01 be a desired â€œunreliability levelâ€ of our conclusions (cf. the
Validation procedure). We now carry out the following two steps:
1. Building a grid of values of 8. As we remember from Â§3.1, the implication (22)â‡’ (23) indeed holds true
for â€œsafeâ€ values of + and *, e.g., for * = *s = 025 and + =+s =O1
âˆš
lnn with appropriately chosen O1.
From Corollary 3.1 it follows that if 3s is given by (18) with * = *s and + =+s, then, setting
8s =3s/3âˆ—xâˆ—
we have
Prob
{
âˆ’0	xâˆ—
 8s
dâˆ‘
l=1
ll	xâˆ—
0	xâˆ—

}
â‰¥ 1âˆ’  (30)
Indeed, the matrices B0    Bd satisfy (22) and therefore satisfy (23) with * = *s, + = +s. Applying Corol-
lary 3.1 to the matrices A0 = 3âˆ’1s B0 = 3âˆ’1s 3âˆ—xâˆ—0	xâˆ—
 = 8âˆ’1s 0	xâˆ—
, Al = Bl = l	xâˆ—
, l = 1     d, we
conclude that (30) indeed holds true.
Now let us ï¬nd 8+ â‰¥ 8s such that the relation
Prob
{
âˆ’0	xâˆ—
 8+
dâˆ‘
l=1
ll	xâˆ—
0	xâˆ—

}
â‰¥ 1âˆ’ 
is â€œhighly unlikelyâ€ to be true. For example, assuming  1/2, we can generate a short (say, with L = 100
elements) pilot sample of realizations 1     L of  ; compute, for every iâ‰¤ L, the largest 8= 8i such that the
relation
âˆ’0	xâˆ—
 8i
dâˆ‘
l=1
ill	xâˆ—
0	xâˆ—

holds true; and take, as 8+, the maximum of 8s and of the median of 81     8L.
Finally, we insert into the segment 	8s 8
+
 a moderate number K âˆ’ 2 of â€œintermediateâ€ values of 8, say,
in such a way that the resulting sequence r1 = 8s < r2 < Â· Â· Â· < rK = 8+ forms a geometric progression. This
sequence forms a grid that we are about to use when building 8âˆ—xâˆ—.
2. Running simulations. At this step, we
(i) Generate a training sample of N independent realizations 1     N of  .
(ii) For every k= 1    K compute the integers
Mk =Card
{
iâ‰¤N Â¬
(
âˆ’0	xâˆ—
 rk
dâˆ‘
l=1
ill	xâˆ—
0	xâˆ—

)}
and then the reals
*k =max
{
* âˆˆ 	01

Mkâˆ‘
i=1
(
N
i
)
*i1âˆ’*Nâˆ’i â‰¥ /K
}

Note that if
*k = Prob
{
Â¬
(
âˆ’0	xâˆ—
 rk
dâˆ‘
l=1
ll	xâˆ—
0	xâˆ—

)}

then the probability for the random quantity *k to be <*k is at most /K, so that
Prob *k â‰¥ *k 1â‰¤ kâ‰¤Kâ‰¥ 1âˆ’  (31)
3. Specifying 8âˆ—xâˆ—. In the case of (A.1) we set
8âˆ—xâˆ—= max
1â‰¤kâ‰¤K
{
rk
1+ 4rk3âˆ—xâˆ—
âˆš
lnâˆ’11âˆ’ *kâˆ’1
 *k < 1/2
}
 (32)
and in the case of (A.2) we set
8âˆ—xâˆ—= max
1â‰¤kâ‰¤K
{
rk
1+max	ErfInv/ErfInv *kâˆ’ 10
min	rk3âˆ—xâˆ—ErfInv *k1

 *k < 1/2
}
 (33)
If these formulas are not well deï¬ned (e.g., there is no k such that *k < 1/2) or are well deï¬ned, but result in
8âˆ—xâˆ— < 8s, we set 8âˆ—xâˆ— to the â€œsafeâ€ value 8s.
Note that the quantity 8âˆ—xâˆ— yielded by the calibration procedure is random.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 9
Proposition 3.2. Let xâˆ—3âˆ—xâˆ— > 0 be feasible for (28). Then, with the outlined calibration procedure,
the probability for xâˆ— 8âˆ—xâˆ— to be infeasible for (26) is â‰¤ .
Proof. Assume that *k â‰¥ *k for all k= 1    K (recall that this condition is valid with probability â‰¥ 1âˆ’),
and let us prove that in this case xâˆ— 8âˆ—xâˆ— is feasible for (26). We already know that this is the case when
8âˆ— â‰¡ 8âˆ—xâˆ—= 8s, so that we can restrict ourselves with the case when 8âˆ—xâˆ— is given by a well-deï¬ned formula
((32) in the case of (A.1) or (33) in the case of (A.2)).
In the case of (A.1), let k be such that *k < 1/2 and 8âˆ— = rk/1+ 4rk3âˆ—xâˆ—
âˆš
lnâˆ’11âˆ’ *kâˆ’1 (see (32)),
and let
+k=
1
rk3âˆ—xâˆ—
 3k=
1
+k+4
âˆš
lnâˆ’11âˆ’ *kâˆ’1
 A0=
3âˆ—xâˆ—
3k
0	xâˆ—
 Al=l	xâˆ—
 l=1   d
Then
Arrow3kA0A1    Ad=Arrow3âˆ—xâˆ—0	xâˆ—
1	xâˆ—
    d	xâˆ—
 0
Prob
{
âˆ’+k3kA0ï¸¸ ï¸·ï¸· ï¸¸
râˆ’1k 0	xâˆ—


dâˆ‘
l=1
lAlï¸¸ ï¸·ï¸· ï¸¸
=âˆ‘dl=1 ll 	xâˆ—

+k3kA0
}
= Prob
{
âˆ’0	xâˆ—
 rk
dâˆ‘
l=1
ll	xâˆ—
0	xâˆ—

}
â‰¥ 1âˆ’ *k
where the concluding inequality is valid due to the fact that we are in the case of *k â‰¥ *k. Invoking Corollary 3.1,
we conclude that Probâˆ’A0 
âˆ‘d
l=1 lAl A0â‰¥ 1âˆ’ or, which is the same (due to A0 = 3âˆ—xâˆ—/3k0	xâˆ—
=
1/8âˆ—0	xâˆ—
) as Probâˆ’0	xâˆ—
 8âˆ—
âˆ‘d
l=1l	xâˆ—
0	xâˆ—
â‰¥ 1âˆ’  as claimed.
The result for case (A.2) can be proved in a completely similar way. 
4. Special cases: Diagonal and arrow matrices. In this section, we consider two special cases where
the chance-constrained LMI in (7) possesses a speciï¬c structure which, in principle, allows us to point out
â€œmoderateâ€ + and * which make valid the implication â€œ(22)â‡’ (23),â€ that is, the implication
ArrowB0B1    Bd 0 â‡’ Prob
{
âˆ’+B0 
dâˆ‘
l=1
lBl +B0
}
â‰¥ 1âˆ’* (34)
In particular, using these + , * in the approximation scheme of Â§3.2, we can avoid the necessity of using the
validating procedure.
4.1. Diagonal case. The ï¬rst special case we consider is where 0	x
1	x
    d	x
 in (7) are diagonal
matrices. We refer to this situation as the diagonal case. Note that in spite of its simplicity, this case is of
deï¬nite interest: It is the case of chance-constrained system of linear inequalitiesâ€”the entity of primary interest
for chance-constrained linear programming. We start with the following observation:
Lemma 4.1. Let  âˆˆRd be a random vector and Bl =DiagB1l     Bsl , l= 01     d, be block-diagonal
matrices of common block-diagonal structure. Assume that for certain function +*, * âˆˆ 01/2, and every
j â‰¤ s the structure of the blocks Bjl ensures the implication
âˆ€* âˆˆ 01/2 ArrowBj0    Bjd 0 â‡’ Prob
{
âˆ’+*Bj0 
dâˆ‘
l=1
lB
j
l +*Bj0
}
â‰¥ 1âˆ’*
Then one has
âˆ€* âˆˆ 01/2 ArrowB0    Bd 0 â‡’ Prob
{
âˆ’+*/sB0 
dâˆ‘
l=1
lBl +*/sB0
}
â‰¥ 1âˆ’*
This statement is an immediate consequence of the fact that ArrowB0    Bd  0 if and only if
ArrowBj0    B
j
d 0 for every j = 1     s.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
10 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
Theorem 4.1. Let B0B1    Bd be diagonal nÃ— n matrices satisfying ArrowB0B1    Bd  0, and
1     d be random variables satisfying the assumption
(A.3) 1     d are independent, all with zero mean, and Eexp
2
l â‰¤ exp1, 1â‰¤ lâ‰¤ d
(note that (A.3) is implied by (A.1)). Then the implication (34) holds true for every * âˆˆ 01/2 with
+ =+n*= 1
3
âˆš
38 ln2n/*
If, in addition to (A.3), the entries in  are symmetrically distributed, then the above conclusion remains valid
with
+ =+nS *=
âˆš
3 ln2n/*
Finally, if  satisï¬es (A.2), then the same conclusion remains valid with
+ =+nG *= ErfInv*/2nâ‰¤
âˆš
2 lnn/*
Proof. By Lemma 4.1, it sufï¬ces to prove the statement in the scalar case n = 1, where the relation
ArrowB0    Bd 0 means simply that B0 â‰¥
âˆšâˆ‘d
l=1B
2
l . There is nothing to prove when B0 = 0; assuming
B0 > 0 and setting hl = Bl/B0, all we need is to prove that whenever  satisï¬es (A.3) and h âˆˆRd is deterministic,
then
h2 â‰¤ 1 â‡’ Prob
{âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
lhl
âˆ£âˆ£âˆ£âˆ£>+*}â‰¤ * 0<* < 1/2 (35)
where +Â·, depending on the situation, is either +1Â·, or +1S Â·, or +1G Â·. This result is readily given by
standard facts on large deviations; to make the presentation self-contained, here is the demonstration. All we
need is to prove that if h âˆˆRd, h2 â‰¤ 1, then
âˆ€+ > 0 Prob
{âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
hll
âˆ£âˆ£âˆ£âˆ£>+}â‰¤
ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³
2 expâˆ’9+ 2/38  satisï¬es (A.3)
2 expâˆ’+ 2/3  satisï¬es (A.3) and is
symmetrically distributed
2>+  âˆ¼ 0 Id
(36)
where >s= âˆ« 
s
2-âˆ’1/2 expâˆ’r2/2dr is the error function.
The case of  âˆ¼  0 Id is evident. Now assume that  satisï¬es (A.3). Let ) âˆˆ R, sl =
âˆ‘l
r=1 )hrr , and
J = l hl)>
âˆš
3/2. We have
Eexpsl=Eexpslâˆ’1 exp)hll=Eexpsl Â·?l ?l =E)hll (37)
(we have taken into account that l is independent of slâˆ’1). We claim that
?l â‰¤
{
exp2)2h2l /3 l âˆˆ J
exp7/12+ 2)2h2l /3 l âˆˆ J 
(38)
Indeed, it is easily seen that
exptâ‰¤ t+ exp2t2/3
for all t âˆˆ R, whence Eexp)hll â‰¤ Eexp2)2h2l 2l /3; when l âˆˆ J , the latter expectation is at most
Eexp2l 
2)2h2l /3 by HÃ¶lder inequality, as required in (38). Now let l âˆˆ J . We have )hls â‰¤ s2+ )2h2l /4 for
all s, whence
Eexp)hll â‰¤ exp)2h2l /4Eexp2l â‰¤ exp1+)2h2l /4
â‰¤ exp7/12+ 2)2h2l /3
as required in (38).
Combining (37) and (38), we get
E
{
exp
{
)
dâˆ‘
l=1
hll
}}
â‰¤ exp
{
2)2
[ dâˆ‘
l=1
h2l
]/
3
}
exp7/12CardJ 
â‰¤ exp2)2/3 exp7/12 Â· 2/3 Â·)2
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 11
where the concluding inequality follows from the facts that h2 â‰¤ 1 and that h2l > 3/2)2 when l âˆˆ J , which
combines with h2 â‰¤ 1 to imply that CardJ â‰¤ 2)2/3. Thus,
E
{
exp
{
)
dâˆ‘
l=1
hll
}}
â‰¤ exp19)2/18
whence, by Tschebyshev inequality,
Prob
{âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
hll
âˆ£âˆ£âˆ£âˆ£>+}â‰¤ 2min)>0 exp19)2/18âˆ’)+= 2 expâˆ’9+ 2/38
Now let  satisfy (A.3) and be symmetrically distributed. For ) > 0, let us set sl = cosh)
âˆ‘l
r=1 hrr. Then
Esl=E
{
slâˆ’1 cosh)hll+ sinh
(
)
lâˆ’1âˆ‘
r=1
hrr
)
sinh)hll
}
=Eslâˆ’1Ecosh)hllï¸¸ ï¸·ï¸· ï¸¸
?l

whence
Esd=?1 Â·    Â·?d
Setting J = l )2h2l â‰¤ 2 and taking into account that coshtâ‰¤ expt2/2 for all t, for l âˆˆ J we have
?l =Ecosh)hllâ‰¤Eexp)2h2l 2l /2â‰¤ exp)2h2l /2
where the concluding inequality is given by the facts that )2h2l /2â‰¤ 1 and Eexp2l â‰¤ exp1 in view of the
HÃ¶lder inequality. When l âˆˆ J , we, the same as above, have
cosh)hllâ‰¤ exp)hllâ‰¤ exp2+)2h2l /4
whence ?l â‰¤ exp1+)2h2l /4â‰¤ exp1/2+)2h2l /2. We therefore get
E
{
cosh
(
)
dâˆ‘
l=1
hll
)}
â‰¤ exp
{
)2
[ dâˆ‘
l=1
h2l
]/
2
}
expCardJ /2
and, similarly to the previous case, CardJ â‰¤ )2/2, whence
E
{
cosh
(
)
dâˆ‘
l=1
hll
)}
â‰¤ exp3)2/4
When âˆ‘dl=1 hll>+ , we have cosh)âˆ‘dl=1 hll > exp)+/2, so that
Prob
{âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
hll
âˆ£âˆ£âˆ£âˆ£>+}â‰¤ 2 inf)>0 exp3)2/4âˆ’)+= 2 expâˆ’+ 2/3
as required in (36). 
Comparison with other approximations of a chance-constrained LP. As mentioned earlier, the diagonal
case arises when solving chance-constrained linear programming problems that we prefer to pose in the form
of (26):
max
x8
{
8
Fxâˆ’ f â‰¥ 0 cT xâ‰¤ cÂ¯
ProbAxâˆ’ b â‰¥ 0â‰¥ 1âˆ’ 
}
 	A b 
= 	A0 b0
+8
dâˆ‘
l=1
l	A
l bl

 
max
x8
ï£±ï£´ï£´ï£²ï£´ï£´ï£³8
Fxâˆ’ f â‰¥ 0 cT xâ‰¤ cÂ¯
Prob
{
0	x
+8
dâˆ‘
l=1
ll	x
 0
}
â‰¥ 1âˆ’ 
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾  l	x
=DiagAlxâˆ’ bl 0â‰¤ lâ‰¤ d
(39)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
12 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
With our approximation scheme, the safe, tractable approximation of the resulting chance-constrained problem is,
as it is immediately seen, the quasi-convex program
max
x8
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³8
Fxâˆ’ f â‰¥ 0 cT xâ‰¤ cÂ¯
8
âˆšâˆšâˆšâˆš dâˆ‘
l=1
[
bli +
Jâˆ‘
j=1
Alijxj
]2
â‰¤âˆ‘
j
A0ijxj âˆ’ b0i  1â‰¤ iâ‰¤ I
ï£¼ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£¾ (40)
where I J are the row and the column sizes of Al. There also exists a more traditional â€œconstraint-by-constraintâ€
way to process a chance constrained LP; speciï¬cally, we somehow choose positive i,
âˆ‘
i i = , and safely
approximate (39) with the chance-constrained problem
max
x8
ï£±ï£´ï£´ï£²ï£´ï£´ï£³8
Fxâˆ’ f â‰¥ 0 cT xâ‰¤ cÂ¯
Prob
{âˆ‘
j
A0ijxj âˆ’ b0i +8
dâˆ‘
l=1
l
[âˆ‘
j
Alijxj âˆ’ bli
]
â‰¥ 0
}
â‰¥ 1âˆ’ i 1â‰¤ iâ‰¤ I
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾  (41)
This problem involves chance-constrained scalar linear inequalities that are much easier to approximate than
the original chance-constrained vector inequality appearing in (39). For the sake of simplicity, consider the case
when  âˆ¼ 0 I and  < 1/2. In this case, (41) is exactly equivalent to the explicit quasi-convex problem
max
x8
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³8
Fxâˆ’ f â‰¥ 0 cT xâ‰¤ cÂ¯
ErfInvi8
âˆšâˆšâˆšâˆš dâˆ‘
l=1
[
bli +
Jâˆ‘
j=1
Alijxj
]2
â‰¤âˆ‘
j
A0ijxj âˆ’ b0i  1â‰¤ iâ‰¤ I
ï£¼ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£¾  (42)
Note that an attempt to treat the parameters i of our construction as decision variables in (42) failsâ€”the resulting
problem loses convexity; this is why the parameters i should be chosen in advance, and the most natural way
to choose them is to set i = /I , i = 1     I . Note that with this choice of i, problem (42) is equivalent
to (40), up to rescaling 8 â†’ 8/ErfInv/I. This, however, does not mean that the approximations are identical;
although both of them lead to the same optimal decision vector xâˆ—, they differ in what is the resulting lower
bound 8âˆ— on the true feasibility radius 8âˆ—xâˆ— of xâˆ— (recall that this radius is the largest 8 for which xâˆ— 8 is
feasible for the chance-constrained problem of interest (39)). Speciï¬cally, for approximation (42), 8âˆ— is exactly
the optimal value of the approximation, whereas for (40) 8âˆ— is given by the calibration routine. Experiments
show that which of these two lower bounds is less conservative depends on the problemâ€™s data, so that in practice
it makes sense to build both these bounds and to use the larger of them.
4.2. Arrow case. We are about to justify the implication (34) in the Arrow case, where the matrices Bl,
l= 1     d, are of the form
Bl = 	ef Tl + fleT 
+lG (43)
where e fl âˆˆRn, l âˆˆR, and G âˆˆ Sn. We meet this case in the chance-constrained conic quadratic optimization;
see (6). Indeed, the matrices l	x
, 1 â‰¤ l â‰¤ d, arising in (6) are, for every x, matrices of the form (43).
Therefore, all we need when building and processing the safe tractable approximation, as developed in Â§3.2 for
the chance-constrained LMI in (6), is the validity of (34) for matrices Bl of the form (43).
Theorem 4.2. Let the n Ã— n matrices B1    Bd of the form (43) along with a matrix B0 âˆˆ Sn satisfy
the premise in (34). Let, further, 1     d be independent random variables with zero means and such that
E2l  â‰¤ A2, l = 1     d (note that in the cases of (A.1) and (A.2), one can take A = 1, and in the case of
(A.3) one can take A =âˆšexp1âˆ’ 1). Then, for every * âˆˆ 01/2 and with +* given by
a 2A
âˆš
2/* [general case]
b min	2
âˆš
2/*4+ 4âˆšln2/*
 [case (A.1)]
c 4+ErfInv* [case (A.2)]
(44)
one has
+ â‰¥+* â‡’ Prob
{
âˆ’+B0 
dâˆ‘
l=1
lBl +B0
}
â‰¥ 1âˆ’* (45)
that is, with our +*, the implication in (34) holds true.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 13
Proof. First of all, when l, l = 1     d, satisfy (A.3), we indeed have E2l  â‰¤ exp1 âˆ’ 1
due to t2 â‰¤ expt2âˆ’ 1 for all t. Further, by continuity argument, it sufï¬ces to consider the case where
ArrowB0B1    Bd 0 and B0  0
In this case, setting Al = Bâˆ’1/20 BlBâˆ’1/20 , the relation ArrowB0    Bd  0 is equivalent to
âˆ‘d
l=1A
2
l  I , and
the target relation (45) is equivalent to
+ â‰¥+* â‡’ Prob
{
âˆ’+In 
dâˆ‘
l=1
lAl +In
}
â‰¥ 1âˆ’*
with +* given by (44). Thus, all we need to prove is the following.
Lemma 4.2. Let Bl, l= 1     d, be of the form of (43), let B0  0, and let the matrices Al = Bâˆ’1/20 BlBâˆ’1/20
satisfy
âˆ‘
l A
2
l  I . Let l, further, satisfy the premise in Theorem 4.2. Then, for every * âˆˆ 01/2, one has
Prob
{âˆ¥âˆ¥âˆ¥âˆ¥ dâˆ‘
l=1
lBl
âˆ¥âˆ¥âˆ¥âˆ¥â‰¤+*}â‰¥ 1âˆ’* (46)
where  Â·  is the standard matrix norm (the largest singular value) and +* is given by (44).
Proof of Lemma 4.2. Observe that Al, 1â‰¤ lâ‰¤ d are also of the form (43):
Al = 	ghTl +hlgT 
+lH 	g = Bâˆ’1/20 e hl = Bâˆ’1/20 fl H = Bâˆ’1/20 GBâˆ’1/20 

Note that by rescaling hl we can ensure that g2 = 1, and then rotate the coordinates to make g the ï¬rst basic
orth. In this situation, matrices Al become matrices of the form
Al =
[
ql r
T
l
rl lQ
]
 (47)
Finally, by appropriate scaling of l, we can ensure that Q = 1. We have
A2l =
[
q2l + rTl rl qlrTl +lrTl Q
qlrl+lQrl rlrTl +2l Q2
]

We conclude that
âˆ‘d
l=1A
2
l  In implies that
âˆ‘d
l=1q
2
l + rTl rlâ‰¤ 1 and 	
âˆ‘d
l=1 
2
l 
Q
2  Inâˆ’1; because Q2 = 1, we
arrive at the relations
a
dâˆ‘
l=1
2l â‰¤ 1 b
dâˆ‘
l=1
q2l + rTl rlâ‰¤ 1 (48)
Now let pl = 0 rTl T âˆˆRn. We have
S â‰¡
dâˆ‘
l=1
lAl =
[
g
( dâˆ‘
l=1
lplï¸¸ ï¸·ï¸· ï¸¸
E
)T
+ EgT
]
+Diag
{
dâˆ‘
l=1
lqlï¸¸ ï¸·ï¸· ï¸¸
F

( dâˆ‘
l=1
llï¸¸ ï¸·ï¸· ï¸¸
G
)
Q
}
â‡’ S â‰¤ gET + EgT +max	F GQ
= E2+max	F G

Setting
H=
dâˆ‘
l=1
rTl rl I=
dâˆ‘
l=1
q2l 
we have H+Iâ‰¤ 1 by (48.b). Besides this,
EET E = âˆ‘
l lâ€²
Ellâ€²p
T
l plâ€² =
dâˆ‘
l=1
E2l r
T
l rl 	l are independent, El= 0

â‰¤ A2
dâˆ‘
l=1
rTl rl â‰¤ A2H
â‡’ ProbE2 > tâ‰¤
A2H
t2
âˆ€ t > 0 	Tschebyshev inequality

Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
14 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
EG2 =
dâˆ‘
l=1
E2l 
2
l â‰¤ A2
dâˆ‘
l=1
2l â‰¤ A2 	see (48.a)

â‡’ ProbG> tâ‰¤ A
2
t2
âˆ€ t > 0 	Tschebyshev inequality

EF2 =
dâˆ‘
l=1
E2l q
2
l â‰¤ A2I
â‡’ ProbF> tâ‰¤ A
2I
t2
âˆ€ t > 0 	Tschebyshev inequality

Thus, for every + > 0 and all  âˆˆ 01 we have
ProbS>+ â‰¤ ProbE2+max	F G
 > +â‰¤ ProbE2 >+
+ProbF> 1âˆ’++ProbG> 1âˆ’+â‰¤ A
2
+ 2
[
H
2
+ I+ 1
1âˆ’2
]

whence, due to H+Iâ‰¤ 1, one has
ProbS>+â‰¤ A
2
+ 2
max
Hâˆˆ	01

min
âˆˆ01
[
H
2
+ 2âˆ’H
1âˆ’2
]
= 8A
2
+ 2

so that
+ â‰¥ 2Aâˆš2/* â‡’ ProbS>+â‰¤ * (49)
which is the â€œgeneral caseâ€ of our lemma (cf. (44.a)). It remains to justify the reï¬nements in the cases of (A.1)
and (A.2). In the case of (A.1), we have A â‰¤ 1 so that whenever "+ > 4, we have ProbS â‰¥ "+< 1/2 by (49).
Invoking Theorem 3.1, we conclude that for all ) â‰¥ 1 we have ProbS â‰¥ ) "+ â‰¤ 2 expâˆ’"+ 2) âˆ’ 12/16.
Given * âˆˆ 01/2 and setting ) = 1 + 4"+âˆ’1âˆšln2/*, we get ProbS â‰¥ "+ + 4âˆšln2/* â‰¤ *; because
this relation holds true for every "+ > 4, we see that, in addition to (49), ProbS â‰¥ 4 + 4âˆšln2/* â‰¤ *,
0< * < 1/2, which proves the â€œ(A.1)-versionâ€ of the lemma. Now let (A.2) be the case. Here (49) is satisï¬ed
with A = 1, meaning that whenever s âˆˆ 01/2, we have ProbS â‰¥ 2âˆš2/sâ‰¤ s. Applying Theorem 3.1 with
s in the role of *, we conclude that whenever s âˆˆ 01/2 and ) â‰¥ 1, we have
Prob
{S â‰¥ 2)âˆš2/s}â‰¤ Erf(ErfInvs+ )âˆ’ 1max	2âˆš2/s ErfInvs
)
It follows that setting
+âˆ—*= inf
s)
{
2)
âˆš
2/s
s âˆˆ 01/2 ) â‰¥ 1
ErfInvs+ )âˆ’ 1max	2âˆš2/s ErfInvs
â‰¥ ErfInv*
}

we ensure the relation ProbS â‰¥+âˆ—*â‰¤ * for all * âˆˆ 01/2. It is immediately seen that +*, given in
(44) for case (A.2), is an upper bound on +âˆ—*, so that (46) holds true in the case of (A.2). 
4.3. Simulation-free safe, tractable approximations of chance-constrained LMIs. Assume that the struc-
ture of LMI (8) ensures that the collections of matrices F0	x
1	x
    d	x
, for all x and all F â‰¥ 0, belong
to a set  with the following property:
(P) We can point out functions +1*, +2*, 0 < * < 1/2, such that whenever a collection of matrices
B0B1    Bd belongs to  and satisï¬es the condition ArrowB0B1    Bd 0, we have
âˆ€0<* < 1/2 Prob
{
âˆ’+1*B0 
dâˆ‘
l=1
lBl +1*B0
}
â‰¥ 1âˆ’* whenever  satisï¬es (A.1)4
âˆ€0<* < 1/2 Prob
{
âˆ’+2*B0 
dâˆ‘
l=1
lBl +2*B0
}
â‰¥ 1âˆ’* whenever  satisï¬es (A.2).
(50)
For example,
â€¢ using some deep results from functional analysisâ€”the â€œnoncommutative Khintchine inequalityâ€
(Buchholz [7]), it can be easily veriï¬ed that P is true for all matrices AA1    Ad, provided that +12*=
O1
âˆš
lnn/*; see Proposition A.1 in the appendix or Man-Cho So [12]. The same is true when  is comprised
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 15
of all collections of diagonal nÃ— n matrices, see Theorem 4.1, and it is easily seen that in the latter case the
outlined value of +* is, up to an O1 factor, the smallest possible;
â€¢ restricting  to be all collections B0B1    Bd of symmetric nÃ—n matrices with B1    Bd of the form
eT fl+ f Tl e+lG, (P), it was shown in Theorem 4.2 that (50) is satisï¬ed with +12*=O1
âˆš
ln1/*.
In the case of (P), we can build safe, tractable approximations of our problems of interest (7) and (26), avoiding
the necessity to use simulations. Speciï¬cally, combining (50) with Corollary 3.1, we see that the problem
min
x
{
cT x
F xâ‰¤ 0
Arrow?0	x
1	x
    d	x
 0
}

?âˆ’1 =
ï£±ï£´ï£²ï£´ï£³
inf
0<*<1/2
[
+1*+ 4
âˆš
lnâˆ’11âˆ’*âˆ’1
]
 case of (A.1)
inf
0<*<1/2
	+2*+max	ErfInv/ErfInv*âˆ’ 10
min	+2*ErfInv*

  case of (A.2)
(51)
is a safe, tractable approximation of (7).
By exactly the same reasons, given a feasible solution xâˆ—3âˆ— > 0 to (28) and setting 8âˆ— = ?/3âˆ—, with ?
given by (51), we ensure that xâˆ— 8âˆ— is a feasible solution to (26).
It is not difï¬cult to see that in the cases of chance-constrained linear and conic quadratic programming (covered
by Theorems 4.1 and 4.2, respectively), the corresponding â€œsimulation-freeâ€ safe, tractable approximations are
not too conservative. For example, in the case of (A.2) there exists an absolute constant C > 0 such that a
vector x that does not satisfy the constraint ArrowCâˆ’1?0	x
1	x
    d	x
  0 does not necessarily
satisfy the chance constraint of interest (8), provided that nâ‰¤ 1. However, we shall see in Â§6 that in practice
simulation-based approximations can be signiï¬cantly less conservative than the simulation-free ones.
5. Majorization. One way to bound from above the probability of violating a randomly perturbed LMI:
qx = Prob
{
0	x
+
dâˆ‘
l=1
ll	x
  0
}

is to replace the random perturbations  with easier-to-handle perturbations Ë†â€”to which we know how to bound
from above the quantity
qx = Prob
{
0	x
+
dâˆ‘
l=1
Ë†ll	x
  0
}

If in addition Ë† is â€œmore diffuseâ€ than  , meaning that qxâ‰¥ qx for all x, we indeed end up with a bounding
scheme for qÂ·. For example, let the entries in  be independent with zero means and unbounded ranges. With
our present results, we cannot handle this situation unless l are Gaussian. In order to overcome this difï¬culty,
we could replace l with â€œmore diffuseâ€ Gaussian random variables Ë†l, which we do know how to handle.
For the above idea to be meaningful we should properly specify the notion of â€œbeing more diffuse.â€
We are about to present two speciï¬cations of this type, known as monotone and convex stochastic dominances,
respectively.
5.1. Monotone dominance and comparison theorem. For our purposes, it sufï¬ces to restrict ourselves
with monotone dominance on the space  of all symmetric w.r.t. 0 and unimodal probability distributions on
the axis. The latter notion is deï¬ned as follows:
Deï¬nition 5.1. A probability distribution P on the axis is called unimodal and symmetric if P possesses a
density pÂ· that is an even function nonincreasing on 	0.1
A probability distribution P âˆˆ  is said to be monotonically dominating another distribution Q âˆˆ 
(notation: P m Q, or, equivalently, Q m P ), if
âˆ« 
t
dPs â‰¥ âˆ« 
t
dQs for every t â‰¥ 0, or, equivalently,2âˆ«
f sdPsâ‰¥ âˆ« f sdQs for every even and bounded function f s that is nondecreasing on the nonnegative
ray R+.
1 In literature, a unimodal symmetric distribution is deï¬ned as a convex combination of the unit mass sitting at the origin and of what is
called unimodal and symmetric in Deï¬nition 5.1. For the sake of simplicity, we forbid a mass at the origin; note that all results to follow
remain valid when such a mass is allowed.
2 This equivalence is well known; to be self-contained, we present the proof in the appendix.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
16 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
With a slight abuse of notation, if E is a random variable with distribution P and probability density pÂ·,
then every one of the relations E âˆˆ , pÂ· âˆˆ  is interpreted as the inclusion P âˆˆ . Similarly, if E, G
are random variables with distributions P , respectively, Q, and probability densities pÂ·, respectively, qÂ·, then
every one of the relations Gm E, qÂ·m pÂ· means that PQ âˆˆ and P m Q. Relation m is the natural
â€œcounterpartâ€ of the relation m.
Important facts on the monotone dominance that we need later are summarized in:
Proposition 5.1. (i) m is a partial order on .
(ii) If piÂ·m qiÂ·, i= 1     I , and Hi â‰¥ 0 are such that
âˆ‘
i Hi = 1, then
âˆ‘
i HipiÂ·m
âˆ‘
i HiqiÂ·.
(iii) If E âˆˆ is a random variable, and ,  â‰¥ 1, is a deterministic real, then E m E.
(iv) If piÂ· âˆˆ weakly converge as iâ†’ to a probability density pÂ· (meaning that
âˆ«
gspisdsâ†’âˆ«
gspsds for every continuous g with compact support), qiÂ· âˆˆ  weakly converge as i â†’  to a
probability density qÂ· and piÂ·m qiÂ· for every i, then pÂ· âˆˆ, qÂ· âˆˆ, and pÂ·m qÂ·.
(v) If El âˆˆ nl=1, Gl âˆˆ nl=1 are collections of independent random variables such that El m Gl,
l= 1     n, and l, l= 1     n, are deterministic reals, then
âˆ‘n
l=1 lEl m
âˆ‘m
l=1 lGl
(vi) Let E âˆˆ  be supported on 	âˆ’11
,  be uniformly distributed on 	âˆ’11
, and G âˆ¼  02/-. Then
E m  m G.
(vii) [Comparison Theorem] Let l âˆˆ dl=1, Ë†dl=1 be two collections of independent random variables
such that l m Ë†l for all l. Then for every closed convex and symmetric w.r.t. the origin set QâŠ‚Rd, one has
Prob = 	14    4 d
 âˆˆQâ‰¥ ProbË† = 	Ë†14    4 Ë†d
 âˆˆQ
To the best of our knowledge, some of the facts presented in Proposition 5.1, most notably the comparison
theorem, are new; to be on the safe side, we provide full proofs of all these facts in the appendix.
5.2. Convex dominance and the majorization theorem. To conclude this section, we present another
â€œGaussian majorizationâ€ result. Its advantage is that the random variables l are not required to be symmetrically
or unimodally distributed; what is needed, essentially, is just independence plus zero means. We start with
recalling the deï¬nition of convex dominance. Let n be the space of Borel probability distributions on R
n with
zero mean. For a random variable G taking values in Rn, we denote by PG the corresponding distribution, and
we write G âˆˆn to express that PG âˆˆn. Let 	
n be the set of all convex function f on Rn with linear growth,
meaning that there exists cf < such that f u â‰¤ cf 1+u2 for all u.
Deï¬nition 5.2. Let E G âˆˆn. We say that G convexly dominates E (notation: E c G, or PE c PG, or
Gc E, or PG c PE) if âˆ«
f udPEuâ‰¤
âˆ«
f udPGu
for every f âˆˆ	
n.
The relevant facts on convex dominance that we need are summarized in:
Proposition 5.2. (i) c is a partial order on n.
(ii) If P1     PkQ1    Qk âˆˆn, and Pi c Qi for every i, then
âˆ‘
i
iPi c
âˆ‘
i
iQi for all nonnegative
weights i with unit sum.
(iii) If E1     EkG1    Gk âˆˆn are independent random variables such that Ei c Gi for every i, and si
are deterministic reals, then
âˆ‘
i
siEi c
âˆ‘
i
siGi.
(iv) If E is symmetrically distributed w.r.t. 0 and t â‰¥ 1 is deterministic, then tE c E.
(v) Let P1Q1 âˆˆ r , P2Q2 âˆˆ s be such that Pi c Qi, i = 12. Then P1 Ã— P2 c Q1 Ã— Q2. In particu-
lar, if E1     EnG1    Gn âˆˆ1 are independent and such that Ei c Gi for every i, then E1     EnT c
G1    Gn
T .
(vi) Let E âˆˆ1 be supported on 	âˆ’11
 and Gâˆ¼ 0-/2. Then E c G.
(vii) Assume that E âˆˆ n is supported in the unit cube u u â‰¤ 1 and is â€œabsolutely symmetrically
distributed,â€ meaning that if J is a diagonal matrix with diagonal entries Â±1, then JE has the same distribution
as E. Also let Gâˆ¼ 0 -/2In. Then E c G.
(viii) Let EG âˆˆn, E âˆ¼ 0K, Gâˆ¼ 0? with K?. Then E c G.
(ix) [Majorization theorem] Let Gâˆ¼ 0 Id, and let  âˆˆd be such that  c G. Let, further, QâŠ‚Rd be a
closed convex set such that
* â‰¡ ProbG âˆˆQ< 1/2
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 17
Then for every ) > 1, one has
Prob âˆˆ )Qâ‰¤ inf
1â‰¤I<)
1
)âˆ’I
âˆ« 
I
ErfsErfInv*ds â‰¤ inf
1â‰¤I<)
1
2)âˆ’I
âˆ« 
I
exp
{
âˆ’ s
2ErfInv2*
2
}
ds (52)
All of the above facts, except for the Majorization Theorem, are well known; proofs can be found in
Nemirovski and Shapiro [14]. The present Majorization Theorem is a slight reï¬nement of what is called
â€œMajorization Theoremâ€ in Nemirovski and Shapiro [14]; the proof of this reï¬nement is given in the appendix.
5.3. Calibration-based on Gaussian majorization. We can utilize the preceding facts in the calibration
procedure as follows.
Utilizing comparison theorem. Assume that the perturbations l are independent and possess unimodal and
symmetric distributions Pl such that Pl m  0A2 for certain A and all l (the latter is, e.g., the case when
l are supported on 	âˆ’11
 and A =
âˆš
2/-; see Proposition 5.1.(vi)). Setting G âˆ¼  0 Id and invoking the
comparison theorem, we conclude that for every deterministic symmetric matrices A0A1    Ad and every
r > 0 we have
Prob
{
âˆ’A0 
r
A
dâˆ‘
l=1
lAl A0
}
â‰¥ Prob
{
âˆ’A0  r
dâˆ‘
l=1
GlAl A0
}
 (53)
Given matrices A0    Ad and 3âˆ— > 0 such that Arrow3âˆ—A0A1    Ad  0, along with  âˆˆ 01, the
purpose of the calibration procedure is to build a (random) 1âˆ’ -reliable lower bound on the quantity
8âˆ— =max
{
8 Prob
{
âˆ’A0  8
dâˆ‘
l=1
lAl A0
}
â‰¥ 1âˆ’ 
}
 (54)
By (53), in order to build such a bound, we can apply the plain calibration procedure to ï¬nd a 1âˆ’ -reliable
lower bound râˆ— on the quantity
râˆ— =max
{
r Prob
{
âˆ’A0  r
dâˆ‘
l=1
GlAl A0
}
â‰¥ 1âˆ’ 
}
and to set 8âˆ— = râˆ—/A . This approach allows us to extend the above constructions beyond the scope of Assump-
tion A; moreover, we shall see in Â§6 that this approach makes sense even in the case when  obeys (A.1) and
thus can be processed â€œas it is.â€ The reason is that the constant factors in the measure concentration inequalities
of Theorem 3.1 in the case of (A.2) are better than in the case of (A.1).
Utilizing majorization theorem. Now assume that the random variables 1     d are independent with
zero means, and that we can point out A > 0 such that Pl c  0A2. Introducing Gâˆ¼ 0 Id and applying
Proposition 5.2.(v), we conclude that  c AG. Given the input A0    Ad, ,  to the calibration procedure
and applying Majorization Theorem to the closed convex set
Q=Qs =
{
u âˆˆRd âˆ’ sA0 
dâˆ‘
l=1
ulAl  sA0
}

we conclude that
âˆ€
(
s > 0 *sâ‰¡ ProbG âˆˆQsâ‰¡ 1âˆ’Prob
{
âˆ’A0  sâˆ’1
dâˆ‘
l=1
GlAl A0
}
< 1/2
)

Prob
{
âˆ’)sAA0 
dâˆ‘
l=1
lAl  )AsA0
}
= 1âˆ’ProbAâˆ’1 âˆˆ )Qsâ‰¥ 1âˆ’L)*s
L)*= inf
1â‰¤I<)
1
)âˆ’I
âˆ« 
I
ErfsErfInv*ds
(55)
In order to bound from below 8âˆ— (see (54)), we apply the calibration procedure with artiï¬cial random pertur-
bation G in the role of actual perturbation  . Carrying out the ï¬rst two steps of this procedure, we end up with
a collection rk > 0 *k < 1/2 %Kk=1 such that â€œup to probability of bad sampling â‰¤ â€ we have, for 1â‰¤ kâ‰¤ %K,
*k â‰¥ *k = Prob
{
Â¬
(
âˆ’rkâˆ’1A0 
dâˆ‘
l=1
GlAl  rkâˆ’1A0
)}
= ProbG âˆˆQsk sk = 1/rk4
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
18 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
this collection is obtained from the collection rk *kKk=1 built at step 2 of the procedure by discarding the pairs
with *k â‰¥ 1/2. Setting
8âˆ— = max
1â‰¤kâ‰¤ %K
rk
A)k
 )k =min) â‰¥ 1 L ) *kâ‰¤  k= 1     %K
and invoking (55), we see that 8âˆ— is a lower bound on 8âˆ—, provided that *k â‰¤ *k, 1 â‰¤ k â‰¤ %K, which happens
with probability at least 1âˆ’ .
Note that with straightforward modiï¬cations, Gaussian majorization can be used in the validation procedure.
6. Numerical illustrations. In the following illustrations, we focus on problem (26) and on its safe tractable
approximation given by (28) and the calibration procedure.
6.1. The calibration procedure. We start with illustrating the â€œstand-aloneâ€ calibration procedure. Recall
that this procedure is aimed at building 1âˆ’ -reliable lower bound 8âˆ— on the quantity
8âˆ— =max
{
8 p8 = Prob
{
âˆ’A0  8
dâˆ‘
l=1
lAl A0
}
â‰¥ 1âˆ’ 
}
 (56)
where A0A1    Ad are given symmetric nÃ— n matrices such that Arrow3âˆ—A0A1    Ad 0 for a given
3âˆ— > 0.
The questions we tried to answer in our experiments were as follows:
(i) What is the better strategy to be used in the procedureâ€”the plain calibration procedure (PCP) or the
Gaussian majorization version (GCP) of this procedure?
(ii) As we have seen in Â§4.3, there are situations where not too conservative guaranteed lower bounds on 8âˆ—
can be built without simulations at all. Are these â€œ100% reliableâ€ lower bounds more attractive than those given
by calibration procedure?
(iii) From a practical perspective, how conservative is the calibration procedure?
Answers to these questions, based on our rather intensive numerical experimentation, are as follows:
â€¢ The calibration procedure, at least its GCP-version, signiï¬cantly outperforms the simulation-free lower
bounding;
â€¢ GCP signiï¬cantly outperforms PCP;
â€¢ The conservatism of the calibration procedure is not very severe: the ratio 8âˆ—/8âˆ— is usually well within one
order of magnitude.
These observations are summarized in Table 1; they are based on experiments performed as follows: We ran-
domly generate d = 32 matrices A1    Ad of size 32 Ã— 32 and of prescribed structure, speciï¬cally, full
(â€œgeneral caseâ€), diagonal (â€œdiagonal caseâ€), and of the form[
f T
f
]
+I32
f being a vector (â€œarrow caseâ€), and scale the generated matrices to ensure that ArrowFI32A1    Ad 0 if
and only if F â‰¥ 1; the input to the calibration procedure is the collection A0 = I32, A1    A32, 3âˆ— = 1. Data in
Table 1 correspond to 100,000-element training sample. Note that although the performance of the calibration
procedure somehow improves when the sample size grows (see Table 2), this phenomenon is rather moderate.
6.2. Illustration: Chance-constrained truss topology design. A truss is a mechanical construction com-
prised of thin elastic bars linked with each other at nodes. In the simplest Truss topology design (TTD) problem,
one is given a ï¬nite 2D or 3D nodal set, a list of allowed pair connections of nodes by bars, and an external
loadâ€”a collection of forces acting at the nodes. The goal is to assign the tentative bars weights, summing up to
a given constant, in order to get a truss most rigid w.r.t. the load (for details, see, e.g., Ben-Tal and Nemirovski
[2, Chapter 15]). Mathematically, the TTD problem is the semideï¬nite program
min
9 t
ï£±ï£´ï£²ï£´ï£³9
ï£®ï£¯ï£°29 f
T
f
nâˆ‘
i=1
tibib
T
i
ï£¹ï£ºï£» 0 t â‰¥ 0âˆ‘
i
ti = 1
ï£¼ï£´ï£½ï£´ï£¾  (57)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 19
Table 1. Experiments with stand-alone calibration procedure, = 1eâˆ’ 6, n= 32, d= 32.
Case
General Diagonal Arrow
 P 8âˆ—  8âˆ—/8âˆ— â‰¤ 8âˆ—  8âˆ—/8âˆ— â‰¤ 8âˆ—  8âˆ—/8âˆ— â‰¤
1.0eâˆ’2 G 9.8eâˆ’2 8.2eâˆ’3 46 9.8eâˆ’2 2.8eâˆ’3 34 1.6eâˆ’1 2.7eâˆ’3 2.5
4.5eâˆ’1 10 3.0eâˆ’1 11 3.5eâˆ’1 1.1
4.5eâˆ’1 10 3.0eâˆ’1 11 3.5eâˆ’1 1.1
U 1.2eâˆ’1 0.0 69 1.2eâˆ’1 1.0eâˆ’5 49 2.0eâˆ’1 1.0eâˆ’5 3.5
9.5eâˆ’2 89 8.6eâˆ’2 70 8.8eâˆ’2 8.0
5.6eâˆ’1 15 3.8eâˆ’1 16 4.4eâˆ’1 1.6
R 1.3eâˆ’2 0.0 40 1.3eâˆ’2 0.0 27 1.1eâˆ’1 1.0eâˆ’5 3.6
9.5eâˆ’2 55 8.5eâˆ’2 42 8.6eâˆ’2 4.8
3.3eâˆ’1 16 2.3eâˆ’1 15 2.7eâˆ’1 1.5
1.0eâˆ’4 G 8.6eâˆ’2 0.0 44 8.6eâˆ’2 2.0eâˆ’5 29 1.3eâˆ’1 2.0eâˆ’5 2.2
3.5eâˆ’1 11 2.3eâˆ’1 11 2.6eâˆ’1 1.1
3.5eâˆ’1 11 2.3eâˆ’1 11 2.6eâˆ’1 1.1
U 1.1eâˆ’1 0.0 68 1.1eâˆ’1 0.0 42 1.6eâˆ’1 0.0 3.1
7.1eâˆ’2 11 6.6eâˆ’2 69 6.7eâˆ’2 7.5
4.3eâˆ’1 17 2.9eâˆ’1 16 3.2eâˆ’1 1.6
R 8.7eâˆ’3 0.0 55 8.7eâˆ’3 0.0 32 9.8eâˆ’2 0.0 3.1
7.1eâˆ’2 68 6.5eâˆ’2 43 6.6eâˆ’2 4.6
2.6eâˆ’1 19 1.7eâˆ’1 17 1.9eâˆ’1 1.6
1.0eâˆ’6 G 7.9eâˆ’2 0.0 44 7.9eâˆ’2 0.0 29 1.1eâˆ’1 0.0 2.2
2.7eâˆ’1 13 1.9eâˆ’1 12 2.0eâˆ’1 1.2
2.7eâˆ’1 13 1.9eâˆ’1 12 2.0eâˆ’1 1.2
U 9.9eâˆ’2 0.0 71 9.9eâˆ’2 0.0 37 1.4eâˆ’1 0.0 2.9
5.9eâˆ’2 12 5.6eâˆ’2 67 5.7eâˆ’2 7.4
3.4eâˆ’1 21 2.4eâˆ’1 16 2.6eâˆ’1 1.6
R 7.0eâˆ’3 0.0 67 7.0eâˆ’3 0.0 39 8.8eâˆ’2 0.0 3.0
5.9eâˆ’2 79 5.5eâˆ’2 49 5.6eâˆ’2 4.8
2.1eâˆ’1 22 1.4eâˆ’1 20 1.5eâˆ’1 1.7
Notes. Column â€œPâ€: identical to each other distributions of 1     d; G, U, R stand for  01, Uniform	âˆ’11
, and Uniformâˆ’141,
respectively.
Columns â€œ8âˆ—â€: lower bounds on 8
âˆ—. Rows in a cell are as follows:
â€¢ First row: simulation-free bound (Gaussian majorization coupled with Proposition A.1, Theorem 4.1, Theorem 4.2, depending on whether
Al are general/diagonal/arrow)
â€¢ Second row: PCP calibration
â€¢ Third row: GCP calibration.
Gaussian majorization is based either on comparison, or on Majorization Theorem, depending on the type (U/R) of the distributions of l.
Columns â€œ â€: empirical value, over 100,000-element sample, of 1âˆ’ p8, see (56), 8 being set to the largest value in the corresponding
cell of the 8âˆ—-column
Columns â€œ8âˆ—/8
âˆ— â‰¤â€: ratios of the empirical bound on 8âˆ— as yielded by 100,000 sample, to the corresponding lower bounds on 8âˆ— from the
8âˆ—-column.
where 9 is (an upper bound on) the complianceâ€”a natural measure of trussâ€™ rigidity (the less the compliance,
the better), ti are weights of the bars, f represents the external load, and bi are readily given by the geometry
of the nodal set. The dimension M of bis and f is the total # of degrees of freedom of the nodes.
The â€œnominal designâ€ shown in Figure 1(a) is the optimal solution to a small TTD problem with 9Ã—9 planar
nodal grid and where the load f is comprised of a single force (see Figure 1(c)). This design uses just 12 of
the original 81 nodes and 24 of the potential 2,039 bars. In reality, the truss, of course, will be subject not only
to the primary load f , but also to occasional secondary, relatively small, loads affecting the nodes used by the
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
20 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
Table 2. Performance of stand-alone calibration procedure vs. size N of training sample,
= 1.eâˆ’ 6, n= 32, d= 32, general-type matrices Al.
8âˆ—
 P N = 1000 N = 10000 N = 100000
1.0eâˆ’2 G 3.7eâˆ’1 4.4eâˆ’1 4.5eâˆ’1
3.7eâˆ’1 4.4eâˆ’1 4.5eâˆ’1
U 9.4eâˆ’2 9.5eâˆ’2 9.5eâˆ’2
4.7eâˆ’1 5.5eâˆ’1 5.6eâˆ’1
R 9.4eâˆ’2 9.5eâˆ’2 9.5eâˆ’2
2.5eâˆ’1 3.2eâˆ’1 3.3eâˆ’1
1.0eâˆ’4 G 2.5eâˆ’1 3.0eâˆ’1 3.5eâˆ’1
2.5eâˆ’1 3.0eâˆ’1 3.5eâˆ’1
U 7.0eâˆ’2 7.1eâˆ’2 7.1eâˆ’2
3.1eâˆ’1 3.8eâˆ’1 4.4eâˆ’1
R 7.1eâˆ’2 7.1eâˆ’2 7.1eâˆ’2
1.7eâˆ’1 2.3eâˆ’1 2.6eâˆ’1
1.0eâˆ’6 G 2.0eâˆ’1 2.4eâˆ’1 2.7eâˆ’1
2.0eâˆ’1 2.4eâˆ’1 2.7eâˆ’1
U 5.9eâˆ’2 6.0eâˆ’2 5.9eâˆ’2
2.5eâˆ’1 3.0eâˆ’1 3.4eâˆ’1
R 5.9eâˆ’2 6.0eâˆ’2 5.9eâˆ’2
1.4eâˆ’1 1.8eâˆ’1 2.1eâˆ’1
Notes. Column â€œPâ€: see Table 1. The ï¬rst number in â€œ8âˆ—â€-cells corresponds to PCP, the
second corresponds to GCP.
construction. The truss should, of course, withstand these loads as well. This is by far not the case with the
truss on Figure 1(a)â€”it can be crushed by a very small occasional load. Indeed, a typical random load fËœ acting
on the 12 nodes of the nominal design, and of very small size as compared to f , say (fËœ 2 â‰¤ 10âˆ’7f 2), results
in compliances that are about 10 times larger than the compliance caused by fâ€”a phenomenon illustrated on
Figure 1(b). A natural way to â€œcureâ€ the nominal design is to reformulate the TTD problem, explicitly imposing
(a): Nominal design,
      12 nodes, 24 bars.
(b)  Dotted lines: Positions of nodes in
       deformated nominal design, sample
       of 100 loads ~  (0, 10â€“16 I20)
f
f
(c): 12-node set with left-most
       nodes fixed and the load of interest.
       M = 20 degrees of freedom.
(d): 54 tentative bars (f): Clouded nodes: Positions of nodes in deformated
      chance-constrained design, sample of 100
      loads ~  (0, 10â€“2 I20)
(e): Chance-constrained design, 12
       nodes, 33 bars. Compliance w.r.t.
       the load of interest 1.025.
Figure 1. Nominal and chance-constrained designs.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 21
the requirement that the would-be truss should carry occasional random loads well. Speciï¬cally, we
â€¢ replace the original 81-point nodal set with the 12-point set of nodes actually used by the nominal design
(Figure 1(c)). Note that among these nodes, the two leftmost ones are ï¬xed by boundary conditions (â€œare in the
wallâ€), so that the total number M of degrees of freedom of this reduced nodal set is 2Ã— 10= 20;
â€¢ allow for all pair connections of the resulting 12 nodes by tentative bars (except for clearly redundant bar
linking the two ï¬xed nodes and the bars incident to more than 2 nodes); the resulting 54 tentative bars are
shown on Figure 1(d);
â€¢ assume that the occasional loads are random âˆ¼  0 82I20, where 8 is an uncertainty level, and take, as
the â€œcorrectedâ€ truss, the chance-constrained designâ€”the optimal solution to the following chance-constrained
semideï¬nite program:
max
8 t
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
8
Atï¸· ï¸¸ï¸¸ ï¸·ï£®ï£¯ï£°29 f
T
f
54âˆ‘
i=1
tibib
T
i
ï£¹ï£ºï£» 0 t â‰¥ 0 âˆ‘
i
ti = 1
Probâˆ¼ 0 I20
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
ï£®ï£¯ï£° 29 8
T
8
54âˆ‘
i=1
tibib
T
i
ï£¹ï£ºï£»
ï¸¸ ï¸·ï¸· ï¸¸
0	t
+8
âˆ‘M
l=1 ll 	t

 0
ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£¾
â‰¥ 1âˆ’ 
ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£¾
 (58)
where 9 is slightly greater than the optimal value 9âˆ— in the original TTD problem (in our experiment, we set
9 = 10259âˆ—). In other words, we are now looking for truss for which the compliance w.r.t. the primary load
f is nearly optimalâ€”is at most 9 , and which is capable of withstanding equally well to â€œnearly allâ€ (up to
probability ) random occasional loads of the form 8 ,  âˆ¼  0 I20; under these restrictions, we intend to
maximize 8, i.e., to maximize (the 1âˆ’ -quantile of) the rigidity of the truss w.r.t. occasional loads (cf. (26)).
Note that the robust optimization version of the outlined strategy was proposed and discussed in full details in
Ben-Tal and Nemirovski [1].
Implementing the outlined strategy, we built and solved the safe tractable approximation
min
3 t
ï£±ï£²ï£³3
At 0 t â‰¥ 0 âˆ‘
i
ti = 1
Arrow30	t
1	t
    M	t
 0
ï£¼ï£½ï£¾ (59)
(cf. (27)) of the chance-constrained TTD problem (58). After a feasible solution tâˆ— to the approximation is
found, we used the calibration procedure to build a 1âˆ’ -reliable lower bound 8âˆ— on the largest 8= 8âˆ—tâˆ—
such that tâˆ— 8 is feasible for (58). In our experiment, we worked with pretty high reliability requirements:
 = = 1.eâˆ’10. The results are presented in Table 3 and are illustrated on Figure 1. Note that we are in the
arrow case, so that we can build a simulation-free lower bound on 8âˆ—tâˆ—; see Â§4.3. With our data, this load
is 4.01eâˆ’3â€”more than 10 times worse than the best simulation-based extremely reliable (= 1eâˆ’10) bound
presented in Table 3.
Comparison with the scenario approximation. We have used the TTD example to compare our approx-
imation scheme with the scenario one (see the introduction). The latter, to the best of our knowledge, is the
Table 3. Lower bounds for 8âˆ—tâˆ— in the chance-constrained TTD problem vs. the
size N of training sample, = = 1eâˆ’10.
N = 1000 N = 10000 N = 100000
8âˆ— 0035 0041 0043
8âˆ—tâˆ—/8âˆ— â‰¤ 199 164 156
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
22 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
only existing alternative for processing chance-constrained LMIs. The scenario approximation of the chance
constrained problem of interest (58) is the semideï¬nite program
max
8 t
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
8
ï£®ï£¯ï£°29 f
T
f
54âˆ‘
i=1
tibib
T
i
ï£¹ï£ºï£» 0 t â‰¥ 0 âˆ‘
i
ti = 1
ï£®ï£¯ï£° 29 8	
j 
T
8j
54âˆ‘
i=1
tibib
T
i
ï£¹ï£ºï£» 0 1â‰¤ j â‰¤ J
ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£¾
 (60)
where 1     J is a sample drawn from  0 I20; the sample size J is given by (5) where one should set
m= dim t + dim8= 55. Needless to say, the scenario approximation with the above  = = 1eâˆ’10 requires
a completely unrealistic sample size; this is why we ran the scenario approximation with  = 001, = 0001.
Although these levels of unreliability are by far too dangerous for actual truss design, they are acceptable in our
current comparison context. With the outlined , the sample size J as given by (5) is 42,701, and the optimal
value in (60) turned out to be 8SA = 00797. For comparison, our approximation with  = 001 and = 0001
results in 8âˆ— = 0105â‰ˆ 1318SA; keeping = 001 and reducing  to 1.eâˆ’6, we still get 8âˆ— = 0103â‰ˆ 1298SA.
Note that the design given by (59) also seems to be better than the one given by (60): At uncertainty level
8 = 0105, the empirical probabilities (over 100,000-element sample of random occasional loads) for the two
designs to yield a compliance worse than the desired upper bound 9 were 0.0077 and 0.0097, respectively. Thus,
in the experiment we are reporting, our approximation scheme is a clear winner.
Appendix A. Some proofs.
Proof of Equivalence in Deï¬nition 5.1. We should prove that if ps, qs are nonincreasing on R+
and such that
âˆ«
R+ psds =
âˆ«
R+
qsds, and  is the family of all bounded nondecreasing functions on R+,
then {
âˆ€ f âˆˆ
âˆ«
f spsds â‰¤
âˆ«
f sqsds
}
â‡”
{
âˆ€ t â‰¥ 0
âˆ« 
t
psds â‰¤
âˆ« 
t
qsds
}
 (61)
By standard continuity arguments, the left condition in (61) is equivalent to the similar condition with  replaced
with the space 	 of all continuously differentiable bounded nondecreasing functions on R+.
Setting Ps= âˆ« 
s
prdr , Qs= âˆ« 
s
qrdr , for every f âˆˆ	 we have
I	f 
 =
âˆ« 
0
f s	qsâˆ’ps
ds =âˆ’
âˆ« 
0
f sdQs+
âˆ« 
0
f sdPs
= f 0	Q0âˆ’P0
+
âˆ« 
0
f â€²s	Qsâˆ’Ps
ds =
âˆ« 
0
f â€²s	Qsâˆ’Ps
ds
We see that I	f 
 â‰¥ 0 for every continuously differentiable nondecreasing and bounded f if and only ifâˆ« 
0 gs	Qs âˆ’ Ps
ds â‰¥ 0 for every nonnegative summable function gÂ· on R+; because PÂ·, QÂ· are
continuous, the latter is the case if and only if Qsâ‰¥ Ps for all s â‰¥ 0. 
Proof of Proposition 5.1. Relations (i)â€“(iv) are evident in view of the equivalence mentioned in Deï¬ni-
tion 5.1.
(v): Relation E m G clearly implies that E m G for every deterministic . In view of this fact, in order to
prove (v) it sufï¬ces to prove that if the densities p pq q belong to  and p m q, p m q, then p âˆ— p and
q âˆ— q belong to  and p âˆ— pm q âˆ— q.
10. Let us verify that p âˆ— p âˆˆ. We should prove that the density p âˆ— ps= âˆ« psâˆ’ r prdr is even
(which is evident) and is nonincreasing on R+. By standard approximation arguments, it sufï¬ces to verify the
latter fact when the probability densities p, p, in addition to being even and nonincreasing on R+, are smooth.
In this case we have
p âˆ— pâ€²s=
âˆ«
pâ€²sâˆ’ r prdr =
âˆ«
psâˆ’ r pâ€²rdr =
âˆ« 0
âˆ’
psâˆ’ tâˆ’ps+ t pâ€²tdt (62)
Let s â‰¥ 0. Then for t â‰¤ 0 we have s + t = sâˆ’ t â‰¥ s+ t, and because p is even and nonincreasing on R+,
we conclude that psâˆ’ t= psâˆ’ tâ‰¤ ps+ t= ps+ t, so that psâˆ’ tâˆ’ps+ tâ‰¤ 0 when s â‰¥ 0 t â‰¤ 0.
Because, in addition, pâ€²t â‰¥ 0 when t â‰¤ 0, the concluding quantity in (62) is nonpositive, meaning that the
density p âˆ— p is even and is nonincreasing on R+.
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 23
20. Now let us verify that if âˆ— is the family of all even bounded and continuously differentiable functions
on R that are nondecreasing on R+, then f+ = p âˆ— f âˆˆâˆ— whenever f âˆˆâˆ—. The only nontrivial claim is that
f+ is nondecreasing on R+, and when verifying it, we, the same as in 10, can assume that p is not only even
and nonincreasing on R+, but is also smooth. In this case we have f â€²+s=
âˆ«
f sâˆ’ rpâ€²rdr = âˆ« 0âˆ’f sâˆ’ tâˆ’
f s+ tpâ€²tdt. Assuming s â‰¥ 0, t â‰¤ 0 and taking into account that f is even and is nondecreasing on R+, we
have f sâˆ’ t= f sâˆ’ t= f s+ tâ‰¥ f s+ t= f s+ t; because pâ€²tâ‰¥ 0 when t â‰¤ 0, we conclude thatâˆ« 0
âˆ’f sâˆ’ tâˆ’ f s+ tpâ€²tdt â‰¥ 0 when s â‰¥ 0.
30. Now we can conclude the proof of (v). We already know from 10 that the convolutions of every two
of the four densities p pq q belong to . All we should prove is that when pÂ·m qÂ· and pÂ·m qÂ·,
then p âˆ— pÂ·m q âˆ— qÂ·.
30.(a) Let us ï¬rst verify that p âˆ— pÂ·m p âˆ— qÂ·, that is,âˆ«
f sp âˆ— psds â‰¤
âˆ«
f sp âˆ— qsds (63)
for every even bounded function f that is nondecreasing on R+. By evident continuity reasons, it sufï¬ces to
verify that (63) holds true for every f âˆˆâˆ—. Taking into account that p is even, we getâˆ«
f sp âˆ— psds =
âˆ«
f spsâˆ’ t ptds dt =
âˆ«
f âˆ—pt ptdt
and by similar reasons âˆ«
f sp âˆ— qsds =
âˆ«
f âˆ—pt qtdt
As we know from 20, f âˆ—p âˆˆâˆ— whenever f âˆˆâˆ—, and (63) follows from the fact that pÂ·m qÂ·.
30.(b) The result of 30.(a) states that p âˆ— pm p âˆ— q. By the same result, but with swapped roles of â€œplainâ€
and â€œÌ‚ â€ components, we further have p âˆ— q m q âˆ— q. As we know from (i), m is a partial order, so that
p âˆ— pm p âˆ— q and p âˆ— q m q âˆ— q imply the desired relation p âˆ— pm q âˆ— q. (v) is proved.
(vi): To prove that E m  , observe that because E âˆˆ  and E is supported on 	âˆ’11
, the density of E
clearly is the weak limit of convex combinations of densities of uniform distributions on segments of the form
	âˆ’aa
 with aâ‰¤ 1. Every one of these uniform distributions is m the distribution of  by (iii), so that their
convex combinations are m the distribution of  by (ii). Applying (iv), we conclude that E m  .
To prove that  m G, let pÂ· and qÂ· be the respective densities (both of them belong to ), and let
PËœ t= âˆ« t0 psds = 12 min	t1
, QËœt= âˆ« t0 qsds; this function is concave in t â‰¥ 0 because qÂ· is nonincreasing
on R+. To prove that  s G is exactly the same as to verify that PËœ tâ‰¥ QËœt for all t â‰¥ 0. This is indeed the case
when 0â‰¤ t â‰¤ 1, because QËœ0= 0, QËœâ€²0= 1/2, and QËœ is concave on R+, whereas PËœ t= 12 t = QËœ0+ tQËœâ€²0
when 0â‰¤ t â‰¤ 1. And, of course, PËœ t= 1/2â‰¥ QËœt when t â‰¥ 1. (vi) is proved.
(vii): 10. Observe, ï¬rst, that whenever pÂ· âˆˆ, then there exists a sequence ptÂ· âˆˆt=1 such that
(a) every ptÂ· is a convex combination of densities of uniform symmetric w.r.t. 0 distributions;
(b) pt â†’ p as tâ†’ in the sense thatâˆ«
f sptsdsâ†’
âˆ«
f spsds as tâ†’
for every bounded piecewise-continuous function f on the axis.
20. We have the following:
Lemma A.1. Let Q âŠ‚ Rd be a nonempty convex compact set symmetric w.r.t. the origin, and let
p1Â·     pdÂ· qÂ· âˆˆ be such that p1Â·     pdâˆ’1Â· are densities of uniform distributions and pdÂ·m
qÂ·. Then, âˆ«
Q
p1x1p2x2   pdâˆ’1xdâˆ’1pdxddxâ‰¥
âˆ«
Q
p1x1p2x2   pdâˆ’1xdâˆ’1qxddx (64)
Proof of Lemma A.1. Let Kl, 1â‰¤ l < d, be the support of the density pl, so that Kl is a segment on the
axis symmetric w.r.t. 0. Let us set K = K1 Ã— Â· Â· Â· Ã— Kdâˆ’1 Ã—R, Q = Q âˆ© K, so that Q is a convex compact set
symmetric w.r.t. the origin, and let
f s=mesdâˆ’1x âˆˆ Q xd = s
The function f s is even; denoting by M the projection of Q onto the xd-axis and applying the symmeterization
principle of Brunn-Minkowski, we conclude that f 1/dâˆ’1s is concave, even, and continuous on M, whence, of
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
24 Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS
course, f 1/dâˆ’1s is nonincreasing in Mâˆ©R+. We see that the function f s is even, bounded, and nonnegative,
and is nonincreasing on R+, whence âˆ«
f spdsds â‰¥
âˆ«
f sqsds (65)
due to pdÂ·m qÂ·. It remains to note that the left- and the right-hand sides in (64) are proportional, with a
common positive coefï¬cient, to the respective sides in (65). 
30. Now we can complete the proof of (vii). Clearly, all we need is to show that if p1Â·     pdÂ· qdÂ· âˆˆ
 and pdÂ·m qdÂ·, thenâˆ«
Q
p1x1p2x2   pdâˆ’1xdâˆ’1pdxddxâ‰¥
âˆ«
Q
p1x1p2x2   pdâˆ’1xdâˆ’1qdxddx
By continuity argument and in view of 10, it sufï¬ces to verify the same relation when p1Â·     pdâˆ’1Â· are
convex combinations of densities of uniform and symmetric w.r.t. the origin distributions. Because both sides
in our target inequality are linear in every one of p1     pdâˆ’1, to prove the latter fact is the same as to prove
it when every one of p1     pdâˆ’1 is a uniform distribution symmetric w.r.t. the origin. In the latter case, the
required statement is given by Lemma A.1. 
Proof of Proposition 5.2.(ix). Under the premise of the statement to be proved, Q contains the centered
at the origin Â·2-ball of the radius ErfInv* (Lemma 3.1.(i)), so that the Minkowski function Fx = inf
t  tâˆ’1x âˆˆQ of Q belongs to 	
n. Let I âˆˆ 	1 ), and let x=max	Fxâˆ’I0
We clearly have Â· âˆˆ	
n,
so that âˆ«
xdPExâ‰¤
âˆ«
xdPGx (66)
For s â‰¥ I, let ps= ProbG âˆˆ sQ= ProbG > sâˆ’I. By Lemma 3.1.(ii) we have
s â‰¥ I â‡’ psâ‰¤ ErfsErfInv* (67)
We have
âˆ«
xdPGx=âˆ’
âˆ« 
I
sâˆ’Idps= âˆ« 
I
psds â‰¤ âˆ« 
I
ErfsErfInv*ds (the concluding inequality
is due to (67)), whence
âˆ«
xdPEx â‰¤
âˆ« 
I
ErfsErfInv*ds by (66). Now, when E âˆˆ )Q, we have E â‰¥
)âˆ’I. Invoking the Tschebyshev inequality, we arrive at
ProbE âˆˆ )Qâ‰¤ EE
)âˆ’I â‰¤
1
)âˆ’I
âˆ« 
I
ErfsErfInv*ds 
Proof of Property P via Noncommutative Khintchine Inequality. We start with the following
deep fact of functional analysis due to Lust-Piquard [11], Pisier [16], and Buchholz [7]; see Tropp [17, Propo-
sition 10]:
Noncommutative Khintchine Inequality (NKI): Let Gâˆ¼ 0 Id, and let Q1    Qd be deterministic matri-
ces. Then for every p âˆˆ 	2 one has
E
{âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
GlQl
âˆ£âˆ£âˆ£âˆ£p
p
}
â‰¤ 	2âˆ’1/4âˆšp-/e
pmax[âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
QlQ
T
l
âˆ£âˆ£âˆ£âˆ£
p/2

âˆ£âˆ£âˆ£âˆ£ dâˆ‘
l=1
QTl Ql
âˆ£âˆ£âˆ£âˆ£
p/2
]p/2
 (68)
where Ap = AAp, AA being the vector of singular values of a matrix A.
As an immediate corollary of NKI, we have the following
Proposition A.1. Let Bl âˆˆ Sn, nâ‰¥ 2, be such that
âˆ‘d
l=1B
2
l â‰¤ I , and let
cn = inf
2â‰¤p<
	2âˆ’1/4
âˆš
p-/en1/p

Then cn â‰¤O1
âˆš
lnn and for all * âˆˆ 01/2 one has
Prob
{
âˆ’+*I 
dâˆ‘
l=1
lBl +*I
}
â‰¥ 1âˆ’*
+*=
{
16cnErfInv03* we are in the case of (A.1)
cn/* we are in the case of (A.2)
(69)
Ben-Tal and Nemirovski: On Safe Tractable Approximations of Chance-Constrained Linear Matrix Inequalities
Mathematics of Operations Research 34(1), pp. 1â€“25, Â© 2009 INFORMS 25
Proof. When p = max	2 lnn
, we clearly have 2âˆ’1/4âˆšp-/en1/p â‰¤ O1âˆšp â‰¤ O1âˆšlnn, so that cn â‰¤
O1
âˆš
lnn. Now assume that (A.2) is the case. Applying NKI with Ql = Bl and taking into account
that âˆ‘dl=1 lBlp â‰¤ âˆ‘dl=1 lBl and taking into account that âˆ‘dl=1B2l p/2p/2 â‰¤ n due to âˆ‘l B2l  In, we get
Eâˆ‘dl=1 lBlp â‰¤ 	2âˆ’1/4âˆšp-/e
pn whence Probâˆ‘l lBl > + â‰¤ 	2âˆ’1/4âˆšp-/e
n1/p for every + > 0 by
Tschebyshev Inequality. The resulting bound is valid for every p âˆˆ 	2, and the (A.2)-version of (69) follows.
The (A.1)-version of (69) follows from the (A.2)-version of this relation due to Gaussian majorization. 
Acknowledgments. This research was partly supported by BSF Grant 2002038; research of the second
author was supported by NSF Grant 0619977. The authors are greatly indebted to A. Man-Cho So, who brought
to their attention the noncommutative Khintchine inequality.
References
[1] Ben-Tal, A., A. Nemirovski. 1997. Stable truss topology design via semideï¬nite programming. SIAM J. Optim. 7(4) 991â€“1016.
[2] Ben-Tal, A., A. Nemirovski. 2001. Lectures on Modern Convex Optimization: Analysis, Algorithms and Engineering Applications.
MPS-SIAM Series on Optimization, SIAM, Philadelphia.
[3] Ben-Tal, A., A. Nemirovski. 2002. On tractable approximations of uncertain linear matrix inequalities affected by interval uncertainty.
SIAM J. Optim. 12 811â€“833.
[4] Ben-Tal, A., A. Nemirovski, C. Roos. 2002. Robust solutions of uncertain quadratic and conic-quadratic problems. SIAM J. Optim. 13
535â€“560.
[5] Ben-Tal, A., A. Nemirovski, C. Roos. 2003. Extended matrix cube theorems with applications to N-Theory in control. Math. Oper.
Res. 28 497â€“523.
[6] Borell, C. 1975. The Brunn-Minkowski inequality in Gauss space. Inventiones Mathematicae 30(2) 207â€“216.
[7] Buchholz, A. 2001. Operator Khintchine inequality in the non-commutative probability. Mathematische Annalen 391 1â€“16.
[8] Calaï¬ore, G., M. C. Campi. 2005. Uncertain convex programs: Randomized solutions and conï¬dence levels. Math. Programming 102
25â€“46.
[9] de Farias, D. P., B. Van Roy. 2004. On constraint sampling in the linear programming approach to approximate dynamic programming.
Math. Oper. Res. 29 462â€“478.
[10] Erdogan, G., G. Iyengar. 2006. Ambiguous chance constrained problems and robust optimization. Math. Programming B 107(1â€“2)
37â€“61.
[11] Lust-Piquard, F. 1986. InÃ©galitÃ©s de khintchine dans Cp 1< p <. Comptes Rendus de lâ€™AcadÃ©mie des Sciences de Paris, SÃ©rie I
393(7) 289â€“292.
[12] Man-Cho So, A. 2008. Improved approximation bound for quadratic optimization problems with orthogonality constraints. Technical
report, Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T.,
Hong Kong.
[13] Nemirovski, A. 2007. Sums of random symmetric matrices and quadratic optimization under orthogonality constraints. Math. Pro-
gramming B 109 283â€“317.
[14] Nemirovski, A., A. Shapiro. 2006. Scenario approximations of chance constraints. G. Calaï¬ore, F. Dabbene, eds. Probabilistic and
Randomized Methods for Design Under Uncertainty. Springer-Verlag, New York, 3â€“48.
[15] Nemirovski, A., A. Shapiro. 2006. Convex approximations of chance constrained programs. SIAM J. Optim. 17(4) 969â€“996.
[16] Pisier, G. 1998. Non-commutative vector valued Lp spaces and completely p-summing maps. AustÃ©risque 247 1â€“111.
[17] Tropp, J. A. 2008. The random paving property for uniformly bounded matrices. Studia Mathematica 185 67â€“82.
